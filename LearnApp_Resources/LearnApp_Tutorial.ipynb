{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51f7ebe1",
   "metadata": {},
   "source": [
    "\n",
    "# Snekmer Learn - Apply  Demo\n",
    "\n",
    "\n",
    " <b>Learn/Apply</b> is a protein annotation method that uses cosine similarity to compare a user-generated kmer counts matrix to the kmer counts of an novel genome, predicting the annotation of each protein within.\n",
    "\n",
    "**Learn** relies on a large training set of genomes to make predictions. With each addition to the training set, the accuracy increases.\n",
    "\n",
    "**Apply** requires several outputs from Learn as well some novel genomes or sequences.\n",
    "\n",
    "\n",
    "In this notebook, we will demonstrate how to use Snekmer Learn/Apply with small training dataset of 20 genomes and 2 \"unknown\" genomes.\n",
    "\n",
    "\n",
    "\n",
    "## Getting Started with LEARN\n",
    "\n",
    "### Setup\n",
    "\n",
    "First, install Snekmer using the instructions in the [user installation guide](https://github.com/PNNL-CompBio/Snekmer/).\n",
    "\n",
    "Before running Snekmer, verify that files have been placed in an **_input_** directory placed at the same level as the **_config.yaml_** file. The assumed file directory structure is illustrated below.\n",
    "\n",
    "    .\n",
    "    ├── input\n",
    "    │   ├── A.fasta\n",
    "    │   ├── B.fasta\n",
    "    │   ├── C.fasta\n",
    "    │   ├── D.fasta\n",
    "    │   └── etc.\n",
    "    ├── config.yaml\n",
    "    ├── annotations\n",
    "        └── TIGRFAMS.ann\n",
    "    \n",
    "(Note: Snekmer automatically creates the **_output_** directory when creating output files, so there is no need to create this folder in advance. Additionally, inclusion of background sequences is optional, but is illustrated above for interested users.)\n",
    "\n",
    "To ensure that snekmer is available in the Jupyter notebook do the following:\n",
    "    conda activate snekmer \n",
    "    conda install -c anaconda ipykernel \n",
    "    python -m ipykernel install --user --name=snekmer\n",
    "    jupyter notebook\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3730b1",
   "metadata": {},
   "source": [
    "### Notes on Using Snekmer\n",
    "\n",
    "Snekmer assumes that the user will primarily process input files using the command line. For more detailed instructions, refer to the [README](https://github.com/PNNL-CompBio/Snekmer).\n",
    "\n",
    "The basic process for running Snekmer Learn-Apply is as follows:\n",
    "\n",
    "1. Verify that your file directory structure is correct and that the top-level directory contains a **_config.yaml_** file.\n",
    "    - A **_config.yaml_** template has been included in the Snekmer codebase at **_resources/learn_apply/config.yaml_**.\n",
    "2. Modify the **_config.yaml_** with the desired parameters.\n",
    "3. Use the command line to navigate to the directory containing both the **_config.yaml_** file and **_input_** directory.\n",
    "4. Run `snekmer learn`, then copy the appropriate outputs a seperate directory to run `snekmer apply`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb714c7d",
   "metadata": {},
   "source": [
    "# Running Snekmer Learn Pipeline\n",
    "\n",
    "### Setup\n",
    "\n",
    "To set up the workflow such that operation mimics the command line implementation of Snekmer Leanr/Apply, we will initialize a dictionary (rather than a YAML file) and gather all input files. Input files are detected here using `glob.glob`, exactly as Snekmer performs input file detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7544bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# built-in imports\n",
    "import itertools\n",
    "import gzip\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "from os.path import basename, join\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "import snekmer as skm\n",
    "from os.path import basename, dirname, exists, join, splitext, split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from Bio import SeqIO\n",
    "from pathlib import Path\n",
    "import copy\n",
    "from scipy.stats import rankdata\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "import pyarrow as pa\n",
    "import pyarrow.csv as csv\n",
    "import itertools\n",
    "import sklearn\n",
    "from scipy.interpolate import interp1d\n",
    "import os\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "import re\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489c0a97",
   "metadata": {},
   "source": [
    "## Configuration File Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01a798b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define config\n",
    "# (note: handled via config.yaml in the snekmer CLI workflow)\n",
    "\n",
    "config = {\n",
    "    \n",
    "    # required parameters\n",
    "    \"k\": 8,\n",
    "    \"alphabet\": 2,  # choices 0-5 or names (see alphabet module), or None\n",
    "    \"min_rep_thresh\": 1,\n",
    "    \"processes\": 2,\n",
    "\n",
    "    # input handling\n",
    "    \"input\": {\n",
    "        \"example_index_file\": False,\n",
    "        \"feature_set\": False,\n",
    "        \"file_extensions\": [\"fasta\", \"fna\", \"faa\", \"fa\"],\n",
    "        \"regex\": r\"[a-z]{3}[A-Z]{1}\",  # regex to parse family from filename\n",
    "    },\n",
    "    \n",
    "    # output handling\n",
    "    \"output\": {\n",
    "        \"nested_dir\": False,  # if True, saves into {save_dir}/{alphabet name}/{k}\n",
    "        \"verbose\": True, # if True, logs verbose outputs\n",
    "        \"format\": \"simple\",  # choices: [\"simple\", \"gist\", \"sieve\"]\n",
    "        \"filter_duplicates\": True,\n",
    "        \"n_terminal_file\": False,\n",
    "        \"shuffle_n\": False,\n",
    "        \"shuffle_sequences\": False,\n",
    "    },\n",
    "    \n",
    "    # LearnApply Parameters\n",
    "    \"learnapp\": {\n",
    "        \"save_apply_associations\": False\n",
    "    }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265cd443",
   "metadata": {},
   "source": [
    "## Rule 0: Receive files\n",
    "\n",
    "Before going through the workflow, we glob all filenames contained within the input directory that end in the pre-defined file extensions and/or the extension and `.gz`.\n",
    "\n",
    "Note that while in this notebook, the path to the demo files is specified with the `input_dir` variable, the Snekmer CLI assumes that input files are stored according to the file structure specified above in the **Setup** section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "252902b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zipped files:\t []\n",
      "unzipped files:\t ['LearnApp_Tutorial_Files/LEARN/input/UP000315395_2594265.fasta', 'LearnApp_Tutorial_Files/LEARN/input/UP000313849_676201.fasta', 'LearnApp_Tutorial_Files/LEARN/input/UP000319374_2585119.fasta', 'LearnApp_Tutorial_Files/LEARN/input/UP000319088_2600309.fasta', 'LearnApp_Tutorial_Files/LEARN/input/UP000317977_2528013.fasta', 'LearnApp_Tutorial_Files/LEARN/input/UP000319776_92403.fasta', 'LearnApp_Tutorial_Files/LEARN/input/UP000315466_1411316.fasta', 'LearnApp_Tutorial_Files/LEARN/input/UP000310227_2562283.fasta', 'LearnApp_Tutorial_Files/LEARN/input/UP000319209_2594004.fasta', 'LearnApp_Tutorial_Files/LEARN/input/UP000316313_1293412.fasta', 'LearnApp_Tutorial_Files/LEARN/input/UP000319639_2592816.fasta', 'LearnApp_Tutorial_Files/LEARN/input/UP000316827_1981880.fasta', 'LearnApp_Tutorial_Files/LEARN/input/UP000317332_2567861.fasta', 'LearnApp_Tutorial_Files/LEARN/input/UP000316252_2590779.fasta', 'LearnApp_Tutorial_Files/LEARN/input/UP000316154_239.fasta', 'LearnApp_Tutorial_Files/LEARN/input/UP000310017_2583587.fasta', 'LearnApp_Tutorial_Files/LEARN/input/UP000319210_66857.fasta', 'LearnApp_Tutorial_Files/LEARN/input/UP000380825_2584524.fasta', 'LearnApp_Tutorial_Files/LEARN/input/UP000319897_1715348.fasta', 'LearnApp_Tutorial_Files/LEARN/input/UP000319825_1880.fasta']\n"
     ]
    }
   ],
   "source": [
    "# collect all fasta-like files, unzipped filenames, and basenames\n",
    "input_dir = \"LearnApp_Tutorial_Files/LEARN/input/\"\n",
    "input_files = glob(os.path.join(input_dir, \"*\"))\n",
    "zipped = [fa for fa in input_files if fa.endswith(\".gz\")]\n",
    "unzipped = [\n",
    "    fa.rstrip(\".gz\")\n",
    "    for fa, ext in itertools.product(input_files, config[\"input\"][\"file_extensions\"])\n",
    "    if fa.rstrip(\".gz\").endswith(f\".{ext}\")\n",
    "]\n",
    "\n",
    "print(\"zipped files:\\t\", zipped)\n",
    "print(\"unzipped files:\\t\", unzipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4dcec8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zipped filename wildcards:\t []\n",
      "unzipped filename wildcards:\t ['UP000315395_2594265', 'UP000313849_676201', 'UP000319374_2585119', 'UP000319088_2600309', 'UP000317977_2528013', 'UP000319776_92403', 'UP000315466_1411316', 'UP000310227_2562283', 'UP000319209_2594004', 'UP000316313_1293412', 'UP000319639_2592816', 'UP000316827_1981880', 'UP000317332_2567861', 'UP000316252_2590779', 'UP000316154_239', 'UP000310017_2583587', 'UP000319210_66857', 'UP000380825_2584524', 'UP000319897_1715348', 'UP000319825_1880']\n"
     ]
    }
   ],
   "source": [
    "# map extensions to basename (basename.ext.gz -> {basename: ext})\n",
    "UZ_MAP = {\n",
    "    skm.utils.split_file_ext(f)[0]: skm.utils.split_file_ext(f)[1] for f in zipped\n",
    "}\n",
    "\n",
    "FA_MAP = {\n",
    "    skm.utils.split_file_ext(f)[0]: skm.utils.split_file_ext(f)[1] for f in unzipped\n",
    "}\n",
    "\n",
    "UZS = list(UZ_MAP.keys())\n",
    "FAS = list(FA_MAP.keys())\n",
    "\n",
    "print(\"zipped filename wildcards:\\t\", UZS)\n",
    "print(\"unzipped filename wildcards:\\t\", FAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a989792",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output directory:\t LearnApp_Tutorial_Files/LEARN/output\n"
     ]
    }
   ],
   "source": [
    "# define output directory (and create if missing)\n",
    "output_dir = \"LearnApp_Tutorial_Files/LEARN/output\"\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(\"output directory:\\t\", output_dir)\n",
    "\n",
    "# validity check\n",
    "skm.alphabet.check_valid(config[\"alphabet\"])  # raises error if invalid alphabet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5e3cf2",
   "metadata": {},
   "source": [
    "## Rule 0.5: Unzip files\n",
    "\n",
    "Any zipped files detected by the above are automatically unzipped. The zipped version of the file is copied into a separate subdirectory.\n",
    "\n",
    "**Snakemake code:**\n",
    "\n",
    "    # if any files are gzip compressed, unzip them\n",
    "    rule unzip:\n",
    "    input:\n",
    "        join(\"input\", \"{uz}.gz\")\n",
    "    output:\n",
    "        join(\"input\", \"{uz}\")\n",
    "    params:\n",
    "        outdir=join(\"input\", \"zipped\")\n",
    "    shell:\n",
    "        \"mkdir {params.outdir} && gunzip -c {input} > {output} && mv {input} {params.outdir}/.\"\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "755c9813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if any files are gzip compressed, unzip them\n",
    "for uz in UZS:\n",
    "    input_ = os.path.join(input_dir, f\"{uz}.{UZ_MAP[uz]}.gz\")\n",
    "    output_ = os.path.join(input_dir, f\"{uz}.{UZ_MAP[uz]}\")\n",
    "    outdir = os.path.join(input_dir, \"zipped\")\n",
    "    \n",
    "    ! mkdir -p $outdir && gunzip -c $input_ > $output_ && mv $input_ $outdir/.\n",
    "\n",
    "    print(\"input:\\t\", input_)\n",
    "    print(\"output:\\t\", output_)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e019930",
   "metadata": {},
   "source": [
    "## Rule 1: Preprocess\n",
    "\n",
    "In this step, we parse user-defined parameters into an appropriate format for subsequent pipeline steps.\n",
    "\n",
    "Parameter options include:\n",
    "- `k`: Define kmer length\n",
    "- `alphabet`: Define the translation alphabet\n",
    "\n",
    "The Snakemake code is not shown due to length, but the converted Python-ized code is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86378b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaco059/miniconda3/envs/snekmer_kmer_association/lib/python3.10/site-packages/numpy/lib/npyio.py:713: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  val = np.asanyarray(val)\n"
     ]
    }
   ],
   "source": [
    "for fa in unzipped:\n",
    "    # this is handled by snakemake but we'll specify it here\n",
    "    base = f'{skm.utils.split_file_ext(fa)[0]}.kmers'\n",
    "    output_kmerobj = os.path.join(output_dir, \"kmerize\", base)\n",
    "    if not os.path.exists(os.path.join(output_dir, \"kmerize\")):\n",
    "        os.mkdir(os.path.join(output_dir, \"kmerize\"))\n",
    "        \n",
    "    base = f'{skm.utils.split_file_ext(fa)[0]}.npz'\n",
    "    output_data = os.path.join(output_dir, \"vector\", base)\n",
    "    if not os.path.exists(os.path.join(output_dir, \"vector\")):\n",
    "        os.mkdir(os.path.join(output_dir, \"vector\"))\n",
    "    \n",
    "    fasta = SeqIO.parse(fa, \"fasta\")\n",
    "\n",
    "    # initialize kmerization object\n",
    "    kmer = skm.vectorize.KmerVec(alphabet=config[\"alphabet\"], k=config[\"k\"])\n",
    "\n",
    "    vecs, seqs, ids, lengths = list(), list(), list(), list()\n",
    "    for f in fasta:\n",
    "        vecs.append(kmer.reduce_vectorize(f.seq))\n",
    "        seqs.append(\n",
    "            skm.vectorize.reduce(\n",
    "                f.seq,\n",
    "                alphabet=config[\"alphabet\"],\n",
    "                mapping=skm.alphabet.FULL_ALPHABETS,\n",
    "            )\n",
    "        )\n",
    "        ids.append(f.id)\n",
    "        lengths.append(len(f.seq))\n",
    "\n",
    "    # save seqIO output and transformed vecs\n",
    "    np.savez_compressed(output_data, ids=ids, seqs=seqs, vecs=vecs, lengths=lengths)\n",
    "\n",
    "    with open(output_kmerobj, \"wb\") as f:\n",
    "        pickle.dump(kmer, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c169d7c6",
   "metadata": {},
   "source": [
    "## Rule 2: Learn\n",
    "In this step, we generate kmer counts for each fasta input file. In the following step these are merged to find cumulative kmer counts. These kmer counts can be thought of as training the annotation model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffa8c2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts Data Generated for:  LearnApp_Tutorial_Files/LEARN/output/vector/UP000315395_2594265.npz\n",
      "Counts Data Generated for:  LearnApp_Tutorial_Files/LEARN/output/vector/UP000313849_676201.npz\n",
      "Counts Data Generated for:  LearnApp_Tutorial_Files/LEARN/output/vector/UP000319374_2585119.npz\n",
      "Counts Data Generated for:  LearnApp_Tutorial_Files/LEARN/output/vector/UP000319088_2600309.npz\n",
      "Counts Data Generated for:  LearnApp_Tutorial_Files/LEARN/output/vector/UP000317977_2528013.npz\n",
      "Counts Data Generated for:  LearnApp_Tutorial_Files/LEARN/output/vector/UP000319776_92403.npz\n",
      "Counts Data Generated for:  LearnApp_Tutorial_Files/LEARN/output/vector/UP000315466_1411316.npz\n",
      "Counts Data Generated for:  LearnApp_Tutorial_Files/LEARN/output/vector/UP000310227_2562283.npz\n",
      "Counts Data Generated for:  LearnApp_Tutorial_Files/LEARN/output/vector/UP000319209_2594004.npz\n",
      "Counts Data Generated for:  LearnApp_Tutorial_Files/LEARN/output/vector/UP000316313_1293412.npz\n",
      "Counts Data Generated for:  LearnApp_Tutorial_Files/LEARN/output/vector/UP000319639_2592816.npz\n",
      "Counts Data Generated for:  LearnApp_Tutorial_Files/LEARN/output/vector/UP000316827_1981880.npz\n",
      "Counts Data Generated for:  LearnApp_Tutorial_Files/LEARN/output/vector/UP000317332_2567861.npz\n",
      "Counts Data Generated for:  LearnApp_Tutorial_Files/LEARN/output/vector/UP000316252_2590779.npz\n",
      "Counts Data Generated for:  LearnApp_Tutorial_Files/LEARN/output/vector/UP000316154_239.npz\n",
      "Counts Data Generated for:  LearnApp_Tutorial_Files/LEARN/output/vector/UP000310017_2583587.npz\n",
      "Counts Data Generated for:  LearnApp_Tutorial_Files/LEARN/output/vector/UP000319210_66857.npz\n",
      "Counts Data Generated for:  LearnApp_Tutorial_Files/LEARN/output/vector/UP000380825_2584524.npz\n",
      "Counts Data Generated for:  LearnApp_Tutorial_Files/LEARN/output/vector/UP000319897_1715348.npz\n",
      "Counts Data Generated for:  LearnApp_Tutorial_Files/LEARN/output/vector/UP000319825_1880.npz\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"LearnApp_Tutorial_Files/LEARN/output/learn\"):\n",
    "    os.makedirs(\"LearnApp_Tutorial_Files/LEARN/output/learn\")\n",
    "\n",
    "for fa in unzipped:\n",
    "    annot_files = glob(join(\"LearnApp_Tutorial_Files/LEARN/annotations\", \"*.ann\"))\n",
    "    base = f'{skm.utils.split_file_ext(fa)[0]}.npz'\n",
    "    input_data = os.path.join(output_dir, \"vector\", base)\n",
    "    \n",
    "    Annotation = list()\n",
    "    for f in annot_files:\n",
    "        Annotation.append(pd.read_table(f))\n",
    "    annotations = pd.concat(Annotation)\n",
    "    Seq_Anot = {}\n",
    "    Seqs= Annotation[0]['id'].tolist()\n",
    "    ANNs = Annotation[0]['TIGRFAMs'].tolist()\n",
    "    for i,seqid in enumerate(Seqs):\n",
    "        Seq_Anot[seqid] = ANNs[i]\n",
    "    Seqs = set(Seqs)\n",
    "    ANNs = set(ANNs)\n",
    "\n",
    "    output_data = os.path.join(output_dir, \"vector\", base)\n",
    "    fasta = SeqIO.parse(fa, \"fasta\")\n",
    "    # initialize kmerization object\n",
    "    kmer = skm.vectorize.KmerVec(alphabet=config[\"alphabet\"], k=config[\"k\"])\n",
    "\n",
    "    vecs, seqs, ids, lengths = list(), list(), list(), list()\n",
    "\n",
    "    for f in fasta:\n",
    "        vecs.append(kmer.reduce_vectorize(f.seq))\n",
    "        seqs.append(\n",
    "            skm.vectorize.reduce(\n",
    "                f.seq,\n",
    "                alphabet=config[\"alphabet\"],\n",
    "                mapping=skm.alphabet.FULL_ALPHABETS,\n",
    "            )\n",
    "        )\n",
    "        ids.append(f.id)\n",
    "        lengths.append(len(f.seq))\n",
    "        \n",
    "    df, kmerlist = skm.vectorize.make_feature_matrix(vecs)\n",
    "\n",
    "    seqids = ids\n",
    "    kmer_totals = []\n",
    "    for item in kmerlist:\n",
    "        kmer_totals.append(0)\n",
    "\n",
    "    ##### Generate Kmer Counts\n",
    "    k_len = len(kmerlist[0])\n",
    "    seq_kmer_dict = {}\n",
    "    counter = 0\n",
    "    for i,seq in enumerate(seqids):\n",
    "        v = seqs[i]\n",
    "        kmer_counts = dict()\n",
    "        items = []\n",
    "        for item in range(0,(len((v)) - k_len +1)):\n",
    "            items.append(v[item:(item+k_len)])\n",
    "        for j in items:\n",
    "            kmer_counts[j] = kmer_counts.get(j, 0) + 1  \n",
    "        store = []\n",
    "        for i,item in enumerate(kmerlist):\n",
    "            if item in kmer_counts:\n",
    "                store.append(kmer_counts[item])\n",
    "                kmer_totals[i] += kmer_counts[item]\n",
    "            else:\n",
    "                store.append(0)\n",
    "        seq_kmer_dict[seq]= store\n",
    "\n",
    "\n",
    "    #Filter out Non-Training Annotations \n",
    "    Annotation_Counts = {}\n",
    "    total_seqs = len(seq_kmer_dict)\n",
    "    for i,seqid in enumerate(list(seq_kmer_dict)):\n",
    "        x =re.findall(r'\\|(.*?)\\|', seqid)[0]\n",
    "        if x not in Seqs:\n",
    "            del seq_kmer_dict[seqid]\n",
    "        else:\n",
    "            if Seq_Anot[x] not in seq_kmer_dict:\n",
    "                seq_kmer_dict[Seq_Anot[x]] = seq_kmer_dict.pop(seqid)\n",
    "            else:\n",
    "                zipped_lists = zip(seq_kmer_dict.pop(seqid), seq_kmer_dict[Seq_Anot[x]])\n",
    "                seq_kmer_dict[Seq_Anot[x]] = [x + y for (x, y) in zipped_lists]\n",
    "            if Seq_Anot[x] not in Annotation_Counts:\n",
    "                Annotation_Counts[Seq_Anot[x]] = 1\n",
    "            else: \n",
    "                Annotation_Counts[Seq_Anot[x]] += 1\n",
    "\n",
    "    #Construct Kmer Counts Output\n",
    "    Kmer_Counts = pd.DataFrame(seq_kmer_dict.values())        \n",
    "    Kmer_Counts.insert(0,\"Annotations\",Annotation_Counts.values(),True)\n",
    "    Kmer_Counts.insert(1,\"Kmer Count\",(Kmer_Counts[list(Kmer_Counts.columns[1:])].sum(axis=1).to_list()),True)\n",
    "    kmer_totals[0:0] = [0,total_seqs]\n",
    "    colnames = [\"Sequence count\"] + [\"Kmer Count\"] + list(kmerlist)\n",
    "    Kmer_Counts = pd.DataFrame(np.insert(Kmer_Counts.values, 0, values=(kmer_totals), axis=0))\n",
    "    Kmer_Counts.columns = colnames\n",
    "    new_index = [\"Totals\"] + list(Annotation_Counts.keys())\n",
    "    Kmer_Counts.index = new_index\n",
    "    print(\"Counts Data Generated for: \",input_data)\n",
    "\n",
    "\n",
    "    #### Write Output\n",
    "    out_name = \"LearnApp_Tutorial_Files/LEARN/output/learn/kmer-counts-\" + str(input_data)[44:-4] + \".csv\"\n",
    "    Kmer_Counts_out = pa.Table.from_pandas(Kmer_Counts,preserve_index=True)\n",
    "    csv.write_csv(Kmer_Counts_out, out_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228c7834",
   "metadata": {},
   "source": [
    "## Rule 3: Merge\n",
    "In this step, we merge all of the previously generated counts datafiles. While running this pipeline through the command line, we also have the option of merging in a previously generated counts file. This allows for additive kmer count integration for massive project scaling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d099124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "databases merged:  0 \n",
      "\n",
      "databases merged:  1 \n",
      "\n",
      "databases merged:  2 \n",
      "\n",
      "databases merged:  3 \n",
      "\n",
      "databases merged:  4 \n",
      "\n",
      "databases merged:  5 \n",
      "\n",
      "databases merged:  6 \n",
      "\n",
      "databases merged:  7 \n",
      "\n",
      "databases merged:  8 \n",
      "\n",
      "databases merged:  9 \n",
      "\n",
      "databases merged:  10 \n",
      "\n",
      "databases merged:  11 \n",
      "\n",
      "databases merged:  12 \n",
      "\n",
      "databases merged:  13 \n",
      "\n",
      "databases merged:  14 \n",
      "\n",
      "databases merged:  15 \n",
      "\n",
      "databases merged:  16 \n",
      "\n",
      "databases merged:  17 \n",
      "\n",
      "databases merged:  18 \n",
      "\n",
      "databases merged:  19 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_counts = glob(\"LearnApp_Tutorial_Files/LEARN/output/learn/*\")\n",
    "\n",
    "for file_num,f in enumerate(input_counts):\n",
    "    print(\"databases merged: \",file_num,\"\\n\")\n",
    "    Kmer_Counts = pd.read_csv(str(f), index_col=\"__index_level_0__\", header=0, engine=\"pyarrow\")\n",
    "#     print(Kmer_Counts)\n",
    "    if file_num == 0:\n",
    "        running_merge = Kmer_Counts\n",
    "    elif file_num >= 1:\n",
    "        running_merge = (pd.concat([running_merge,Kmer_Counts]).reset_index().groupby('__index_level_0__', sort=False).sum(min_count=1)).fillna(0)\n",
    "\n",
    "running_merge_out = pa.Table.from_pandas(running_merge,preserve_index=True)\n",
    "csv.write_csv(running_merge_out, \"LearnApp_Tutorial_Files/LEARN/output/learn/kmer-counts-total.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279d5576",
   "metadata": {},
   "source": [
    "## Rule 3: Eval_Apply\n",
    "In this step, we find the cosine similarity score between the merged kmer count database and each kmer counts from each sequence in the fasta files. Essentially, we are comparing self-predictions against actual values. This output is used in the next step to calculate confidence scores.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a5e2e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File completed: seq-annotation-scores-UP000315395_2594265.csv\n",
      "File completed: seq-annotation-scores-UP000313849_676201.csv\n",
      "File completed: seq-annotation-scores-UP000319374_2585119.csv\n",
      "File completed: seq-annotation-scores-UP000319088_2600309.csv\n",
      "File completed: seq-annotation-scores-UP000317977_2528013.csv\n",
      "File completed: seq-annotation-scores-UP000319776_92403.csv\n",
      "File completed: seq-annotation-scores-UP000315466_1411316.csv\n",
      "File completed: seq-annotation-scores-UP000310227_2562283.csv\n",
      "File completed: seq-annotation-scores-UP000319209_2594004.csv\n",
      "File completed: seq-annotation-scores-UP000316313_1293412.csv\n",
      "File completed: seq-annotation-scores-UP000319639_2592816.csv\n",
      "File completed: seq-annotation-scores-UP000316827_1981880.csv\n",
      "File completed: seq-annotation-scores-UP000317332_2567861.csv\n",
      "File completed: seq-annotation-scores-UP000316252_2590779.csv\n",
      "File completed: seq-annotation-scores-UP000316154_239.csv\n",
      "File completed: seq-annotation-scores-UP000310017_2583587.csv\n",
      "File completed: seq-annotation-scores-UP000319210_66857.csv\n",
      "File completed: seq-annotation-scores-UP000380825_2584524.csv\n",
      "File completed: seq-annotation-scores-UP000319897_1715348.csv\n",
      "File completed: seq-annotation-scores-UP000319825_1880.csv\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"LearnApp_Tutorial_Files/LEARN/output/eval_apply\"):\n",
    "    os.makedirs(\"LearnApp_Tutorial_Files/LEARN/output/eval_apply\")\n",
    "\n",
    "compare_associations = \"LearnApp_Tutorial_Files/LEARN/output/learn/kmer-counts-total.csv\"\n",
    "annotation = [\"LearnApp_Tutorial_Files/LEARN/annotations/TIGRFAMs_annotation.ann\"]\n",
    "for fa in unzipped:\n",
    "    # this is handled by snakemake but we'll specify it here\n",
    "\n",
    "    ###\n",
    "    base = f'{skm.utils.split_file_ext(fa)[0]}.npz'\n",
    "    output_data = os.path.join(output_dir, \"vector\", base)\n",
    "\n",
    "    fasta = SeqIO.parse(fa, \"fasta\")\n",
    "\n",
    "    # initialize kmerization object\n",
    "    kmer = skm.vectorize.KmerVec(alphabet=config[\"alphabet\"], k=config[\"k\"])\n",
    "\n",
    "    vecs, seqs, ids, lengths = list(), list(), list(), list()\n",
    "\n",
    "    for f in fasta:\n",
    "        vecs.append(kmer.reduce_vectorize(f.seq))\n",
    "        seqs.append(\n",
    "            skm.vectorize.reduce(\n",
    "                f.seq,\n",
    "                alphabet=config[\"alphabet\"],\n",
    "                mapping=skm.alphabet.FULL_ALPHABETS,\n",
    "            )\n",
    "        )\n",
    "        ids.append(f.id)\n",
    "        lengths.append(len(f.seq))\n",
    "\n",
    "\n",
    "    ##### Generate Inputs\n",
    "    Annotation = list()\n",
    "    Kmer_Count_Totals = pd.read_csv(str(compare_associations), index_col=\"__index_level_0__\", header=0, engine=\"c\")\n",
    "    for f in annotation:\n",
    "        Annotation.append(pd.read_table(f))\n",
    "    Seqs = Annotation[0]['id'].tolist()\n",
    "    ANNs = Annotation[0]['TIGRFAMs'].tolist()\n",
    "    Seq_Anot = {}\n",
    "    for i,seqid in enumerate(Seqs):\n",
    "        Seq_Anot[seqid] = ANNs[i]\n",
    "    Seqs = set(Seqs)\n",
    "    ANNs = set(ANNs)\n",
    "#     df, kmerlist = skm.io.load_npz(input.data)\n",
    "    seqids = ids\n",
    "    kmer_totals = []\n",
    "    for item in kmerlist:\n",
    "        kmer_totals.append(0)\n",
    "\n",
    "    ##### Generate Kmer Counts\n",
    "    seq_kmer_dict = {}\n",
    "    counter = 0\n",
    "    k_len = len(kmerlist[0])\n",
    "    for i,seq in enumerate(seqids):\n",
    "        v = seqs[i]\n",
    "        kmer_counts = dict()\n",
    "        items = []\n",
    "        for item in range(0,(len((v)) - k_len +1)):\n",
    "            items.append(v[item:(item+k_len)])\n",
    "        for j in items:\n",
    "            kmer_counts[j] = kmer_counts.get(j, 0) + 1  \n",
    "        store = []\n",
    "        for i,item in enumerate(kmerlist):\n",
    "            if item in kmer_counts:\n",
    "                store.append(kmer_counts[item])\n",
    "                kmer_totals[i] += kmer_counts[item]\n",
    "            else:\n",
    "                store.append(0)\n",
    "        seq_kmer_dict[seq]= store\n",
    "\n",
    "\n",
    "    ###### ADD Known / Unknown tag to mark for confidence assessment\n",
    "    Annotation_Counts = {}\n",
    "    total_seqs = len(seq_kmer_dict)\n",
    "    count = 0\n",
    "    for seqid in list(seq_kmer_dict):\n",
    "        x =re.findall(r'\\|(.*?)\\|', seqid)[0]\n",
    "        if x not in Seqs:\n",
    "            seq_kmer_dict[(x+\"_unknown_\"+str(count))] = seq_kmer_dict.pop(seqid)\n",
    "        else: \n",
    "            seq_kmer_dict[(Seq_Anot[x] + \"_known_\" + str(count))] = seq_kmer_dict.pop(seqid)\n",
    "        count +=1\n",
    "    Annotation_Counts = {}\n",
    "    total_seqs = len(seq_kmer_dict)\n",
    "\n",
    "    ######  Construct Kmer Counts Dataframe\n",
    "    Kmer_Counts = pd.DataFrame(seq_kmer_dict.values())        \n",
    "    Kmer_Counts.insert(0,\"Annotations\",1,True)\n",
    "    kmer_totals.insert(0,total_seqs)\n",
    "    Kmer_Counts = pd.DataFrame(np.insert(Kmer_Counts.values, 0, values=kmer_totals, axis=0))\n",
    "    Kmer_Counts.columns = [\"Sequence count\"] + list(kmerlist)\n",
    "    Kmer_Counts.index = [\"Totals\"] + list(seq_kmer_dict.keys())\n",
    "\n",
    "\n",
    "    ##### Make New Counts Data match Kmer Counts Totals Format\n",
    "    if len(str(Kmer_Counts.columns.values[10])) == len(str(Kmer_Count_Totals.columns.values[10])):\n",
    "        compare_check = True\n",
    "    else: \n",
    "        compare_check = False\n",
    "    if compare_check == True:\n",
    "        check_1 = len(Kmer_Counts.columns.values)\n",
    "        alphabet_initial = set(itertools.chain(*[list(x) for x in Kmer_Counts.columns.values[10:check_1]]))\n",
    "        alphabet_compare = set(itertools.chain(*[list(x) for x in Kmer_Count_Totals.columns.values[10:check_1]]))\n",
    "        if alphabet_compare == alphabet_initial:\n",
    "            compare_check = True\n",
    "        else: \n",
    "            compare_check = False\n",
    "    if compare_check == False:\n",
    "        print(\"Compare Check Failed. \")\n",
    "        sys.exit()\n",
    "\n",
    "    new_cols = set(Kmer_Counts.columns)\n",
    "    compare_cols = set(Kmer_Count_Totals.columns)\n",
    "    add_to_compare = []\n",
    "    add_to_new = []\n",
    "    for val in new_cols:\n",
    "        if val not in compare_cols:\n",
    "            add_to_compare.append(val)\n",
    "    for val in compare_cols:\n",
    "        if val not in new_cols:\n",
    "            add_to_new.append(val)\n",
    "\n",
    "    Kmer_Count_Totals = pd.concat([Kmer_Count_Totals, pd.DataFrame(dict.fromkeys(add_to_compare, 0), index=Kmer_Count_Totals.index)], axis=1)\n",
    "    Kmer_Count_Totals.drop(columns=Kmer_Count_Totals.columns[:2], index=\"Totals\", axis=0, inplace=True)\n",
    "    Kmer_Counts = pd.concat([Kmer_Counts, pd.DataFrame(dict.fromkeys(add_to_new, 0), index=Kmer_Counts.index)], axis=1)\n",
    "    Kmer_Counts.drop(columns=Kmer_Counts.columns[-1:].union(Kmer_Counts.columns[:1]), index=\"Totals\", axis=0, inplace=True)\n",
    "\n",
    "    #Perform Cosine Similarity between Kmer Counts Totals and Counts and Sums DF\n",
    "    cosine_df = sklearn.metrics.pairwise.cosine_similarity(Kmer_Count_Totals,Kmer_Counts).T\n",
    "    final_matrix_with_scores = pd.DataFrame(cosine_df, columns=Kmer_Count_Totals.index, index=Kmer_Counts.index)\n",
    "\n",
    "    #Write Output\n",
    "    out_name = \"LearnApp_Tutorial_Files/LEARN/output/eval_apply/seq-annotation-scores-\" + str(fa)[36:-6] + \".csv\"\n",
    "\n",
    "    final_matrix_with_scores_write = pa.Table.from_pandas(final_matrix_with_scores)\n",
    "    csv.write_csv(final_matrix_with_scores_write, out_name)\n",
    "    print(\"File completed: seq-annotation-scores-\" + str(fa)[36:-6] + \".csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c666673e",
   "metadata": {},
   "source": [
    "## Rule 4: Eval_Conf\n",
    "In this step, we evalate if the cosine scores between kmer counts dataframes accurately predict the annotation. The ratio of true positive to false positives is taken and we generate our global confidence scores.  Each delta will be mapped to a confidence score. **Delta** is defined as the difference between the two highest cosine similarity scores. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70fd3155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframes joined:  1  out of  20 .\n",
      "Dataframes joined:  2  out of  20 .\n",
      "Dataframes joined:  3  out of  20 .\n",
      "Dataframes joined:  4  out of  20 .\n",
      "Dataframes joined:  5  out of  20 .\n",
      "Dataframes joined:  6  out of  20 .\n",
      "Dataframes joined:  7  out of  20 .\n",
      "Dataframes joined:  8  out of  20 .\n",
      "Dataframes joined:  9  out of  20 .\n",
      "Dataframes joined:  10  out of  20 .\n",
      "Dataframes joined:  11  out of  20 .\n",
      "Dataframes joined:  12  out of  20 .\n",
      "Dataframes joined:  13  out of  20 .\n",
      "Dataframes joined:  14  out of  20 .\n",
      "Dataframes joined:  15  out of  20 .\n",
      "Dataframes joined:  16  out of  20 .\n",
      "Dataframes joined:  17  out of  20 .\n",
      "Dataframes joined:  18  out of  20 .\n",
      "Dataframes joined:  19  out of  20 .\n",
      "Dataframes joined:  20  out of  20 .\n",
      "\n",
      "Global Confidence scores mapped to Delta:\n",
      " Delta\n",
      "0.00    0.380282\n",
      "0.01    0.709163\n",
      "0.02    0.869565\n",
      "0.03    0.935065\n",
      "0.04    0.981752\n",
      "          ...   \n",
      "0.96    1.000000\n",
      "0.97    1.000000\n",
      "0.98    1.000000\n",
      "0.99    1.000000\n",
      "1.00    1.000000\n",
      "Length: 101, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"LearnApp_Tutorial_Files/LEARN/output/eval_conf\"):\n",
    "    os.makedirs(\"LearnApp_Tutorial_Files/LEARN/output/eval_conf\")\n",
    "\n",
    "\n",
    "eval_apply_data = glob(\"LearnApp_Tutorial_Files/LEARN/output/eval_apply/seq-annotation-scores-*\")\n",
    "        #### Generate Input Data\n",
    "for j,f in enumerate(eval_apply_data):\n",
    "    seq_ann_scores = pd.read_csv(f, index_col=\"__index_level_0__\", header=0, engine=\"c\")\n",
    "    max_value_index = seq_ann_scores.idxmax(axis=\"columns\")\n",
    "    result = max_value_index.keys()\n",
    "    TF = list()\n",
    "    Known = list()\n",
    "    for i,item in enumerate(list(max_value_index)):\n",
    "        if item in result[i]:\n",
    "            TF.append(\"T\")\n",
    "        else:\n",
    "            TF.append(\"F\")\n",
    "        if \"unknown\" in result[i]:\n",
    "            Known.append(\"Unknown\")\n",
    "        else:\n",
    "            Known.append(\"Known\")\n",
    "\n",
    "    seq_ann_vals = seq_ann_scores.values\n",
    "    seq_ann_vals = seq_ann_scores.values[np.arange(len(seq_ann_scores))[:,None],np.argpartition(-seq_ann_vals,np.arange(2),axis=1)[:,:2]]\n",
    "\n",
    "    diff_df = pd.DataFrame(seq_ann_vals, columns = ['Top','Second'])\n",
    "    diff_df['Delta'] = -(np.diff(seq_ann_vals, axis=1).round(decimals=2))\n",
    "    diff_df['Prediction'] = list(max_value_index)\n",
    "    diff_df['Actual'] = result\n",
    "    diff_df[\"T/F\"] = TF\n",
    "    diff_df[\"Known/Unknown\"] = Known\n",
    "\n",
    "    #### Create CrossTabs - ie True/False Count Sums and sum within .01 intervals\n",
    "    known_true_diff_df = diff_df[(diff_df[\"Known/Unknown\"] == \"Known\") & (diff_df[\"T/F\"] == \"T\")]\n",
    "    known_false_diff_df = diff_df[(diff_df[\"Known/Unknown\"] == \"Known\") & (diff_df[\"T/F\"] == \"F\")]\n",
    "    possible_vals = [round(x * 0.01,2) for x in range(0, 101)]\n",
    "    true_crosstab = pd.crosstab(known_true_diff_df.Prediction,known_true_diff_df.Delta)\n",
    "    false_crosstab = pd.crosstab(known_false_diff_df.Prediction,known_false_diff_df.Delta)\n",
    "\n",
    "    if j == 0:\n",
    "        true_running_crosstab = true_crosstab\n",
    "        false_running_crosstab = false_crosstab\n",
    "    else:\n",
    "        true_running_crosstab = (pd.concat([true_running_crosstab,true_crosstab]).reset_index().groupby('Prediction', sort=False).sum(min_count=1)).fillna(0)\n",
    "        false_running_crosstab = (pd.concat([false_running_crosstab,false_crosstab]).reset_index().groupby('Prediction', sort=False).sum(min_count=1)).fillna(0)\n",
    "\n",
    "\n",
    "    add_to_true_df = pd.DataFrame(0, index = sorted(set(false_running_crosstab.index) - set(true_running_crosstab.index)), columns= true_running_crosstab.columns)\n",
    "    add_to_false_df = pd.DataFrame(0, index = sorted(set(true_running_crosstab.index) - set(false_running_crosstab.index)), columns= false_running_crosstab.columns)\n",
    "\n",
    "    true_running_crosstab = pd.concat([true_running_crosstab,add_to_true_df])[sorted(list(set(possible_vals) & set(true_running_crosstab.columns)))].assign(**dict.fromkeys(list(map(str, sorted(list(set(possible_vals) ^ set(true_running_crosstab.columns.astype(float)))))),0))\n",
    "    false_running_crosstab = pd.concat([false_running_crosstab,add_to_false_df])[sorted(list(set(possible_vals) & set(false_running_crosstab.columns)))].assign(**dict.fromkeys(list(map(str, sorted(list(set(possible_vals) ^ set(false_running_crosstab.columns.astype(float)))))),0))\n",
    "\n",
    "    true_running_crosstab.index.names = ['Prediction']\n",
    "    false_running_crosstab.index.names = ['Prediction']\n",
    "    true_running_crosstab.sort_index(inplace=True) \n",
    "    false_running_crosstab.sort_index(inplace=True) \n",
    "    true_running_crosstab.columns = true_running_crosstab.columns.astype(float)\n",
    "    false_running_crosstab.columns = false_running_crosstab.columns.astype(float)\n",
    "    true_running_crosstab = true_running_crosstab[sorted(true_running_crosstab.columns)]\n",
    "    false_running_crosstab = false_running_crosstab[sorted(false_running_crosstab.columns)]\n",
    "\n",
    "\n",
    "    print(\"Dataframes joined: \", j+1, \" out of \",len(eval_apply_data) , \".\")\n",
    "\n",
    "#### Generate Each Global CrossTab\n",
    "ratio_running_crosstab = (true_running_crosstab/(true_running_crosstab + false_running_crosstab))\n",
    "true_total_dist = true_running_crosstab.sum(numeric_only=True, axis=0)\n",
    "false_total_dist = false_running_crosstab.sum(numeric_only=True, axis=0)\n",
    "ratio_total_dist = (true_running_crosstab.sum(numeric_only=True, axis=0)/(true_running_crosstab.sum(numeric_only=True, axis=0) + false_running_crosstab.sum(numeric_only=True, axis=0)))\n",
    "\n",
    "####Interpolate For final Ratio, this only will affect upper limit values if there is a decent amount of data\n",
    "ratio_total_dist = ratio_total_dist.interpolate(method=\"linear\")\n",
    "\n",
    "##### Write Final Confidence Results\n",
    "ratio_total_dist.to_csv(\"LearnApp_Tutorial_Files/LEARN/output/eval_conf/global-confidence-scores.csv\")\n",
    "csv.write_csv(pa.Table.from_pandas(true_running_crosstab), \"LearnApp_Tutorial_Files/LEARN/output/eval_conf/true-total.csv\")\n",
    "csv.write_csv(pa.Table.from_pandas(false_running_crosstab), \"LearnApp_Tutorial_Files/LEARN/output/eval_conf/false-total.csv\")\n",
    "csv.write_csv(pa.Table.from_pandas(ratio_running_crosstab), \"LearnApp_Tutorial_Files/LEARN/output/eval_conf/confidence-matrix.csv\")\n",
    "\n",
    "print(\"\\nGlobal Confidence scores mapped to Delta:\\n\", ratio_total_dist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf215f60",
   "metadata": {},
   "source": [
    "## Learn Pipeline is done.\n",
    "\n",
    "Key outputs include:  \n",
    "* Kmer counts database: /output/learn/kmer-counts-total.csv  \n",
    "* Global confidence scores: output/eval_conf/global-confidence-scores.csv  \n",
    "\n",
    "The next step is to prepare for the Apply pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12b48a0",
   "metadata": {},
   "source": [
    "## Intermediate Steps\n",
    "\n",
    "Users will have to extract key outputs and copy them into a new directory to run the Apply pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a370faba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LearnApp_Tutorial_Files/APPLY/confidence/global_confidence_scores.csv'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not os.path.exists(\"LearnApp_Tutorial_Files/APPLY/counts\"):\n",
    "    os.makedirs(\"LearnApp_Tutorial_Files/APPLY/counts\")\n",
    "\n",
    "if not os.path.exists(\"LearnApp_Tutorial_Files/APPLY/confidence\"):\n",
    "    os.makedirs(\"LearnApp_Tutorial_Files/APPLY/confidence\")\n",
    "    \n",
    "shutil.copyfile(\"LearnApp_Tutorial_Files/LEARN/output/learn/kmer-counts-total.csv\", \"LearnApp_Tutorial_Files/APPLY/counts/kmer-counts-total.csv\")\n",
    "shutil.copyfile(\"LearnApp_Tutorial_Files/LEARN/output/eval_conf/global-confidence-scores.csv\", \"LearnApp_Tutorial_Files/APPLY/confidence/global-confidence-scores.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c0aced",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Getting Started with APPLY\n",
    "\n",
    "### Setup\n",
    "\n",
    "\n",
    "Before running Snekmer Apply, verify that files have been placed in an **_input_** directory placed at the same level as the **_config.yaml_** file. The assumed file directory structure is illustrated below.\n",
    "\n",
    "    .\n",
    "    ├── input\n",
    "    │   ├── W.fasta\n",
    "    │   ├── X.fasta\n",
    "    │   ├── Y.fasta\n",
    "    │   ├── Z.fasta\n",
    "    │   └── etc.\n",
    "    ├── config.yaml\n",
    "    ├── counts\n",
    "    │   └── kmer-counts-total.csv\n",
    "    └── confidence\n",
    "        └── global-confidence-scores.csv\n",
    "     \n",
    "        \n",
    "    \n",
    "Note: Snekmer automatically creates the **_output_** directory when creating output files, so there is no need to create this folder in advance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb45f440",
   "metadata": {},
   "source": [
    "# Running Snekmer Apply Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cc4ce7",
   "metadata": {},
   "source": [
    "## Rule 0.5: Unzip files\n",
    "\n",
    "Any zipped files detected by the above are automatically unzipped. The zipped version of the file is copied into a separate subdirectory.\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60b5cf28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaco059/miniconda3/envs/snekmer_kmer_association/lib/python3.10/site-packages/numpy/lib/npyio.py:713: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  val = np.asanyarray(val)\n"
     ]
    }
   ],
   "source": [
    "# if any files are gzip compressed, unzip them\n",
    "for uz in UZS:\n",
    "    input_ = os.path.join(input_dir, f\"{uz}.{UZ_MAP[uz]}.gz\")\n",
    "    output_ = os.path.join(input_dir, f\"{uz}.{UZ_MAP[uz]}\")\n",
    "    outdir = os.path.join(input_dir, \"zipped\")\n",
    "    \n",
    "    ! mkdir -p $outdir && gunzip -c $input_ > $output_ && mv $input_ $outdir/.\n",
    "\n",
    "    print(\"input:\\t\", input_)\n",
    "    print(\"output:\\t\", output_)\n",
    "    \n",
    "\n",
    "### kmerize/vectorize\n",
    "for fa in unzipped:\n",
    "    # this is handled by snakemake but we'll specify it here\n",
    "    base = f'{skm.utils.split_file_ext(fa)[0]}.kmers'\n",
    "    output_kmerobj = os.path.join(output_dir, \"kmerize\", base)\n",
    "    if not os.path.exists(os.path.join(output_dir, \"kmerize\")):\n",
    "        os.mkdir(os.path.join(output_dir, \"kmerize\"))\n",
    "        \n",
    "    base = f'{skm.utils.split_file_ext(fa)[0]}.npz'\n",
    "    output_data = os.path.join(output_dir, \"vector\", base)\n",
    "    if not os.path.exists(os.path.join(output_dir, \"vector\")):\n",
    "        os.mkdir(os.path.join(output_dir, \"vector\"))\n",
    "    \n",
    "    fasta = SeqIO.parse(fa, \"fasta\")\n",
    "\n",
    "    # initialize kmerization object\n",
    "    kmer = skm.vectorize.KmerVec(alphabet=config[\"alphabet\"], k=config[\"k\"])\n",
    "\n",
    "    vecs, seqs, ids, lengths = list(), list(), list(), list()\n",
    "    for f in fasta:\n",
    "        vecs.append(kmer.reduce_vectorize(f.seq))\n",
    "        seqs.append(\n",
    "            skm.vectorize.reduce(\n",
    "                f.seq,\n",
    "                alphabet=config[\"alphabet\"],\n",
    "                mapping=skm.alphabet.FULL_ALPHABETS,\n",
    "            )\n",
    "        )\n",
    "        ids.append(f.id)\n",
    "        lengths.append(len(f.seq))\n",
    "\n",
    "    # save seqIO output and transformed vecs\n",
    "    np.savez_compressed(output_data, ids=ids, seqs=seqs, vecs=vecs, lengths=lengths)\n",
    "\n",
    "    with open(output_kmerobj, \"wb\") as f:\n",
    "        pickle.dump(kmer, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844b5fc0",
   "metadata": {},
   "source": [
    "## Rule 1: Preprocess\n",
    "\n",
    "In this step, we parse user-defined parameters into an appropriate format for subsequent pipeline steps.\n",
    "\n",
    "Parameter options include:\n",
    "- `k`: Define kmer length\n",
    "- `alphabet`: Define the translation alphabet\n",
    "\n",
    "Note: This essentially the same step as in Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d71dd4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zipped files:\t []\n",
      "unzipped files:\t ['LearnApp_Tutorial_Files/APPLY/input/UP000481030_1602942.fasta', 'LearnApp_Tutorial_Files/APPLY/input/UP000480297_2681465.fasta', 'LearnApp_Tutorial_Files/APPLY/input/UP000480288_2708300.fasta', 'LearnApp_Tutorial_Files/APPLY/input/UP000482960_1076125.fasta', 'LearnApp_Tutorial_Files/APPLY/input/UP000483286_2682977.fasta']\n",
      "output directory:\t LearnApp_Tutorial_Files/APPLY/output\n"
     ]
    }
   ],
   "source": [
    "# collect all fasta-like files, unzipped filenames, and basenames\n",
    "input_dir = \"LearnApp_Tutorial_Files/APPLY/input/\"\n",
    "input_files = glob(os.path.join(input_dir, \"*\"))\n",
    "zipped = [fa for fa in input_files if fa.endswith(\".gz\")]\n",
    "unzipped = [\n",
    "    fa.rstrip(\".gz\")\n",
    "    for fa, ext in itertools.product(input_files, config[\"input\"][\"file_extensions\"])\n",
    "    if fa.rstrip(\".gz\").endswith(f\".{ext}\")\n",
    "]\n",
    "\n",
    "print(\"zipped files:\\t\", zipped)\n",
    "print(\"unzipped files:\\t\", unzipped)\n",
    "# define output directory (and create if missing)\n",
    "output_dir = \"LearnApp_Tutorial_Files/APPLY/output\"\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(\"output directory:\\t\", output_dir)\n",
    "\n",
    "# validity check\n",
    "skm.alphabet.check_valid(config[\"alphabet\"])  # raises error if invalid alphabet\n",
    "# if any files are gzip compressed, unzip them\n",
    "for uz in UZS:\n",
    "    input_ = os.path.join(input_dir, f\"{uz}.{UZ_MAP[uz]}.gz\")\n",
    "    output_ = os.path.join(input_dir, f\"{uz}.{UZ_MAP[uz]}\")\n",
    "    outdir = os.path.join(input_dir, \"zipped\")\n",
    "    \n",
    "    ! mkdir -p $outdir && gunzip -c $input_ > $output_ && mv $input_ $outdir/.\n",
    "\n",
    "    print(\"input:\\t\", input_)\n",
    "    print(\"output:\\t\", output_)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c88ae1",
   "metadata": {},
   "source": [
    "## Rule 2: Apply\n",
    "In this step, we find the cosine similarity score between the kmer-counts-total.csv and the kmer count vector from each sequence in the fasta files. Essentially, we are comparing new kmer count frequencies against the trained model/dataframe. \n",
    "\n",
    " \n",
    "This compares each new sequence against every sequence have in the trained model.   \n",
    "\n",
    "Optional output: \n",
    "* Specify learnapp parameter 'save_results' = True in the config file.\n",
    "* This will output a matrix of all cosine similarity scores. \n",
    "* **Warning**: these files may take up a lot of storage space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5d6c72d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing...\n",
      "finished...\n",
      "making kmer list\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaco059/miniconda3/envs/snekmer_kmer_association/lib/python3.10/site-packages/pandas/core/indexes/base.py:5055: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  result = getitem(key)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Prediction     Score  delta  Confidence\n",
      "SeqID                                                                 \n",
      "tr|A0A6L3UVZ1|A0A6L3UVZ1_9BACI  TIGR01782  0.159632   0.00    0.380282\n",
      "tr|A0A6L3UWN7|A0A6L3UWN7_9BACI  TIGR00231  0.201725   0.00    0.380282\n",
      "tr|A0A6L3UX61|A0A6L3UX61_9BACI  TIGR00254  0.268196   0.01    0.709163\n",
      "tr|A0A6L3UXS8|A0A6L3UXS8_9BACI  TIGR02937  0.235456   0.00    0.380282\n",
      "tr|A0A6L3UXV5|A0A6L3UXV5_9BACI  TIGR02937  0.218722   0.00    0.380282\n",
      "...                                   ...       ...    ...         ...\n",
      "tr|A0A6L3VDZ3|A0A6L3VDZ3_9BACI  TIGR00384  0.213473   0.00    0.380282\n",
      "tr|A0A6L3VET6|A0A6L3VET6_9BACI  TIGR01780  0.117717   0.00    0.380282\n",
      "tr|A0A6L3VG18|A0A6L3VG18_9BACI  TIGR00914  0.138133   0.00    0.380282\n",
      "tr|A0A6L3VGG9|A0A6L3VGG9_9BACI  TIGR04018  0.236016   0.02    0.869565\n",
      "tr|A0A6L3VI01|A0A6L3VI01_9BACI  TIGR01007  0.173710   0.01    0.709163\n",
      "\n",
      "[4971 rows x 4 columns]\n",
      "parsing...\n",
      "finished...\n",
      "making kmer list\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaco059/miniconda3/envs/snekmer_kmer_association/lib/python3.10/site-packages/pandas/core/indexes/base.py:5055: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  result = getitem(key)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Prediction     Score  delta  Confidence\n",
      "SeqID                                                                 \n",
      "tr|A0A6J4F4E3|A0A6J4F4E3_9PROT  TIGR00653  0.529945   0.27    1.000000\n",
      "tr|A0A6J4F4W2|A0A6J4F4W2_9PROT  TIGR04490  0.165330   0.03    0.935065\n",
      "tr|A0A6J4F5K9|A0A6J4F5K9_9PROT  TIGR00229  0.225698   0.00    0.380282\n",
      "tr|A0A6J4F5U0|A0A6J4F5U0_9PROT  TIGR00362  0.401043   0.15    0.993228\n",
      "tr|A0A6J4F758|A0A6J4F758_9PROT  TIGR00326  0.234519   0.01    0.709163\n",
      "...                                   ...       ...    ...         ...\n",
      "tr|A0A6J4G3Q6|A0A6J4G3Q6_9PROT  TIGR02937  0.266368   0.01    0.709163\n",
      "tr|A0A6J4G4E3|A0A6J4G4E3_9PROT  TIGR00426  0.126779   0.00    0.380282\n",
      "tr|A0A6J4G4E9|A0A6J4G4E9_9PROT  TIGR04131  0.200287   0.00    0.380282\n",
      "tr|A0A6J4G4I0|A0A6J4G4I0_9PROT  TIGR01049  0.709616   0.57    1.000000\n",
      "tr|A0A6J4G4Q6|A0A6J4G4Q6_9PROT  TIGR01965  0.372966   0.00    0.380282\n",
      "\n",
      "[4587 rows x 4 columns]\n",
      "parsing...\n",
      "finished...\n",
      "making kmer list\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaco059/miniconda3/envs/snekmer_kmer_association/lib/python3.10/site-packages/pandas/core/indexes/base.py:5055: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  result = getitem(key)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Prediction     Score  delta  Confidence\n",
      "SeqID                                                                 \n",
      "tr|A0A6M0HIY6|A0A6M0HIY6_9RHIZ  TIGR04183  0.204168   0.00    0.380282\n",
      "tr|A0A6M0HJ66|A0A6M0HJ66_9RHIZ  TIGR01643  0.192836   0.00    0.380282\n",
      "tr|A0A6M0HJN7|A0A6M0HJN7_9RHIZ  TIGR01133  0.171233   0.00    0.380282\n",
      "tr|A0A6M0HJS1|A0A6M0HJS1_9RHIZ  TIGR04183  0.192684   0.00    0.380282\n",
      "tr|A0A6M0HK20|A0A6M0HK20_9RHIZ  TIGR01643  0.234667   0.00    0.380282\n",
      "...                                   ...       ...    ...         ...\n",
      "tr|A0A6M0HXZ3|A0A6M0HXZ3_9RHIZ  TIGR01066  0.146006   0.03    0.935065\n",
      "tr|A0A6M0HYD7|A0A6M0HYD7_9RHIZ  TIGR01730  0.172446   0.00    0.380282\n",
      "tr|A0A6M0HYM5|A0A6M0HYM5_9RHIZ  TIGR04131  0.140279   0.01    0.709163\n",
      "tr|A0A6M0HZ22|A0A6M0HZ22_9RHIZ  TIGR00460  0.147662   0.00    0.380282\n",
      "tr|A0A6M0HZC6|A0A6M0HZC6_9RHIZ  TIGR00757  0.174282   0.03    0.935065\n",
      "\n",
      "[4688 rows x 4 columns]\n",
      "parsing...\n",
      "finished...\n",
      "making kmer list\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaco059/miniconda3/envs/snekmer_kmer_association/lib/python3.10/site-packages/pandas/core/indexes/base.py:5055: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  result = getitem(key)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Prediction     Score  delta  Confidence\n",
      "SeqID                                                                 \n",
      "tr|A0A6V8KMR1|A0A6V8KMR1_9ACTN  TIGR01188  0.336461   0.15    0.993228\n",
      "tr|A0A6V8KMT8|A0A6V8KMT8_9ACTN  TIGR01830  0.319477   0.08    0.992126\n",
      "tr|A0A6V8KMV4|A0A6V8KMV4_9ACTN  TIGR00229  0.227885   0.02    0.869565\n",
      "tr|A0A6V8KNR0|A0A6V8KNR0_9ACTN  TIGR04183  0.265409   0.01    0.709163\n",
      "tr|A0A6V8KQ77|A0A6V8KQ77_9ACTN  TIGR00231  0.223534   0.01    0.709163\n",
      "...                                   ...       ...    ...         ...\n",
      "tr|A0A6V8L4S8|A0A6V8L4S8_9ACTN  TIGR03654  0.375962   0.18    1.000000\n",
      "tr|A0A6V8LEJ2|A0A6V8LEJ2_9ACTN  TIGR00229  0.165081   0.01    0.709163\n",
      "tr|A0A6V8LEV2|A0A6V8LEV2_9ACTN  TIGR00594  0.211811   0.01    0.709163\n",
      "tr|A0A6V8LMI1|A0A6V8LMI1_9ACTN  TIGR02601  0.181719   0.00    0.380282\n",
      "tr|A0A6V8LMV5|A0A6V8LMV5_9ACTN  TIGR04057  0.177651   0.00    0.380282\n",
      "\n",
      "[10307 rows x 4 columns]\n",
      "parsing...\n",
      "finished...\n",
      "making kmer list\n",
      "done\n",
      "                               Prediction     Score  delta  Confidence\n",
      "SeqID                                                                 \n",
      "tr|A0A7C9HPG1|A0A7C9HPG1_9DEIO  TIGR01103  0.165495   0.00    0.380282\n",
      "tr|A0A7C9HPT0|A0A7C9HPT0_9DEIO  TIGR01643  0.256224   0.01    0.709163\n",
      "tr|A0A7C9HPW6|A0A7C9HPW6_9DEIO  TIGR00710  0.188671   0.01    0.709163\n",
      "tr|A0A7C9HPZ0|A0A7C9HPZ0_9DEIO  TIGR04057  0.179737   0.01    0.709163\n",
      "tr|A0A7C9HQ49|A0A7C9HQ49_9DEIO  TIGR04183  0.175288   0.00    0.380282\n",
      "...                                   ...       ...    ...         ...\n",
      "tr|A0A7C9MSJ6|A0A7C9MSJ6_9DEIO  TIGR02666  0.296769   0.08    0.992126\n",
      "tr|A0A7C9MSX7|A0A7C9MSX7_9DEIO  TIGR02794  0.307149   0.15    0.993228\n",
      "tr|A0A7C9MSY8|A0A7C9MSY8_9DEIO  TIGR00697  0.190779   0.00    0.380282\n",
      "tr|A0A7C9MTR8|A0A7C9MTR8_9DEIO  TIGR01643  0.185385   0.00    0.380282\n",
      "tr|A0A7C9MTY3|A0A7C9MTY3_9DEIO  TIGR02601  0.272447   0.01    0.709163\n",
      "\n",
      "[4347 rows x 4 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaco059/miniconda3/envs/snekmer_kmer_association/lib/python3.10/site-packages/pandas/core/indexes/base.py:5055: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  result = getitem(key)\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"LearnApp_Tutorial_Files/APPLY/output/apply\"):\n",
    "    os.makedirs(\"LearnApp_Tutorial_Files/APPLY/output/apply\")\n",
    "\n",
    "confidence_associations = \"LearnApp_Tutorial_Files/APPLY/confidence/global-confidence-scores.csv\"\n",
    "compare_associations = \"LearnApp_Tutorial_Files/APPLY/counts/kmer-counts-total.csv\"\n",
    "for fa in unzipped:\n",
    "    # this is handled by snakemake but we'll specify it here\n",
    "\n",
    "    ###\n",
    "    base = f'{skm.utils.split_file_ext(fa)[0]}.npz'\n",
    "    output_data = os.path.join(output_dir, \"vector\", base)\n",
    "\n",
    "    print(\"parsing...\")\n",
    "    fasta = SeqIO.parse(fa, \"fasta\")\n",
    "    print(\"finished...\")\n",
    "\n",
    "    # initialize kmerization object\n",
    "    kmer = skm.vectorize.KmerVec(alphabet=config[\"alphabet\"], k=config[\"k\"])\n",
    "\n",
    "    vecs, seqs, ids, lengths = list(), list(), list(), list()\n",
    "\n",
    "    for f in fasta:\n",
    "        vecs.append(kmer.reduce_vectorize(f.seq))\n",
    "        seqs.append(\n",
    "            skm.vectorize.reduce(\n",
    "                f.seq,\n",
    "                alphabet=config[\"alphabet\"],\n",
    "                mapping=skm.alphabet.FULL_ALPHABETS,\n",
    "            )\n",
    "        )\n",
    "        ids.append(f.id)\n",
    "        lengths.append(len(f.seq))\n",
    "\n",
    "    print(\"making kmer list\")\n",
    "    df, kmerlist = skm.vectorize.make_feature_matrix(vecs)\n",
    "    print(\"done\")\n",
    "    \n",
    "    kmer_count_totals = pd.read_csv(str(compare_associations), index_col=\"__index_level_0__\", header=0, engine=\"c\")\n",
    "    seqids = ids\n",
    "    kmer_totals = []\n",
    "    for item in kmerlist:\n",
    "        kmer_totals.append(0)\n",
    "    k_len = len(kmerlist[0])\n",
    "\n",
    "    ##### Generate Kmer Counts\n",
    "    seq_kmer_dict = {}\n",
    "    counter = 0\n",
    "    for i,seq in enumerate(seqids):\n",
    "        v = seqs[i]\n",
    "        kmer_counts = dict()\n",
    "        items = []\n",
    "        for item in range(0,(len((v)) - k_len +1)):\n",
    "            items.append(v[item:(item+k_len)])\n",
    "        for j in items:\n",
    "            kmer_counts[j] = kmer_counts.get(j, 0) + 1  \n",
    "        store = []\n",
    "        for i,item in enumerate(kmerlist):\n",
    "            if item in kmer_counts:\n",
    "                store.append(kmer_counts[item])\n",
    "                kmer_totals[i] += kmer_counts[item]\n",
    "            else:\n",
    "                store.append(0)\n",
    "        seq_kmer_dict[seq]= store\n",
    "\n",
    "\n",
    "    ######  Construct Kmer Counts Dataframe\n",
    "    total_seqs = len(seq_kmer_dict)\n",
    "    kmer_counts = pd.DataFrame(seq_kmer_dict.values())        \n",
    "    kmer_counts.insert(0,\"Annotations\",1,True)\n",
    "    kmer_totals.insert(0,total_seqs)\n",
    "    kmer_counts = pd.DataFrame(np.insert(kmer_counts.values, 0, values=kmer_totals, axis=0))\n",
    "    kmer_counts.columns = [\"Sequence count\"] + list(kmerlist)\n",
    "    kmer_counts.index = [\"Totals\"] + list(seq_kmer_dict.keys())\n",
    "\n",
    "    new_associations = kmer_counts.iloc[1:, 1:].div(kmer_counts[\"Sequence count\"].tolist()[1:], axis = \"rows\")\n",
    "\n",
    "    ##### Make Kmer Counts Dataframe match Kmer Counts Totals Format\n",
    "    if len(str(kmer_counts.columns.values[10])) == len(str(kmer_count_totals.columns.values[10])):\n",
    "        compare_check = True\n",
    "    else: \n",
    "        compare_check = False\n",
    "    if compare_check == True:\n",
    "        check_1 = len(new_associations.columns.values)\n",
    "        check_2 = len(kmer_count_totals.columns.values)\n",
    "        alphabet_initial = set(itertools.chain(*[list(x) for x in kmer_counts.columns.values[10:check_1]]))\n",
    "        alphabet_compare = set(itertools.chain(*[list(x) for x in kmer_count_totals.columns.values[10:check_1]]))\n",
    "        if alphabet_compare == alphabet_initial:\n",
    "            compare_check = True\n",
    "    if compare_check == False:\n",
    "        print(\"Compare Check Failed. \")\n",
    "        sys.exit()\n",
    "\n",
    "    new_cols = set(kmer_counts.columns)\n",
    "    compare_cols = set(kmer_count_totals.columns)\n",
    "    add_to_compare = []\n",
    "    add_to_new = []\n",
    "    for val in new_cols:\n",
    "        if val not in compare_cols:\n",
    "            add_to_compare.append(val)\n",
    "    for val in compare_cols:\n",
    "        if val not in new_cols:\n",
    "            add_to_new.append(val)\n",
    "\n",
    "    kmer_count_totals = pd.concat([kmer_count_totals, pd.DataFrame(dict.fromkeys(add_to_compare, 0), index=kmer_count_totals.index)], axis=1)\n",
    "    kmer_count_totals.drop(columns=kmer_count_totals.columns[:2], index=\"Totals\", axis=0, inplace=True)\n",
    "    kmer_counts = pd.concat([kmer_counts, pd.DataFrame(dict.fromkeys(add_to_new, 0), index=kmer_counts.index)], axis=1)\n",
    "    kmer_counts.drop(columns=kmer_counts.columns[-1:].union(kmer_counts.columns[:1]), index=\"Totals\", axis=0, inplace=True)\n",
    "\n",
    "\n",
    "    #### Perform Cosine Similarity between Kmer Counts Totals and Counts and Sums DF\n",
    "    cosine_df = sklearn.metrics.pairwise.cosine_similarity(kmer_count_totals,kmer_counts).T\n",
    "    kmer_count_totals = pd.DataFrame(cosine_df, columns=kmer_count_totals.index, index=kmer_counts.index)\n",
    "\n",
    "\n",
    "\n",
    "    ##### Create True Output\n",
    "    # Protein ID, Prediction, Score, delta, Confidence\n",
    "    global_confidence_scores = pd.read_csv(str(confidence_associations))\n",
    "    global_confidence_scores.index= global_confidence_scores[global_confidence_scores.columns[0]]\n",
    "    global_confidence_scores = global_confidence_scores.iloc[: , 1:]\n",
    "    global_confidence_scores = global_confidence_scores[global_confidence_scores.columns[0]].squeeze()\n",
    "\n",
    "    score_rank =[]\n",
    "    sorted_vals = np.argsort(-kmer_count_totals.values, axis=1)[:, :2]\n",
    "    for i,item in enumerate(sorted_vals):\n",
    "        score_rank.append((kmer_count_totals[kmer_count_totals.columns[[item]]][i:i+1]).values.tolist()[0])\n",
    "\n",
    "    delta = []\n",
    "    top_score = []\n",
    "    for score in score_rank:\n",
    "        delta.append(score[0] - score[1])\n",
    "        top_score.append(score[0])\n",
    "\n",
    "    vals = pd.DataFrame({'delta':delta})\n",
    "    predictions = pd.DataFrame(np.array(kmer_count_totals.columns)[sorted_vals][:, :1])\n",
    "\n",
    "    score = pd.DataFrame(top_score)\n",
    "    score.columns = [\"Score\"]\n",
    "    predictions.columns = [\"Prediction\"]\n",
    "    predictions = predictions.astype(str)\n",
    "    vals = vals.round(decimals=2)\n",
    "    vals['Confidence'] = vals[\"delta\"].map(global_confidence_scores)\n",
    "\n",
    "    results = pd.concat([predictions,score,vals],axis=1)\n",
    "    results.index=kmer_count_totals.index\n",
    "    results.index.names = ['SeqID']\n",
    "\n",
    "    #### Write results \n",
    "    out_name_2 = \"LearnApp_Tutorial_Files/APPLY/output/apply/kmer-summary-\" + str(fa)[36:-6] + \".csv\"\n",
    "#     results.reset_index(inplace=True)\n",
    "    results_write = pa.Table.from_pandas(results)\n",
    "    \n",
    "    csv.write_csv(results_write, out_name_2)\n",
    "    print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e1f311",
   "metadata": {},
   "source": [
    "# Apply Pipeline is done.\n",
    "\n",
    "Output is located in /output/apply/kmer-summary-{input file name}.csv.\n",
    "\n",
    "Each output file has 5 columns.\n",
    "* **SeqID**: ID of the sequence whose annotation we are trying to predict.  \n",
    "* **Prediction**: The predicted annotatation for the sequence.  \n",
    "* **Score**: The cosine similarity score between the sequence and the predicted annotation.  \n",
    "* **Delta**: The difference of cosine similarity scores between the top two predicted values.  \n",
    "* **Confidence**: The estimated confidence of the prediction. This is based on the global distribution. Confidence will be more accurate for annotations with more training sequences.  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
