configfile: "config.yaml"

# imports
import glob
# import os
import sys
# import gzip
import random
# import copy

import kmerfeatures as kmf

# from types import *
from Bio import SeqIO
from os.path import (basename, join, splitext)
from Util.SIEVEInit import get_alphabets


# pull config params
params = config['kmer']
files = glob.glob(join(config['input']['fasta_dir'], "*.fasta"))
FNS = [splitext(basename(f))[0] for f in files]


# rules
rule all:
    input:
        expand(join(config['output']['save_dir'], "features", "{fn}.txt"), fn=FNS)
        # expand(join(config['output']['save_dir'], "score"))  # eventually

if config['kmer']['walk']:
    rule perform_kmer_walk:
        input:
            fasta=join(config['input']['fasta_dir'], "{fn}.fasta")
        output:
            # need to fix code to properly generate an output...
        run:
            kmf.walk.kmer_walk(input.fasta)


rule preprocess:
    input:
        fasta=join(config['input']['fasta_dir'], "{fn}.fasta")
    output:
        example_index=join(config['output']['save_dir'], "processed", "{fn}", "example_index.pkl"),
        map_function=join(config['output']['save_dir'], "processed", "{fn}", "map_function.pkl"),
        filter=join(config['output']['save_dir'], "processed", "{fn}", "filter.pkl")
    run:
        # pull all config parameters
        #   sidequest: organize all config parameters by relevant function

        # read fasta file
        seq_list, id_list = kmf.utils.read_fasta(input.fasta,
                                                 include_map=False)

        # optional indexfile with IDs of good feature output examples
        if config['input']['example_indexfile']:
            example_index = kmf.utils.read_example_index(
                config['input']['example_indexfile']
                )
        else:
            example_index = {}

        # feature_sets = []  # what

        # if random alphabet specified, implement randomization
        if config['input']['kmer']['randomize_alphabet']:
            # this = []  # what
            rand_alphabet = kmf.transform.randomize_alphabet(map_function)
            map_function = [residues, map_name, rand_alphabet]

        # if no user-specified save name, use same name as fasta
        if config['output']['output_filebase'] is None:
            features_output_filebase = wildcards.fn

        # if no feature set is specified, define feature space
        if config['output']['feature_set'] is None:
            # prefilter fasta to cut down on the size of feature set
            filter_dict = kmf.features.define_feature_space(
                sequence_dict=sequence_dict,
                kmer=config['input']['kmer']['k'],
                map_function=map_function,
                start=config['input']['kmer']['start'],
                end=config['input']['kmer']['end'],
                min_rep_thresh=config['input']['kmer']['min_rep_thresh'],
                verbose=config['output']['verbose']
                )
        else:
            # read in list of ids to use from file; NO FORMAT CHECK
            filter_list = []
            with open(feature_set, "r") as f:
                for line in f.readlines():
                      filter_list.append(line.split()[0])



rule generate_features:
    input:
        fasta=join(config['input']['fasta_dir'], "processed", "{fn}.?")
    output:
        features=join(config['output']['save_dir'], "features", "{fn}.txt")
    run:
        # this will catch multiples of the same identifier- but may
        # result in problems if there's more than one of the same
        # identifier (i.e. user's files might be messy this way)
        seen = []
        i = 0  # what
        first = True
        for i in range(len(seq_list)):
            seq = seq_list[i]
            seq_id = id_list[i]

            if config['output']['filter_duplicates'] and seq_id in seen:
                continue
            seen.append(seq_id)

            sequences = [seq,]
            ids = [seq_id,]


            # shuffle the N-terminal sequence N times and run SIEVE on the wt and each shuffled sequence
            if shuffle_n:
                example_index[id] = 1.0
                scid_list, scramble_list, example_index = scramble_sequence(id=seq_id, sequence=seq[:30], n=shuffle_n, example_index=example_index)
                sequences += scramble_list
                ids += scid_list

                if output_shuffled_sequences:
                    filename = "%s_shuffled.fasta" % seq_id
                    with open(filename, "w") as f:
                        for i in range(len(ids)):
                            seq_id = ids[i]
                            seq = sequences[i]
                            f.write(">%s\n%s\n" % (seq_id, seq))

            if n_terminal_file:
                addid_list, addseq_list = make_n_terminal_fusions(id=seq_id, filename=n_terminal_file)
                sequences += addseq_list
                ids += addid_list

            residues = None
            if nucleotide:
                residues = "ACGT"


            labels = []
            for j in range(len(sequences)):
                sequence = sequences[j]
                seq_id = ids[j]

                if verbose:
                    print("Constructing features for sequence %s" % seq_id)

                features = [seq_id,]

                features += string_vectorize(sequence=sequence, kmer=kmer, start=start, end=end, map_function=map_function, residues=residues, filter_list=filter_list,
                                              kmer_output=kmer_output)
                if first:
                    labels += string_vectorize(return_labels=True, kmer=kmer, start=start, end=end, map_function=map_function, residues=residues, filter_list=filter_list)
                    if features_output_format == "simple":
                        output_features(format="matrix", output_filename=features_output_filebase, labels=labels)

                first = False
                i += 1

                #print(features)

                if features_output_format == "simple":
                    # we'll output as we go- this is especially good for very large input files
                    output_features(feature_sets=[features,], format="matrix", output_filename=features_output_filebase, write_mode="a")

                # we'll output sieve patterns as we go to provide a record
                if features_output_format in ("sieve", "both"):
                    output_features(feature_sets=[features,], format="sieve", output_filename=features_output_filebase,
                                    example_index=example_index, write_mode="a")

                if features_output_format != "simple":
                    # we only keep this if we are not already dumping each line to an output file
                    feature_sets.append(features)



rule score_features:
    input:
    output:
    run:

# to fix
rule main:
    input:
        join()  # in: some fasta file
    output:
        join()  # out: not yet clear to me
    run:





        # handle.close()  # what

        if features_output_format != "simple":
            output_features(feature_sets=feature_sets, format=features_output_format, output_filename=features_output_filebase, example_index=example_index, labels=labels)
