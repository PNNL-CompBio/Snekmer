# snakemake config
# ruleorder: generate_labels > generate_features

# imports
import glob
import itertools
import json
import random
import sys

import kmerfeatures as kmf
import numpy as np

# from types import *
from Bio import SeqIO
from os import makedirs
from os.path import (basename, dirname, exists, join, splitext)
# from sieveinit import get_alphabets


# collect all fasta-like files and unzipped version of filenames
exts = ['.fasta', '.fna', '.faa', '.fa']
all_files = glob.glob(join(config['input']['fasta_dir'], "*"))
compressed = [fa for fa in all_files if fa.endswith('.gz')]
files = [
    fa.rstrip('.gz') for fa, ext in itertools.product(all_files, exts)
    if fa.rstrip('.gz').endswith(ext)
    ]

UNZIPPED = [splitext(basename(f))[0] for f in compressed]
FAS = [splitext(basename(f))[0] for f in files]
RECURS_DSETS = ['sequences', 'ids', 'residues']
NONRECURS_DSETS = ['map_function', 'example_index', 'filter_list']
DSETS = RECURS_DSETS + NONRECURS_DSETS

# validity check
if config['kmer']['map_function'] not in kmf.ALPHABETS.keys():
    raise ValueError("Invalid alphabet specified; alphabet must be a"
                     " string in the form 'reduced_alphabet_n' where"
                     " n is an integer between 0 and"
                     f" {len(kmf.ALPHABETS)}.")


# subset files for scoring assessment
if config['score']['multifile']['random']:
    assert config['score']['multifile']['random'] > 0, "Must specify >0 files"
    assert config['score']['multifile']['random'] <= len(FAS), "Length out of range"
    indices = np.random.choice([i for i in range(len(FAS))],
                               size=config['score']['multifile']['random'],
                               replace=False)
else:
    if str(config['score']['multifile']['end']) == 'None':
        i_end = len(FAS)
    else:
        i_end = config['score']['multifile']['end']
        assert i_end > config['score']['multifile']['start'], "Invalid range"
        assert i_end <= len(FAS), "Index out of range"
    indices = [i for i in range(config['score']['multifile']['start'],
                                i_end)
               if i not in list(config['score']['multifile']['skip'])]
SCOREFAS = [sorted(FAS)[i] for i in indices]


# define workflow
rule all:
    input:
        expand(join(config['input']['fasta_dir'], '{uz}'), uz=UNZIPPED),
        expand(join(config['output']['save_dir'], "features", "{fa}.txt"), fa=FAS),
        expand(join(config['output']['save_dir'], "score", "features", "{sfa}.json"), sfa=SCOREFAS)
        # expand(join(config['output']['save_dir'], "labels", "{fa}.txt"), )
        # expand(join(config['output']['save_dir'], "score"))  # eventually


# [in-progress] kmer walk
if config['input']['walk']:
    rule perform_kmer_walk:
        input:
            fasta=join(config['input']['fasta_dir'], "{fa}.fasta")
        output:
            # need to fix code to properly generate an output...
        run:
            kmf.walk.kmer_walk(input.fasta)


# if any files are gzip compressed, unzip them
rule unzip:
    input:
        join(config['input']['fasta_dir'], '{uz}.gz')
    output:
        join(config['input']['fasta_dir'], '{uz}')
    params:
        outdir=join(config['input']['fasta_dir'], 'compressed')
    shell:
        "mkdir {params.outdir} && cp {input} {params.outdir} && gunzip {input}"


rule preprocess:
    input:
        fasta=join(config['input']['fasta_dir'], "{fa}.fasta")
    output:
        data=join(config['output']['save_dir'], "processed", "{fa}.json"),
        desc=join(config['output']['save_dir'], "processed", "{fa}.pkl")
    log:
        join(config['output']['save_dir'], "processed", "log", "{fa}.log")
    run:
        # read fasta file
        seq_list, id_list = kmf.utils.read_fasta(input.fasta)

        # if random alphabet specified, implement randomization
        if config['kmer']['randomize_alphabet']:
            rand_alphabet = kmf.transform.randomize_alphabet(config['input']['map_function'])
            map_function = [residues, map_name, rand_alphabet]
        else:
            map_function = config['kmer']['map_function']

        # if no feature set is specified, define feature space
        if not config['input']['feature_set']:
            # prefilter fasta to cut down on the size of feature set
            filter_dict = kmf.features.define_feature_space(
                {k: v for k, v in zip(id_list, seq_list)},
                config['kmer']['k'],
                map_function=map_function,
                start=config['kmer']['start'],
                end=config['kmer']['end'],
                min_rep_thresh=config['kmer']['min_rep_thresh'],
                verbose=config['output']['verbose'],
                log_file=log[0]
                )
            filter_list = list(filter_dict.keys())
            assert len(filter_list) > 0, "Invalid feature space; terminating."

        else:
            # read in list of ids to use from file; NO FORMAT CHECK
            filter_list = []
            with open(config['input']['feature_set'], "r") as f:
                for line in f.readlines():
                    filter_list.append(line.split()[0])

        # optional indexfile with IDs of good feature output examples
        if config['input']['example_index_file']:
            example_index = kmf.utils.read_example_index(
                config['input']['example_index_file']
                )
        else:
            example_index = {}

        # loop thru seqs, apply input params to preprocess seq list
        seen = []  # filter duplicates
        save_data = dict()

        for i in range(len(seq_list)):
            seq = seq_list[i]
            sid = id_list[i]

            # ignore duplicate ids
            if config['output']['filter_duplicates'] and sid in seen:
                continue
            seen.append(sid)

            seqs = [seq]
            sids = [sid]

            # shuffle the N-terminal sequence n times
            if config['output']['shuffle_n']:
                example_index[id] = 1.0
                scid_list, scramble_list, example_index = kmf.transform.scramble_sequence(
                    sid, seq[:30], n=config['output']['shuffle_n'],
                    example_index=example_index
                    )
                seqs += scramble_list
                sids += scid_list

                # include shuffled sequences in output
                if config['output']['shuffle_sequences']:
                    filename = join(config['output']['save_dir'], 'shuffled',
                                    wildcards.fa, "%s_shuffled.fasta" % sid)
                    if not exists(dirname(filename)):
                        makedirs(dirname(filename))
                    with open(filename, "w") as f:
                        for i in range(len(sids)):
                            f.write(">%s\n%s\n" % (sids[i], seqs[i]))

            # run SIEVE on the wt and each shuffled sequence
            if config['output']['n_terminal_file']:
                sids_n, seqs_n = kmf.transform.make_n_terminal_fusions(
                    sid, config['output']['n_terminal_file']
                    )
                seqs += seqs_n
                sids += sids_n
            residues = None
            if config['kmer']['nucleotide']:
                residues = "ACGT"

            # populate dictionary for json save file
            to_save = [seqs, sids, residues]
            save_label = RECURS_DSETS
            for dset, label in zip(to_save, save_label):
                if label in save_data.keys() and save_data[label] is not None:
                    save_data[label] = save_data[label] + dset
                else:
                    save_data[label] = dset

        # save variables not generated in loop
        for dset, label in zip([map_function, example_index, filter_list],
                               NONRECURS_DSETS):
            save_data[label] = dset

        # save all parameters into json
        with open(output.data, 'w') as f:
            json.dump(save_data, f)

        # read and save fasta descriptions into dataframe
        desc = kmf.utils.parse_fasta_description(input.fasta)
        if desc is not None:
            desc.to_pickle(output.desc)
        else:
            pd.DataFrame([]).to_pickle(output.desc)


rule generate_kmer_labels:
    input:
        params=rules.preprocess.output.data
    output:
        labels=join(config['output']['save_dir'], "labels", "{fa}.txt")
    log:
        join(config['output']['save_dir'], "labels", "log", "{fa}.log")
    run:
        # read processed features
        with open(input.params, 'r') as f:
            params = json.load(f)

        # generate labels only
        labels = kmf.transform.generate_labels(
            config['kmer']['k'],
            map_function=params['map_function'],
            residues=params['residues'],
            filter_list=params['filter_list']
            )
        if config['output']['format'] == "simple":
            kmf.features.output_features(output.labels,
                                         "matrix",
                                         labels=labels)

rule generate_kmer_features:
    input:
        params=rules.preprocess.output.data,
        labels=rules.generate_kmer_labels.output.labels
    output:
        features=join(config['output']['save_dir'], "features", "{fa}.txt")
    log:
        join(config['output']['save_dir'], "features", "log", "{fa}.log")
    run:
        # read processed features
        with open(input.params, 'r') as f:
            params = json.load(f)

        # apply user-specified save name, if it exists
        # if config['output']['filename'] is None:
        #     output_file = wildcards.fa

        # generate features for each sequence and output features
        first = True
        for i in range(len(params['sequences'])):
            seq = params['sequences'][i]
            seq_id = params['ids'][i]

            # labels = []

            if config['output']['verbose']:
                with open(log[0], 'a') as f:
                    f.write(f"Constructing features for sequence {seq_id}\n")

            features = [seq_id]

            features += kmf.transform.vectorize_string(
                seq,
                k=config['kmer']['k'],
                start=config['kmer']['start'],
                end=config['kmer']['end'],
                map_function=params['map_function'],
                filter_list=kmf.utils.get_output_kmers(input.labels),  # params['filter_list'],
                verbose=config['output']['verbose'],
                log_file=log[0]
                )

            # print(features)

            # record labels for first sequence only
            if first:
                labels = kmf.utils.get_output_kmers(input.labels)
                # labels += kmf.transform.generate_labels(
                #     config['kmer']['k'],
                #     map_function=params['map_function'],
                #     residues=params['residues'],
                #     filter_list=params['filter_list']
                #     )
                if config['output']['format'] == "simple":
                    kmf.features.output_features(output.features,
                                                 "matrix",
                                                 labels=labels)

            first = False

            # print(features)

            # output as we go (esp. good for very large input files)
            if config['output']['format'] == "simple":
                kmf.features.output_features(
                    output.features, "matrix", feature_sets=[features],
                    mode="a"
                    )

            # output sieve patterns as we go to provide a record
            if config['output']['format'] in ("sieve", "both"):
                kmf.features.output_features(
                    output.features, "sieve", feature_sets=[features],
                    mode="a", example_index=params['example_index']
                    )

            # only append features if not dumping into file
            if config['output']['format'] != "simple":
                feature_sets.append(features)
                kmf.features.output_features(
                    output.features,
                    config['output']['format'],
                    feature_sets=feature_sets,
                    example_index=params['example_index'],
                    labels=labels)


rule aggregate_kmers:
    input:
        files=expand(join(config['output']['save_dir'], "labels", "{sfa}.txt"),
                     sfa=SCOREFAS)
    output:
        kmers=join(config['output']['save_dir'], "score", "kmers.json")
    log:
        join(config['output']['save_dir'], "score", "log", "kmers.log")
    run:
        # collect all kmers (without duplicates)
        kmers = list()
        for i, filename in enumerate(input.files):
            ks = kmf.utils.get_output_kmers(filename)
            if i == 0:
                kmers = ks
            else:
                kmers += list(set(ks) - set(kmers))

        # if labels are improperly captured, try again
        if len(kmers) == 0:
            with open(log[0], 'a') as f:
                logf.write('no kmers parsed from files')

        # save all parameters into json
        with open(output.kmers, 'w') as f:
            json.dump(kmers, f)

        with open(log[0], 'a') as logf:
            logf.write('input files:\n')
            logf.write(str(input.files))


rule score_features:
    input:
        params=join(config['output']['save_dir'], "processed", "{sfa}.json"),
        kmers=join(config['output']['save_dir'], "score", "kmers.json"),
        fasta=join(config['input']['fasta_dir'], "{sfa}.fasta")
    output:
        features=join(config['output']['save_dir'], "score", "features", "{sfa}.json")
    log:
        join(config['output']['save_dir'], "score", "log", "{sfa}.log")
    run:
        # read processed features
        with open(input.params, 'r') as f:
            params = json.load(f)
        # read aggregated kmers
        with open(input.kmers, 'r') as f:
            kmers = json.load(f)

        # revectorize based on full kmer list
        results = {'seq_id': [], 'vector': []}
        seq_list, id_list = kmf.utils.read_fasta(input.fasta)
        for seq, sid in zip(seq_list, id_list):
            results['seq_id'] += [sid]
            results['vector'] += [kmf.transform.vectorize_string(
                seq,
                k=config['kmer']['k'],
                start=config['kmer']['start'],
                end=config['kmer']['end'],
                map_function=params['map_function'],
                filter_list=kmers,  # params['filter_list'],
                verbose=False,  # way too noisy for batch process
                log_file=log[0]
            )]

        with open(output.features, 'w') as f:
            json.dump(results, f)

        # matrix = kmf.score.connection_matrix_from_features(input[0])
        # clusters = kmf.score.cluster_feature_matrix(matrix)

        # clusters.to_hdf(output[0], 'cluster_matrix')
