{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5163fc1-9f35-4eeb-9912-85a88b096b90",
   "metadata": {},
   "source": [
    "# Snekmer Demo\n",
    "\n",
    "In this notebook, we will demonstrate how to apply Snekmer toward the analysis of protein sequences.\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "### Setup\n",
    "\n",
    "First, install Snekmer using the instructions in the [user installation guide](https://github.com/PNNL-CompBio/Snekmer/).\n",
    "\n",
    "Before running Snekmer, verify that files have been placed in an **_input_** directory placed at the same level as the **_config.yaml_** file. The assumed file directory structure is illustrated below.\n",
    "\n",
    "    .\n",
    "    ├── config.yaml\n",
    "    ├── input\n",
    "    │   ├── background\n",
    "    │   │   ├── X.fasta\n",
    "    │   │   ├── Y.fasta\n",
    "    │   │   └── etc.\n",
    "    │   ├── A.fasta\n",
    "    │   ├── B.fasta\n",
    "    │   └── etc.\n",
    "    ├── output\n",
    "    │   ├── ...\n",
    "    │   └── ...\n",
    "    \n",
    "(Note: Snekmer automatically creates the **_output_** directory when creating output files, so there is no need to create this folder in advance. Additionally, inclusion of background sequences is optional, but is illustrated above for interested users.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c73bc3-4fab-408f-b74c-188c5962996c",
   "metadata": {},
   "source": [
    "### Workflow\n",
    "\n",
    "Snekmer proceeds through a defined workflow executed as individual steps on Snakemake. Two operation modes are available: `model` (supervised machine learning) and `cluster` (unsupervised clustering). The user should select the mode that best suits their individual use case.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/biodataganache/Snekmer/main/resources/snekmer_workflow.png?token=ALAWYHDTGPI2NH3GH2WBMNDBXTR42\" width=\"70%\" height=\"70%\" >\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f80e9b9-ba51-47a7-bbd7-18f65801974e",
   "metadata": {},
   "source": [
    "## Using Snekmer\n",
    "\n",
    "Snekmer assumes that the user will primarily process input files using the command line. For more detailed instructions, refer to the [README](https://github.com/PNNL-CompBio/Snekmer).\n",
    "\n",
    "The basic process for running Snekmer is as follows:\n",
    "\n",
    "1. Verify that your file directory structure is correct and that the top-level directory contains a **_config.yaml_** file.\n",
    "    - A **_config.yaml_** template has been included in the Snekmer codebase at **_resources/config.yaml_**.\n",
    "2. Modify the **_config.yaml_** with the desired parameters.\n",
    "3. Use the command line to navigate to the directory containing both the **_config.yaml_** file and **_input_** directory.\n",
    "4. Run `snekmer cluster` or `snekmer model`.\n",
    "\n",
    "Depending on the selected operation mode, output files will vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0bc7da-6c2e-4c18-80f0-1ddc0110c400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710b4d0d-023e-4c80-853c-f2e018e2b3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21117b2-31c7-4ad0-9641-d7b048b35cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply output models to new sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d422c9b-81de-46fe-8595-43a4d9397c6d",
   "metadata": {},
   "source": [
    "## Manually run through workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ceb779-3ae3-4673-8e27-1b0018541adf",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "To set up the workflow such that operation mimics the command line implementation of Snekmer, we will initialize a dictionary (rather than a YAML file) and gather all input files. Input files are detected here using `glob.glob`, exactly as Snekmer performs input file detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91ec337c-6cec-411c-85bb-310512b5cf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# built-in imports\n",
    "import json\n",
    "import os\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "from itertools import (product, repeat)\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# external libraries\n",
    "import snekmer as skm\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from Bio import SeqIO\n",
    "from snakemake.io import expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "214570fc-0472-4e4e-acac-c458d91fbcbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval('False')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e02ca8d7-755e-497b-9ba9-a36fa663a7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define config (note: in the snakemake workflow,\n",
    "#                this is handled through the yaml file)\n",
    "\n",
    "config = {\n",
    "    \n",
    "    # required parameters\n",
    "    'k': 14,\n",
    "    'alphabet': 0,  # choices 0-5 or names (see snekmer.alphabet),  or None\n",
    "    'min_rep_thresh': 1,\n",
    "    'processes': 2,\n",
    "    \n",
    "    # optional parameters\n",
    "    'start': False,\n",
    "    'end': False,\n",
    "    'nucleotide': False,\n",
    "    'randomize_alphabet': False,\n",
    "    'walk': False,\n",
    "\n",
    "    # input handling\n",
    "    'input': {\n",
    "        'example_index_file': False,\n",
    "        'feature_set': False,\n",
    "        'file_extensions:' ['fasta', 'fna', 'faa', 'fa'],\n",
    "        'regex': r\"[a-z]{3}[A-Z]{1}\",  # regex to parse family from filename\n",
    "    },\n",
    "    \n",
    "    # output handling\n",
    "    'output': {\n",
    "        'nested_dir': False,  # if True, saves into {save_dir}/{alphabet name}/{k}\n",
    "        'verbose': True, # if True, logs verbose outputs\n",
    "        'format': 'simple',  # choices: ['simple', 'gist', 'sieve']\n",
    "        'filter_duplicates': True,\n",
    "        'n_terminal_file': False,\n",
    "        'shuffle_n': False,\n",
    "        'shuffle_sequences': False,\n",
    "    },\n",
    "    \n",
    "    # scoring parameters\n",
    "    'score': {\n",
    "        'scaler': True,\n",
    "        'scaler_kwargs': {'n': 0.25},\n",
    "        'labels': None,\n",
    "        'lname': None  # label name\n",
    "    },\n",
    "    \n",
    "    # model parameters\n",
    "    'model': {\n",
    "        'n': 100,\n",
    "        'cv': 5,\n",
    "        'use_score': False,\n",
    "        'random_state': None\n",
    "    }\n",
    "    \n",
    "    # cluster parameters\n",
    "    'cluster': {\n",
    "        'method': 'correlation',\n",
    "        'params': {'t': 0.75, 'criterion': 'distance', 'metric': 'correlation', 'method': 'weighted'}\n",
    "    }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80e630c-08f6-4110-878c-ccdc2d1bfc5e",
   "metadata": {},
   "source": [
    "## Rule 0: Get files\n",
    "\n",
    "Before going through the workflow, we glob all filenames that end in the pre-defined file extensions and/or the extension and `.gz`. Filename globbing is demonstrated via a directory containing a single FASTA file, `12485-2500Genomes-nrfH_protein.fasta`, and a gzipped FAA file, `AmoA_AOA.faa.gz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a2b1fce-6891-4765-9989-df9c7dbd8821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compressed files:\t ['../input/demo/AmoA_AOA.faa.gz']\n",
      "uncompressed files:\t ['../input/demo/12485-2500Genomes-nrfH_protein.fasta', '../input/demo/AmoA_AOA.faa']\n"
     ]
    }
   ],
   "source": [
    "# collect all fasta-like files, unzipped filenames, and basenames\n",
    "input_files = glob(os.path.join(config['input']['fasta_dir'], \"*\"))\n",
    "compressed = [fa for fa in input_files if fa.endswith('.gz')]\n",
    "uncompressed = [fa.rstrip('.gz') for fa, ext\n",
    "                in product(input_files, config['input']['file_extensions'])\n",
    "                if fa.rstrip('.gz').endswith(f\".{ext}\")]\n",
    "\n",
    "print(\"compressed files:\\t\", compressed)\n",
    "print(\"uncompressed files:\\t\", uncompressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ee17f4-eed3-4eaf-9a5c-61793f748e1a",
   "metadata": {},
   "source": [
    "Next, file paths are stripped of directory paths and extensions into the file base name, known in Snakemake as a file's **wildcard**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf15b7a0-a4f7-448c-9f95-e7cbfd8dca0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compressed filename wildcards:\t\t ['AmoA_AOA']\n",
      "uncompressed filename wildcards:\t ['12485-2500Genomes-nrfH_protein', 'AmoA_AOA']\n"
     ]
    }
   ],
   "source": [
    "# map extensions to basename (basename.ext.gz -> {basename: ext})\n",
    "unzipped_ext_map = {\n",
    "    os.path.splitext(\n",
    "        os.path.splitext(\n",
    "            os.path.basename(f)\n",
    "        )[0]\n",
    "    )[0]: os.path.splitext(\n",
    "        os.path.splitext(\n",
    "            os.path.basename(f))[0]\n",
    "    )[1].lstrip('.') for f in compressed\n",
    "}\n",
    "\n",
    "fasta_ext_map = {\n",
    "    os.path.splitext(os.path.basename(f))[0]: os.path.splitext(os.path.basename(f))[1].lstrip('.')\n",
    "    for f in uncompressed\n",
    "}\n",
    "\n",
    "UZS = list(unzipped_ext_map.keys())\n",
    "FAS = list(fasta_ext_map.keys())\n",
    "\n",
    "print(\"compressed filename wildcards:\\t\\t\", UZS)\n",
    "print(\"uncompressed filename wildcards:\\t\", FAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d6a8fa-0df9-46f5-b099-df5608b4de64",
   "metadata": {},
   "source": [
    "Finally, the output (save) path is defined. (Note: this may change in future versions.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07988445-fb9b-464f-8ed9-fa12638034ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output directory:\t ../output/demo/reduced_alphabet_0/k-14\n"
     ]
    }
   ],
   "source": [
    "# define output directory (helpful for multiple runs)\n",
    "out_dir = os.path.join(config['output']['save_dir'],\n",
    "                       config['map_function'],\n",
    "                       f\"k-{config['k']:02}\")\n",
    "\n",
    "print(\"output directory:\\t\", out_dir)\n",
    "\n",
    "# validity check\n",
    "if config['map_function'] not in kmf.alphabet.ALPHABETS.keys():\n",
    "    raise ValueError(\"Invalid alphabet specified; alphabet must be a\"\n",
    "                     \" string in the form 'reduced_alphabet_n' where\"\n",
    "                     \" n is an integer between 0 and\"\n",
    "                     f\" {len(kmf.alphabet.ALPHABETS)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e90b74-4885-42fc-b973-81f851f6ef30",
   "metadata": {},
   "source": [
    "## Rule 0.5: Unzip files\n",
    "\n",
    "Any zipped files detected by the above are automatically unzipped.\n",
    "\n",
    "**Snakemake code:**\n",
    "\n",
    "    # if any files are gzip compressed, unzip them\n",
    "    if len(UZS) > 0:\n",
    "        rule unzip:\n",
    "            input:\n",
    "                # lambda wildcards: '{0}_hmm'.format(dict[wildcards.run])\n",
    "                lambda wildcards: join(config['input']['fasta_dir'], f\"{wildcards.uz}.{unzipped_ext_map[wildcards.uz]}.gz\")\n",
    "            output:\n",
    "                join(config['input']['fasta_dir'], \"{uz}.{uzext}\")\n",
    "            params:\n",
    "                outdir=join(config['input']['fasta_dir'], 'compressed')\n",
    "            shell:\n",
    "                \"mkdir {params.outdir} && cp {input} {params.outdir} && gunzip -c {input} > {output}\"\n",
    "                \n",
    "To run an analogous version of Snakemake syntax in Python, see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da79b205-637a-40a0-9c07-7aef21a61d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\t ../input/demo/AmoA_AOA.faa.gz\n",
      "output:\t ../input/demo/AmoA_AOA.faa\n"
     ]
    }
   ],
   "source": [
    "# if any files are gzip compressed, unzip them\n",
    "if len(UZS) > 0:\n",
    "    for uz in UZS:\n",
    "        input_ = os.path.join(config['input']['fasta_dir'], f\"{uz}.{unzipped_ext_map[uz]}.gz\")\n",
    "        output_ = os.path.join(config['input']['fasta_dir'], f\"{uz}.{unzipped_ext_map[uz]}\")\n",
    "        outdir = os.path.join(config['input']['fasta_dir'], 'compressed')\n",
    "        ! mkdir $outdir && cp $input_ $outdir && gunzip -c $input_ > $output_\n",
    "        \n",
    "        print(\"input:\\t\", input_)\n",
    "        print(\"output:\\t\", output_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8ec8c1-b187-48cf-ac24-4dc3ba23951a",
   "metadata": {},
   "source": [
    "The above step successfully creates the unzipped output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3444f560-987d-44ff-9619-1533211afb59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exists(\"../input/demo/AmoA_AOA.faa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe5d56f-34f6-47fb-af9d-a6a63f4647e5",
   "metadata": {},
   "source": [
    "## Rule 1: Preprocess\n",
    "\n",
    "In this step, we parse user-defined parameters into an appropriate format for subsequent pipeline steps.\n",
    "\n",
    "Parameter options include:\n",
    "- `filter_duplicates`: Remove duplicate sequences\n",
    "- `map_function`: Define alphabet mappings\n",
    "- `k`: Define kmer length\n",
    "- `min_rep_thresh`: Set threshold for minimum replicates required to include a feature in the final feature space\n",
    "- `shuffle_sequences`: Shuffle sequences\n",
    "    - `shuffle_n`: Define shuffle\n",
    "- `example_index_file`: Use example index file to define feature space\n",
    "- `filter_sequences`: Optionally filter sequences\n",
    "- `start` and `end`: Define sequence start and end points\n",
    "- `nucleotide`: Allow nucleotide mapping\n",
    "\n",
    "The Snakemake code is not shown due to length, but the converted Python-ized code is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a41b8ae-cae1-4b9b-9272-18ca38e0c2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input file:\t ../input/demo/12485-2500Genomes-nrfH_protein.fasta\n",
      "output file:\t ../output/demo/reduced_alphabet_0/k-14/processed/12485-2500Genomes-nrfH_protein.json\n",
      "Feature space: 3336 kmers with more than 1 representation in 67 sequences\n",
      "start time:\t2021-04-02 13:00:22.040372\n",
      "end time:\t2021-04-02 13:00:23.813676\n",
      "total time:\t0h 00m 01.773s\n",
      "\n",
      "input file:\t ../input/demo/AmoA_AOA.faa\n",
      "output file:\t ../output/demo/reduced_alphabet_0/k-14/processed/AmoA_AOA.json\n",
      "Feature space: 202 kmers with more than 1 representation in 1 sequences\n",
      "start time:\t2021-04-02 13:00:23.813914\n",
      "end time:\t2021-04-02 13:00:25.467964\n",
      "total time:\t0h 00m 01.654s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read and process parameters from config\n",
    "for fa in FAS:\n",
    "    \n",
    "    # define i/o for each detected file\n",
    "    input_fasta = os.path.join(config['input']['fasta_dir'], f\"{fa}.{fasta_ext_map[fa]}\")\n",
    "    output_data = os.path.join(out_dir, \"processed\", f\"{fa}.json\")\n",
    "    output_desc = os.path.join(out_dir, \"processed\", f\"{fa}_description.json\")\n",
    "    \n",
    "    print(\"input file:\\t\", input_fasta)\n",
    "    print(\"output file:\\t\", output_data)\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "\n",
    "    # read fasta file\n",
    "    seq_list, id_list = skm.io.read_fasta(input_fasta)\n",
    "\n",
    "    # if random alphabet specified, implement randomization\n",
    "    if config['randomize_alphabet']:\n",
    "        rand_alphabet = skm.transform.randomize_alphabet(config['input']['map_function'])\n",
    "        map_function = [residues, map_name, rand_alphabet]\n",
    "    else:\n",
    "        map_function = config['map_function']\n",
    "\n",
    "    # if no feature set is specified, define feature space\n",
    "    if not config['input']['feature_set']:\n",
    "        # prefilter fasta to cut down on the size of feature set\n",
    "        filter_dict = kmf.features.define_feature_space(\n",
    "            {k: v for k, v in zip(id_list, seq_list)},\n",
    "            config['k'],\n",
    "            map_function=map_function,\n",
    "            start=config['start'],\n",
    "            end=config['end'],\n",
    "            min_rep_thresh=config['min_rep_thresh'],\n",
    "            verbose=config['output']['verbose'],\n",
    "            processes=config['processes']\n",
    "            )\n",
    "        filter_list = list(filter_dict.keys())\n",
    "        assert len(filter_list) > 0, \"Invalid feature space; terminating.\"\n",
    "    else:\n",
    "        # read in list of ids to use from file; NO FORMAT CHECK\n",
    "        filter_list = []\n",
    "        with open(config['input']['feature_set'], \"r\") as f:\n",
    "            for line in f.readlines():\n",
    "                filter_list.append(line.split()[0])\n",
    "\n",
    "    # optional indexfile with IDs of good feature output examples\n",
    "    if config['input']['example_index_file']:\n",
    "        example_index = kmf.io.read_example_index(\n",
    "            config['input']['example_index_file']\n",
    "            )\n",
    "    else:\n",
    "        example_index = {}\n",
    "\n",
    "    # loop thru seqs, apply input params to preprocess seq list\n",
    "    seen = []  # filter duplicates\n",
    "    save_data = dict()\n",
    "\n",
    "    # define recursive and nonrecursive saving patterns for params\n",
    "    recursive = ['sequences', 'ids', 'residues']\n",
    "    nonrecursive = ['map_function', 'k', 'example_index', 'filter_list']\n",
    "    all_dsets = recursive + nonrecursive\n",
    "\n",
    "    for i in range(len(seq_list)):\n",
    "        seq = seq_list[i]\n",
    "        sid = id_list[i]\n",
    "\n",
    "        # ignore duplicate ids\n",
    "        if config['output']['filter_duplicates'] and sid in seen:\n",
    "            continue\n",
    "        seen.append(sid)\n",
    "\n",
    "        seqs = [seq]\n",
    "        sids = [sid]\n",
    "\n",
    "        # shuffle the N-terminal sequence n times\n",
    "        if config['output']['shuffle_n']:\n",
    "            example_index[id] = 1.0\n",
    "            scid_list, scramble_list, example_index = kmf.transform.scramble_sequence(\n",
    "                sid, seq[:30], n=config['output']['shuffle_n'],\n",
    "                example_index=example_index\n",
    "                )\n",
    "            seqs += scramble_list\n",
    "            sids += scid_list\n",
    "\n",
    "            # include shuffled sequences in output\n",
    "            if config['output']['shuffle_sequences']:\n",
    "                filename = join(out_dir, 'shuffled',\n",
    "                                wildcards.fa, \"%s_shuffled.fasta\" % sid)\n",
    "                if not exists(dirname(filename)):\n",
    "                    makedirs(dirname(filename))\n",
    "                with open(filename, \"w\") as f:\n",
    "                    for i in range(len(sids)):\n",
    "                        f.write(\">%s\\n%s\\n\" % (sids[i], seqs[i]))\n",
    "\n",
    "        # run SIEVE on the wt and each shuffled sequence\n",
    "        if config['output']['n_terminal_file']:\n",
    "            sids_n, seqs_n = kmf.transform.make_n_terminal_fusions(\n",
    "                sid, config['output']['n_terminal_file']\n",
    "                )\n",
    "            seqs += seqs_n\n",
    "            sids += sids_n\n",
    "        residues = None\n",
    "        if config['nucleotide']:\n",
    "            residues = \"ACGT\"\n",
    "\n",
    "        # populate dictionary for json save file\n",
    "        to_save = [seqs, sids, residues]\n",
    "        save_label = recursive\n",
    "        for dset, label in zip(to_save, save_label):\n",
    "            if label in save_data.keys() and save_data[label] is not None:\n",
    "                save_data[label] = save_data[label] + dset\n",
    "            else:\n",
    "                save_data[label] = dset\n",
    "\n",
    "    # save variables not generated in loop\n",
    "    for dset, label in zip(\n",
    "        [map_function, config['k'], example_index, filter_list],\n",
    "        nonrecursive\n",
    "    ):\n",
    "        save_data[label] = dset\n",
    "\n",
    "    # save all parameters into json\n",
    "    if not exists(join(out_dir, \"processed\")):\n",
    "        makedirs(join(out_dir, \"processed\"))\n",
    "    with open(output_data, 'w') as f:\n",
    "        json.dump(save_data, f)\n",
    "\n",
    "    # read and save fasta descriptions into dataframe\n",
    "    try:\n",
    "        desc = kmf.utils.parse_fasta_description(input_fasta)\n",
    "        desc.to_json(output_desc)\n",
    "    except AttributeError:  # if no description exists > empty df\n",
    "        DataFrame([]).to_json(output_desc)\n",
    "\n",
    "    # record script runtime\n",
    "    end_time = datetime.now()\n",
    "    print(f\"start time:\\t{start_time}\")\n",
    "    print(f\"end time:\\t{end_time}\")\n",
    "    print(f\"total time:\\t{kmf.utils.format_timedelta(end_time - start_time)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7acf72b0-c55e-437e-bd37-2641f044bc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# check to see if output files created\n",
    "for fa in FAS:\n",
    "    print(exists(join(out_dir, \"processed\", f\"{fa}.json\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f362a5b-8e0f-42af-9674-92e2ee414ab8",
   "metadata": {},
   "source": [
    "## Rule 2: Generate kmer labels\n",
    "\n",
    "In this step, kmer labels for the kmers in the generated feature space are saved.\n",
    "\n",
    "As previously, the Python-ized version of this Snakemake rule is demonstrated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "914c7f07-a444-4e1d-b664-ffa0115f461e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input file:\t ../output/demo/reduced_alphabet_0/k-14/processed/12485-2500Genomes-nrfH_protein.json\n",
      "output file:\t ../output/demo/reduced_alphabet_0/k-14/labels/12485-2500Genomes-nrfH_protein.txt\n",
      "start time:\t2021-04-02 13:02:19.851811\n",
      "end time:\t2021-04-02 13:02:19.856593\n",
      "total time:\t0h 00m 00.5s\n",
      "\n",
      "input file:\t ../output/demo/reduced_alphabet_0/k-14/processed/AmoA_AOA.json\n",
      "output file:\t ../output/demo/reduced_alphabet_0/k-14/labels/AmoA_AOA.txt\n",
      "start time:\t2021-04-02 13:02:19.856829\n",
      "end time:\t2021-04-02 13:02:19.858173\n",
      "total time:\t0h 00m 00.1s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fa in FAS:\n",
    "    input_params = join(out_dir, \"processed\", f\"{fa}.json\")\n",
    "    output_labels = join(out_dir, \"labels\", f\"{fa}.txt\")\n",
    "    print(\"input file:\\t\", input_params)\n",
    "    print(\"output file:\\t\", output_labels)\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "\n",
    "    # read processed features\n",
    "    with open(input_params, 'r') as f:\n",
    "        params = json.load(f)\n",
    "        \n",
    "    if not exists(join(out_dir, \"labels\")):\n",
    "        makedirs(join(out_dir, \"labels\"))\n",
    "\n",
    "    # generate labels only\n",
    "    labels = kmf.transform.generate_labels(\n",
    "        config['k'],\n",
    "        map_function=params['map_function'],\n",
    "        residues=params['residues'],\n",
    "        filter_list=params['filter_list']\n",
    "        )\n",
    "    if config['output']['format'] == \"simple\":\n",
    "        kmf.features.output_features(\n",
    "            output_labels, \"matrix\", labels=labels\n",
    "            )\n",
    "\n",
    "    # record script runtime\n",
    "    end_time = datetime.now()\n",
    "    print(f\"start time:\\t{start_time}\")\n",
    "    print(f\"end time:\\t{end_time}\")\n",
    "    print(f\"total time:\\t{kmf.utils.format_timedelta(end_time - start_time)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b9e0832-a9dd-4d53-a6e7-2180aac6a9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# check that output files were created correctly\n",
    "for fa in FAS:\n",
    "    print(exists(join(out_dir, \"labels\", f\"{fa}.txt\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf89b91f-9311-4e7d-b1d5-28b6dfca0c24",
   "metadata": {},
   "source": [
    "## Rule 3: Standardize kmers\n",
    "\n",
    "In this step, the feature spaces for each individual file are used to parse all files in the directory. For instance, the `12485-2500Genomes-nrfH_protein` feature set is used to kmerize all sequences in both `12485-2500Genomes-nrfH_protein` and `AmoA_AOA` sets, and then the `AmoA_AOA` feature set is used to kmerize all sequences in both sequence files.\n",
    "\n",
    "As previously, the Python-ized version of this Snakemake rule is demonstrated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8f31a27-4d18-4342-be4b-85df62fb69dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input file:\t ../output/demo/reduced_alphabet_0/k-14/labels/12485-2500Genomes-nrfH_protein.txt\n",
      "output files:\t ['../output/demo/reduced_alphabet_0/k-14/features/12485-2500Genomes-nrfH_protein/12485-2500Genomes-nrfH_protein.json', '../output/demo/reduced_alphabet_0/k-14/features/12485-2500Genomes-nrfH_protein/AmoA_AOA.json']\n",
      "start time:\t2021-04-02 13:11:35.152273\n",
      "end time:\t2021-04-02 13:11:35.761661\n",
      "total time:\t0h 00m 00.609s\n",
      "\n",
      "input file:\t ../output/demo/reduced_alphabet_0/k-14/labels/AmoA_AOA.txt\n",
      "output files:\t ['../output/demo/reduced_alphabet_0/k-14/features/AmoA_AOA/12485-2500Genomes-nrfH_protein.json', '../output/demo/reduced_alphabet_0/k-14/features/AmoA_AOA/AmoA_AOA.json']\n",
      "start time:\t2021-04-02 13:11:35.763001\n",
      "end time:\t2021-04-02 13:11:35.873444\n",
      "total time:\t0h 00m 00.110s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fa in FAS:\n",
    "    input_kmers = join(out_dir, \"labels\", f\"{fa}.txt\")\n",
    "    input_params = join(out_dir, \"processed\", f\"{fa}.json\")\n",
    "    input_fastas = uncompressed\n",
    "    output_files = expand(join(out_dir, \"features\", f\"{fa}\", \"{fa2}.json\"), fa2=FAS)\n",
    "    print(\"input file:\\t\", input_kmers)\n",
    "    print(\"output files:\\t\", output_files)\n",
    "    \n",
    "    if not exists(join(out_dir, \"features\", f\"{fa}\")):\n",
    "        makedirs(join(out_dir, \"features\", f\"{fa}\"))\n",
    "\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    # get kmers for this particular set of sequences\n",
    "    kmers = kmf.io.read_output_kmers(input_kmers)\n",
    "\n",
    "    # read processed features\n",
    "    with open(input_params, 'r') as f:\n",
    "        params = json.load(f)\n",
    "\n",
    "    # sort i/o lists to match wildcard order\n",
    "    fastas = sorted(input_fastas)\n",
    "    outfiles = sorted(output_files)\n",
    "\n",
    "    # revectorize based on full kmer list\n",
    "    results = {'seq_id': [], 'vector': []}\n",
    "    for i, fa in enumerate(fastas):\n",
    "        seq_list, id_list = kmf.io.read_fasta(fa)\n",
    "        for seq, sid in zip(seq_list, id_list):\n",
    "            results['seq_id'] += [sid]\n",
    "            results['vector'] += [kmf.transform.vectorize_string(\n",
    "                seq,\n",
    "                k=config['k'],\n",
    "                start=config['start'],\n",
    "                end=config['end'],\n",
    "                map_function=params['map_function'],\n",
    "                filter_list=kmers,  # params['filter_list'],\n",
    "                verbose=False,  # way too noisy for batch process\n",
    "            )]\n",
    "\n",
    "        with open(outfiles[i], 'w') as f:\n",
    "            json.dump(results, f)\n",
    "\n",
    "    # record script runtime\n",
    "    end_time = datetime.now()\n",
    "    print(f\"start time:\\t{start_time}\")\n",
    "    print(f\"end time:\\t{end_time}\")\n",
    "    print(f\"total time:\\t{kmf.utils.format_timedelta(end_time - start_time)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d28aff12-1ac9-4450-95e2-d42946ace6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# check that output directories were created correctly\n",
    "for fa in FAS:\n",
    "    print(exists(join(out_dir, \"features\", f\"{fa}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "321c4b6b-0e0d-4599-93fe-0bbb01c64813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../output/demo/reduced_alphabet_0/k-14/features/12485-2500Genomes-nrfH_protein/12485-2500Genomes-nrfH_protein.json', '../output/demo/reduced_alphabet_0/k-14/features/12485-2500Genomes-nrfH_protein/AmoA_AOA.json']\n",
      "['../output/demo/reduced_alphabet_0/k-14/features/AmoA_AOA/12485-2500Genomes-nrfH_protein.json', '../output/demo/reduced_alphabet_0/k-14/features/AmoA_AOA/AmoA_AOA.json']\n"
     ]
    }
   ],
   "source": [
    "# check contents of each output directory\n",
    "for fa in FAS:\n",
    "    print(sorted(glob(join(out_dir, \"features\", f\"{fa}\", \"*\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c400b1c3-754a-4bfc-8386-21a5ea277ddc",
   "metadata": {},
   "source": [
    "## Rule 4: Score features\n",
    "\n",
    "Lastly, in this final step, the files parsed into each feature space are scored for their faithful differentiation of each protein family, and then clustering is performed on the individual sequences. The results are all collated and stored into a Pandas DataFrame (`pandas.DataFrame` object).\n",
    "\n",
    "As previously, the Python-ized version of this Snakemake rule is demonstrated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9f105d7a-b1da-4f72-a43d-372cd7eb551f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input files:\t ['../output/demo/reduced_alphabet_0/k-14/features/12485-2500Genomes-nrfH_protein/12485-2500Genomes-nrfH_protein.json', '../output/demo/reduced_alphabet_0/k-14/features/12485-2500Genomes-nrfH_protein/AmoA_AOA.json']\n",
      "output file:\t ../output/demo/reduced_alphabet_0/k-14/features/12485-2500Genomes-nrfH_protein.json\n",
      "start time:\t2021-04-02 13:18:47.742731\n",
      "vecfiles_to_df time:\t0h 00m 00.66s\n",
      "class_probabilities time:\t0h 00m 00.114s\n",
      "total time:\t0h 00m 00.299s\n",
      "\n",
      "input files:\t ['../output/demo/reduced_alphabet_0/k-14/features/AmoA_AOA/12485-2500Genomes-nrfH_protein.json', '../output/demo/reduced_alphabet_0/k-14/features/AmoA_AOA/AmoA_AOA.json']\n",
      "output file:\t ../output/demo/reduced_alphabet_0/k-14/features/AmoA_AOA.json\n",
      "start time:\t2021-04-02 13:18:48.042871\n",
      "vecfiles_to_df time:\t0h 00m 00.6s\n",
      "class_probabilities time:\t0h 00m 00.11s\n",
      "total time:\t0h 00m 00.23s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fa in FAS:\n",
    "    input_files = expand(join(out_dir, \"features\", f\"{fa}\", \"{fa2}.json\"), fa2=FAS)\n",
    "    output_df = join(out_dir, \"features\", f\"{fa}.json\")\n",
    "    output_scores = join(out_dir, \"score\", f\"{fa}.json\")\n",
    "    print(\"input files:\\t\", input_files)\n",
    "    print(\"output file:\\t\", output_df)\n",
    "    \n",
    "    if not exists(join(out_dir, \"score\")):\n",
    "        makedirs(join(out_dir, \"score\"))\n",
    "        \n",
    "    start_time = datetime.now()\n",
    "    print(f\"start time:\\t{start_time}\")\n",
    "\n",
    "    # parse all data\n",
    "    label = config['score']['lname']\n",
    "    data = kmf.io.vecfiles_to_df(\n",
    "        input_files, labels=config['score']['labels'], label_name=label\n",
    "        )\n",
    "\n",
    "    timepoint = datetime.now()\n",
    "    print(f\"vecfiles_to_df time:\\t{kmf.utils.format_timedelta(timepoint - start_time)}\")\n",
    "\n",
    "    # parse family names and only add if some are valid\n",
    "    families = [kmf.utils.get_family(fn) for fn in data['filename']]\n",
    "    if any(families):\n",
    "        label = 'family'\n",
    "        data[label] = families\n",
    "\n",
    "    # define feature matrix of kmer vectors\n",
    "    feature_matrix = kmf.score.to_feature_matrix(data['vector'].values)\n",
    "\n",
    "    # compute class probabilities\n",
    "    labels = data[label].values\n",
    "    class_probabilities = kmf.score.feature_class_probabilities(\n",
    "        feature_matrix.T, labels, processes=config['processes']\n",
    "        ).drop(columns=['presence'])\n",
    "\n",
    "    new_timepoint = datetime.now()\n",
    "    print(f\"class_probabilities time:\\t{kmf.utils.format_timedelta(new_timepoint - timepoint)}\")\n",
    "\n",
    "    # [IN PROGRESS] compute clusters\n",
    "    clusters = kmf.score.cluster_feature_matrix(feature_matrix)\n",
    "    data['cluster'] = clusters\n",
    "\n",
    "    # save all files to respective outputs\n",
    "    data.to_json(output_df)\n",
    "    # np.save(output.npy, feature_matrix)\n",
    "    class_probabilities.to_json(output_scores)\n",
    "\n",
    "    # record script runtime\n",
    "    end_time = datetime.now()\n",
    "    print(f\"total time:\\t{kmf.utils.format_timedelta(end_time - start_time)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3834bad4-f8a5-4957-8e14-ca47367c4461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# check that output files were created correctly\n",
    "for fa in FAS:\n",
    "    print(exists(join(out_dir, \"features\", f\"{fa}.json\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99dbda6-e2e4-44b1-8282-2947b0f62089",
   "metadata": {},
   "source": [
    "The contents of one of these dataframes is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a0d9fc9c-cefa-48ca-96ba-7ebedbe363c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AmoA_AOA\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>seq_id</th>\n",
       "      <th>vector</th>\n",
       "      <th>vec_shape</th>\n",
       "      <th>family</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12485-2500Genomes-nrfH_protein.json</td>\n",
       "      <td>GWA2_CPR_52_12_gwa2_scaffold_1540_12</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[202]</td>\n",
       "      <td>nrfH</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12485-2500Genomes-nrfH_protein.json</td>\n",
       "      <td>GWA2_Desulfovibrionales_65_9_gwa2_scaffold_456...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[202]</td>\n",
       "      <td>nrfH</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12485-2500Genomes-nrfH_protein.json</td>\n",
       "      <td>GWA2_ELX_61_42_gwa2_scaffold_694_73</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[202]</td>\n",
       "      <td>nrfH</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12485-2500Genomes-nrfH_protein.json</td>\n",
       "      <td>GWA2_ELX_62_23_gwa2_scaffold_270_24</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[202]</td>\n",
       "      <td>nrfH</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12485-2500Genomes-nrfH_protein.json</td>\n",
       "      <td>GWA2_ELX_64_40_gwa2_scaffold_4090_5</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[202]</td>\n",
       "      <td>nrfH</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>AmoA_AOA.json</td>\n",
       "      <td>RIFOXYD12_FULL_Deltaproteobacteria_39_22_rifox...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[202]</td>\n",
       "      <td>AmoA_AOA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>AmoA_AOA.json</td>\n",
       "      <td>RIFOXYD12_FULL_Deltaproteobacteria_50_9_rifoxy...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[202]</td>\n",
       "      <td>AmoA_AOA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>AmoA_AOA.json</td>\n",
       "      <td>RIFOXYD12_FULL_Deltaproteobacteria_57_12_rifox...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[202]</td>\n",
       "      <td>AmoA_AOA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>AmoA_AOA.json</td>\n",
       "      <td>RBG_16_scaffold_36447_curated_1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[202]</td>\n",
       "      <td>AmoA_AOA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>AmoA_AOA.json</td>\n",
       "      <td>WP_075055648.1</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[202]</td>\n",
       "      <td>AmoA_AOA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>135 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                filename  \\\n",
       "0    12485-2500Genomes-nrfH_protein.json   \n",
       "1    12485-2500Genomes-nrfH_protein.json   \n",
       "2    12485-2500Genomes-nrfH_protein.json   \n",
       "3    12485-2500Genomes-nrfH_protein.json   \n",
       "4    12485-2500Genomes-nrfH_protein.json   \n",
       "..                                   ...   \n",
       "130                        AmoA_AOA.json   \n",
       "131                        AmoA_AOA.json   \n",
       "132                        AmoA_AOA.json   \n",
       "133                        AmoA_AOA.json   \n",
       "134                        AmoA_AOA.json   \n",
       "\n",
       "                                                seq_id  \\\n",
       "0                 GWA2_CPR_52_12_gwa2_scaffold_1540_12   \n",
       "1    GWA2_Desulfovibrionales_65_9_gwa2_scaffold_456...   \n",
       "2                  GWA2_ELX_61_42_gwa2_scaffold_694_73   \n",
       "3                  GWA2_ELX_62_23_gwa2_scaffold_270_24   \n",
       "4                  GWA2_ELX_64_40_gwa2_scaffold_4090_5   \n",
       "..                                                 ...   \n",
       "130  RIFOXYD12_FULL_Deltaproteobacteria_39_22_rifox...   \n",
       "131  RIFOXYD12_FULL_Deltaproteobacteria_50_9_rifoxy...   \n",
       "132  RIFOXYD12_FULL_Deltaproteobacteria_57_12_rifox...   \n",
       "133                    RBG_16_scaffold_36447_curated_1   \n",
       "134                                     WP_075055648.1   \n",
       "\n",
       "                                                vector vec_shape    family  \\\n",
       "0    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...     [202]      nrfH   \n",
       "1    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...     [202]      nrfH   \n",
       "2    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...     [202]      nrfH   \n",
       "3    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...     [202]      nrfH   \n",
       "4    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...     [202]      nrfH   \n",
       "..                                                 ...       ...       ...   \n",
       "130  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...     [202]  AmoA_AOA   \n",
       "131  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...     [202]  AmoA_AOA   \n",
       "132  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...     [202]  AmoA_AOA   \n",
       "133  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...     [202]  AmoA_AOA   \n",
       "134  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...     [202]  AmoA_AOA   \n",
       "\n",
       "     cluster  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          0  \n",
       "4          0  \n",
       "..       ...  \n",
       "130        0  \n",
       "131        0  \n",
       "132        0  \n",
       "133        0  \n",
       "134        1  \n",
       "\n",
       "[135 rows x 6 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(fa)\n",
    "\n",
    "import pandas as pd\n",
    "pd.read_json(join(out_dir, \"features\", f\"{fa}.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5768a7-2917-4899-baab-158ee520006b",
   "metadata": {},
   "source": [
    "We can assess how well the vector and the clustering both perform. Note that it is immediately obvious that the `AmoA_AOA` vector has some difficulty clustering the different families separately (cluster label `0` is assigned to both `nrfH` and `AmoA_AOA` sequences).\n",
    "\n",
    "The probabilities and scores assigned to each feature in the kmer set is also computed and output into a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f28d7e96-b798-454a-b55d-216f76dca17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AmoA_AOA\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>n_sequences</th>\n",
       "      <th>probability</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AmoA_AOA</td>\n",
       "      <td>68</td>\n",
       "      <td>[0.0147058824, 0.0147058824, 0.029411764700000...</td>\n",
       "      <td>[0.0147058824, 0.0147058824, 0.0144863916, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nrfH</td>\n",
       "      <td>67</td>\n",
       "      <td>[0.0, 0.0, 0.0149253731, 0.1343283582, 0.13432...</td>\n",
       "      <td>[-0.0147058824, -0.0147058824, -0.0144863916, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label  n_sequences                                        probability  \\\n",
       "0  AmoA_AOA           68  [0.0147058824, 0.0147058824, 0.029411764700000...   \n",
       "1      nrfH           67  [0.0, 0.0, 0.0149253731, 0.1343283582, 0.13432...   \n",
       "\n",
       "                                               score  \n",
       "0  [0.0147058824, 0.0147058824, 0.0144863916, 0.0...  \n",
       "1  [-0.0147058824, -0.0147058824, -0.0144863916, ...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(fa)\n",
    "pd.read_json(join(out_dir, \"score\", f\"{fa}.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93930d8c-57b0-4e83-a5ad-4e3ba5086fcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snekmer",
   "language": "python",
   "name": "snekmer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
