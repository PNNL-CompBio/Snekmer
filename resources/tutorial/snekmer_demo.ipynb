{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "775964cc-b245-40e0-813c-946e1b7fc494",
   "metadata": {},
   "source": [
    "# Snekmer Demo\n",
    "\n",
    "In this notebook, we will demonstrate how to apply Snekmer toward the analysis of protein sequences.\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "### Setup\n",
    "\n",
    "First, install Snekmer using the instructions in the [user installation guide](https://snekmer.readthedocs.io/en/latest/getting_started/install.html).\n",
    "\n",
    "Before running Snekmer, verify that files have been placed in an **_input_** directory placed at the same level as the **_config.yaml_** file. The assumed file directory structure is illustrated below.\n",
    "\n",
    "    .\n",
    "    ├── config.yaml\n",
    "    ├── input\n",
    "    │   ├── background\n",
    "    │   │   ├── X.fasta\n",
    "    │   │   ├── Y.fasta\n",
    "    │   │   └── etc.\n",
    "    │   ├── A.fasta\n",
    "    │   ├── B.fasta\n",
    "    │   └── etc.\n",
    "    ├── output\n",
    "    │   ├── ...\n",
    "    │   └── ...\n",
    "    \n",
    "(Note: Snekmer automatically creates the **_output_** directory when creating output files, so there is no need to create this folder in advance. Additionally, inclusion of background sequences is optional, but is illustrated above for interested users.)\n",
    "\n",
    "To ensure that the tutorial runs correctly, activate the conda environment containing your Snekmer installation and run the notebook from the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7319a320",
   "metadata": {},
   "source": [
    "### Workflow\n",
    "\n",
    "Snekmer proceeds through a defined workflow executed as individual steps on Snakemake. Two operation modes are available: `model` (supervised machine learning) and `cluster` (unsupervised clustering). The user should select the mode that best suits their individual use case.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/PNNL-CompBio/Snekmer/main/resources/snekmer_workflow.svg\" width=\"70%\" height=\"70%\" >\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e180e9fb",
   "metadata": {},
   "source": [
    "### Notes on Using Snekmer\n",
    "\n",
    "Snekmer assumes that the user will primarily process input files using the command line. For more detailed instructions, refer to the [documentation](https://snekmer.readthedocs.io/en/latest/getting_started/cli.html).\n",
    "\n",
    "The basic process for running Snekmer is as follows:\n",
    "\n",
    "1. Verify that your file directory structure is correct and that the top-level directory contains a **config.yaml** file.\n",
    "    - A template configuration file is included in the Snekmer code repository [here](https://github.com/PNNL-CompBio/Snekmer/blob/main/resources/config.yaml).\n",
    "2. Modify **config.yaml** as needed.\n",
    "3. Use the command line to navigate to the directory containing both the **config.yaml** file and **_input_** directory.\n",
    "4. Run `snekmer cluster`, `snekmer model`, or `snekmer search`.\n",
    "\n",
    "Depending on the selected operation mode, output files will vary.\n",
    "\n",
    "In the following demo, we will go through the entire Snekmer workflow for supervised model-building (`snekmer model`). By the end of this demo, users should be familiar with how the code generally operates, how input files lead to output files, and the output for each individual step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abeb36e8",
   "metadata": {},
   "source": [
    "## Running Snekmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c66845",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "To set up the workflow such that operation mimics the command line implementation of Snekmer, we will initialize a dictionary (rather than a YAML file) and gather all input files. Input files are detected here using `glob.glob`, exactly as Snekmer performs input file detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0563ab5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# built-in imports\n",
    "import gzip\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from ast import literal_eval\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "from multiprocessing import Pool\n",
    "from shutil import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# import from external libraries\n",
    "import snekmer as skm\n",
    "from Bio import SeqIO\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from snakemake.io import directory, expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcfc3627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define config\n",
    "# (note: handled via config.yaml in the snekmer CLI workflow)\n",
    "\n",
    "config = {\n",
    "    \n",
    "    # required params\n",
    "    \"k\": 14,\n",
    "    \"alphabet\": 0,  # choices 0-5 or names (see alphabet module), or None\n",
    "    \"min_kmer_threshold\": 0,\n",
    "    \n",
    "    # input/output params\n",
    "    \"input_dir\": None,  # defaults to 'input'\n",
    "    \"input_file_exts\": [\n",
    "        \"fasta\",\n",
    "        \"fna\",\n",
    "        \"faa\",\n",
    "        \"fa\",\n",
    "    ], \n",
    "    \n",
    "    # specify valid input file extensions\n",
    "    \"input_file_regex\": \".*\",  # regex to parse family from filename\n",
    "    \"nested_output\": False,  # if True, saves into {save_dir}/{alphabet name}/{k}\n",
    "    \n",
    "    # scoring params\n",
    "    \"score\": {\n",
    "        \"scaler\": True,\n",
    "        \"scaler_kwargs\": {\"n\": 0.25},\n",
    "        \"labels\": None,\n",
    "        \"lname\": \"family\",  # label name\n",
    "    },\n",
    "    \n",
    "    # cluster params\n",
    "    \"cluster\": {\n",
    "        \"method\": \"correlation\",\n",
    "        \"params\": {\"n_clusters\": None, \"distance_threshold\": 95, \"linkage\": \"ward\"},\n",
    "        \"cluster_plots\": True,\n",
    "    },\n",
    "    \n",
    "    # clustering distance matrix params\n",
    "    \"min_rep\": None,\n",
    "    \"max_rep\": None,\n",
    "    \"save_matrix\": False,\n",
    "    \"dist_thresh\": 100,\n",
    "    \n",
    "    # model params\n",
    "    \"model\": {\"cv\": 5, \"random_state\": None},\n",
    "    \n",
    "    # search params\n",
    "    \"model_dir\": \"output/model/\",\n",
    "    \"basis_dir\": \"output/kmerize/\",\n",
    "    \"score_dir\": \"output/scoring/\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f49d4fe",
   "metadata": {},
   "source": [
    "## Rule 0: Get files\n",
    "\n",
    "Before going through the workflow, we glob all filenames contained within the input directory that end in the pre-defined file extensions and/or the extension and `.gz`.\n",
    "\n",
    "Note that while in this notebook, the path to the demo files is specified with the `input_dir` variable, the Snekmer CLI assumes that input files are stored according to the file structure specified above in the **Setup** section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96eeedb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zipped files:\t ['../../../resources/tutorial/input/NapB.faa.gz']\n",
      "unzipped files:\t ['../../../resources/tutorial/input/cNorB.faa', '../../../resources/tutorial/input/NapB.faa', '../../../resources/tutorial/input/nirS.faa']\n"
     ]
    }
   ],
   "source": [
    "# collect all fasta-like files, unzipped filenames, and basenames\n",
    "input_dir = \"../../../resources/tutorial/input/\"\n",
    "input_files = glob(os.path.join(input_dir, \"*\"))\n",
    "zipped = [fa for fa in input_files if fa.endswith(\".gz\")]\n",
    "unzipped = [\n",
    "    fa.rstrip(\".gz\")\n",
    "    for fa, ext in itertools.product(input_files, config[\"input_file_exts\"])\n",
    "    if fa.rstrip(\".gz\").endswith(f\".{ext}\")\n",
    "]\n",
    "\n",
    "print(\"zipped files:\\t\", zipped)\n",
    "print(\"unzipped files:\\t\", unzipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17052c1",
   "metadata": {},
   "source": [
    "Next, file paths are stripped of directory paths and extensions into the file base name, known in Snakemake as a file\"s **wildcard**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c22c2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zipped filename wildcards:\t ['NapB.faa']\n",
      "unzipped filename wildcards:\t ['cNorB', 'NapB', 'nirS']\n"
     ]
    }
   ],
   "source": [
    "# map extensions to basename (basename.ext.gz -> {basename: ext})\n",
    "UZ_MAP = {\n",
    "    skm.utils.split_file_ext(f)[0]: skm.utils.split_file_ext(f)[1] for f in zipped\n",
    "}\n",
    "\n",
    "FA_MAP = {\n",
    "    skm.utils.split_file_ext(f)[0]: skm.utils.split_file_ext(f)[1] for f in unzipped\n",
    "}\n",
    "\n",
    "UZS = [f\"{f}.{ext}\" for f, ext in UZ_MAP.items()]\n",
    "FAS = list(FA_MAP.keys())\n",
    "\n",
    "print(\"zipped filename wildcards:\\t\", UZS)\n",
    "print(\"unzipped filename wildcards:\\t\", FAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18e6a813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample filename wildcards:\t ['cNorB', 'NapB', 'nirS']\n",
      "background filename wildcards:\t []\n"
     ]
    }
   ],
   "source": [
    "# parse any background files\n",
    "bg_files = glob(os.path.join(input_dir, \"background\", \"*\"))\n",
    "if len(bg_files) > 0:\n",
    "    bg_files = [skm.utils.split_file_ext(basename(f))[0] for f in bg_files]\n",
    "NON_BGS, BGS = [f for f in FAS if f not in bg_files], bg_files\n",
    "\n",
    "print(\"sample filename wildcards:\\t\", NON_BGS)\n",
    "print(\"background filename wildcards:\\t\", BGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae58eef",
   "metadata": {},
   "source": [
    "Finally, the output (save) path is defined. (Note: this may change in future versions.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34ffb745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output directory:\t ../../../resources/tutorial/output\n"
     ]
    }
   ],
   "source": [
    "# define output directory (and create if missing)\n",
    "output_dir = os.path.join(\n",
    "    \"../../../resources/tutorial\",\n",
    "    skm.io.define_output_dir(\n",
    "        config[\"alphabet\"], config[\"k\"], nested=config[\"nested_output\"]\n",
    "    )\n",
    ")\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(\"output directory:\\t\", output_dir)\n",
    "\n",
    "# validity check\n",
    "skm.alphabet.check_valid(config[\"alphabet\"])  # raises error if invalid alphabet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84945355",
   "metadata": {},
   "source": [
    "## Rule 1: Process\n",
    "\n",
    "Files are processed into desirable input format. Any zipped files detected by the above are automatically unzipped. The zipped version of the file is copied into a separate subdirectory.\n",
    "\n",
    "**Snakemake code:**\n",
    "\n",
    "    # if any files are gzip compressed, unzip them\n",
    "    rule unzip:\n",
    "    input:\n",
    "        join(\"input\", \"{uz}.gz\")\n",
    "    output:\n",
    "        join(\"input\", \"{uz}\")\n",
    "    params:\n",
    "        outdir=join(\"input\", \"zipped\")\n",
    "    shell:\n",
    "        \"mkdir {params.outdir} && gunzip -c {input} > {output} && mv {input} {params.outdir}/.\"\n",
    "                \n",
    "To run an analogous version of Snakemake syntax in Python, see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e08be267-4d2f-498e-9987-3fa0ccf3726d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset demo files\n",
    "for uz in UZS:\n",
    "    input_ = os.path.join(input_dir, f\"{uz}.gz\")\n",
    "    output_unzipped = os.path.join(input_dir, f\"{uz}\")\n",
    "    output_zipped = os.path.join(input_dir, \"zipped\", f\"{uz}.gz\")\n",
    "\n",
    "    # copy zipped file back to input folder and delete\n",
    "    copy(output_zipped, input_)\n",
    "    os.remove(output_zipped)\n",
    "    os.remove(output_unzipped)\n",
    "    os.rmdir(os.path.dirname(output_zipped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06b73f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\t ../../../resources/tutorial/input/NapB.faa.gz\n",
      "output:\t ../../../resources/tutorial/input/NapB.faa\n"
     ]
    }
   ],
   "source": [
    "# if any files are gzip compressed, unzip them\n",
    "for uz in UZS:\n",
    "    input_ = os.path.join(input_dir, f\"{uz}.gz\")\n",
    "    output_unzipped = os.path.join(input_dir, f\"{uz}\")\n",
    "    output_zipped = os.path.join(input_dir, \"zipped\", f\"{uz}.gz\")\n",
    "\n",
    "    # preserve zipped file\n",
    "    if not os.path.exists(os.path.dirname(output_zipped)):\n",
    "        os.makedirs(os.path.dirname(output_zipped))\n",
    "    copy(input_, output_zipped)\n",
    "\n",
    "    # unzip and save file contents\n",
    "    with gzip.open(input_, \"rb\") as openf, open(output_unzipped, \"wb\") as savef:\n",
    "        file_content = openf.readlines()\n",
    "        for line in file_content:\n",
    "            savef.write(line)\n",
    "\n",
    "    os.remove(input_)\n",
    "\n",
    "    print(\"input:\\t\", input_)\n",
    "    print(\"output:\\t\", output_unzipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2f0e3c",
   "metadata": {},
   "source": [
    "## Rule 2: Kmerize\n",
    "\n",
    "In this step, we parse user-defined parameters into an appropriate format for subsequent pipeline steps.\n",
    "\n",
    "Parameter options include:\n",
    "- `k`: Define kmer length\n",
    "- `alphabet`: Define the translation alphabet\n",
    "\n",
    "The Snakemake code is not shown due to length, but the converted Python-ized code is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3def6a9-ff65-4559-912e-f39b2ad02f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input files:\t ['../../../resources/tutorial/input/cNorB.faa', '../../../resources/tutorial/input/NapB.faa', '../../../resources/tutorial/input/nirS.faa']\n"
     ]
    }
   ],
   "source": [
    "# verify input files\n",
    "input_fasta = unzipped  # take all unzipped seq files as input\n",
    "\n",
    "print(\"input files:\\t\", unzipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbabb325-048e-4c09-a414-20314e2a337f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_basis(fasta: str, min_filter: int):  # -> NDArray\n",
    "    # read seqs file and initialize kmerization object\n",
    "    fasta = SeqIO.parse(fa, \"fasta\")\n",
    "    kmer = skm.vectorize.KmerVec(alphabet=config[\"alphabet\"], k=config[\"k\"])\n",
    "    \n",
    "    kmer_set = list()\n",
    "    for f in fasta:\n",
    "        kmers = list(kmer.reduce_vectorize(f.seq))\n",
    "        kmer_set.extend(kmers)\n",
    "\n",
    "    # capture common basis set with kmers above threshold\n",
    "    kmer_set = Counter(kmer_set)\n",
    "    kmer_set = {x: count for x, count in kmer_set.items() if count >= config[\"min_kmer_threshold\"]}\n",
    "    return list(kmer_set.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51ff7ee7-9663-4be3-ac9a-c579487c4998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../../resources/tutorial/\")\n",
    "\n",
    "import vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63d227f8-9e35-41ca-bedb-4bb38a70ad7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13131\n"
     ]
    }
   ],
   "source": [
    "# create common kmer set based on kmer prevalence in demo files\n",
    "kmer_set = list()\n",
    "for fa in input_fasta:\n",
    "    # read seqs file and initialize kmerization object\n",
    "    fasta = SeqIO.parse(fa, \"fasta\")\n",
    "    kmer = vectorize.KmerVec(alphabet=config[\"alphabet\"], k=config[\"k\"])\n",
    "\n",
    "    for f in fasta:\n",
    "        kmers = list(kmer.reduce_vectorize(f.seq))\n",
    "        kmer_set.extend(kmers)\n",
    "\n",
    "# capture common basis set with kmers above threshold\n",
    "kmer_set = Counter(kmer_set)\n",
    "kmer_set = {x: count for x, count in kmer_set.items() if count >= config[\"min_kmer_threshold\"]}\n",
    "kmer_set = list(kmer_set.keys())\n",
    "print(len(kmer_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4b9ffa6-e5dc-4b68-af87-ee283eed0496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing kmerization for cNorB Family ...\n",
      "Generated 92 kmer vectors from 92 kmers.\n",
      "\n",
      "Initializing kmerization for NapB Family ...\n",
      "Generated 226 kmer vectors from 226 kmers.\n",
      "\n",
      "Initializing kmerization for nirS Family ...\n",
      "Generated 49 kmer vectors from 49 kmers.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# parse name of label (i.e. \"family\" or \"class\")\n",
    "label = (\n",
    "    config[\"score\"][\"lname\"] if str(config[\"score\"][\"lname\"]) != \"None\" else \"label\"\n",
    ")  \n",
    "\n",
    "# generate and save kmer vectors\n",
    "for fa in input_fasta:\n",
    "    fam = skm.utils.split_file_ext(fa)[0]\n",
    "    print(f\"Initializing kmerization for {fam} {label.title()} ...\")\n",
    "    \n",
    "    # specify output files\n",
    "    output_kmerobj = os.path.join(output_dir, \"kmerize\", f\"{fam}.kmers\")\n",
    "    output_data = os.path.join(output_dir, \"vector\", f\"{fam}.npz\")\n",
    "\n",
    "    # create any missing directories (note: snakemake automatically handles this)\n",
    "    if not os.path.exists(os.path.dirname(output_kmerobj)):\n",
    "        os.makedirs(os.path.dirname(output_kmerobj))\n",
    "    if not os.path.exists(os.path.dirname(output_data)):\n",
    "        os.makedirs(os.path.dirname(output_data))\n",
    "\n",
    "    # read seqs file, initialize kmerization object, and specify kmer set\n",
    "    fasta = SeqIO.parse(fa, \"fasta\")\n",
    "    kmer = vectorize.KmerVec(alphabet=config[\"alphabet\"], k=config[\"k\"])\n",
    "    kmer.set_kmer_set(kmer_set)\n",
    "\n",
    "    # reload fasta (to rebuild iterators)\n",
    "    fasta = SeqIO.parse(fa, \"fasta\")\n",
    "    vecs, seqs, ids, lengths = list(), list(), list(), list()\n",
    "    for f in fasta:\n",
    "        seq = skm.vectorize.reduce(\n",
    "            f.seq, alphabet=config[\"alphabet\"], mapping=skm.alphabet.FULL_ALPHABETS,\n",
    "        )\n",
    "        seqs.append(seq)\n",
    "        \n",
    "        vecs.append(list(kmer.vectorize(seq).values()))\n",
    "        ids.append(f.id)\n",
    "        lengths.append(len(f.seq))\n",
    "        \n",
    "    # log output for user\n",
    "    print(f\"Generated {len(vecs)} kmer vectors from {len(seqs)} kmers.\\n\")\n",
    "\n",
    "    # save seqIO output and transformed vecs\n",
    "    np.savez_compressed(output_data, ids=ids, seqs=seqs, vecs=vecs, lengths=lengths)\n",
    "\n",
    "    with open(output_kmerobj, \"wb\") as f:\n",
    "        pickle.dump(kmer, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3275120",
   "metadata": {},
   "source": [
    "## Rule 3: Score\n",
    "\n",
    "In this penultimate step, the files parsed into each feature space are scored for their faithful differentiation of each protein family, and then clustering is performed on the individual sequences. The results are all collated and stored into a Pandas DataFrame (`pandas.DataFrame` object)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23559a6f-dbc2-46e4-afda-c27ce0298f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../../../resources/tutorial/output/vector/cNorB.npz', '../../../resources/tutorial/output/vector/NapB.npz', '../../../resources/tutorial/output/vector/nirS.npz']\n"
     ]
    }
   ],
   "source": [
    "# specify input vector\n",
    "input_data = expand(os.path.join(output_dir, \"vector\", \"{fa}.npz\"), fa=NON_BGS)\n",
    "print(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81781005-80c2-4b37-bf15-6a918105d8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "\n",
    "def load_npz(\n",
    "    filename: str,\n",
    "    columns: Dict[str, str] = {\n",
    "        \"ids\": \"sequence_id\",\n",
    "        \"seqs\": \"sequence\",\n",
    "        \"vecs\": \"sequence_vector\",\n",
    "    },\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compile .npz results into dataframe.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        /path/to/filename for input .npz file.\n",
    "    columns : Dict[str, str]\n",
    "        Mapping for output data column names (the default is\n",
    "            {\n",
    "                \"ids\": \"sequence_id\",\n",
    "                \"seqs\": \"sequence\",\n",
    "                \"vecs\": \"sequence_vector\",\n",
    "            }\n",
    "        ).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Tabulated .npz data.\n",
    "\n",
    "    \"\"\"\n",
    "    data = np.load(filename, allow_pickle=True)\n",
    "\n",
    "    # fill in df based on desired output col names\n",
    "    df = {\"filename\": os.path.splitext(os.path.basename(filename))[0]}\n",
    "\n",
    "    for in_col, out_col in columns.items():\n",
    "        df.update({out_col: list(data[in_col])})\n",
    "\n",
    "        # get seq column for sequence lengths\n",
    "        if \"seq\" in in_col:\n",
    "            df.update({f\"{out_col}_length\": [len(s) for s in data[in_col]]})\n",
    "\n",
    "    return pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6523d9c-aefd-4bd1-8a12-08782ecfe139",
   "metadata": {},
   "source": [
    "We then load and concatenate AAR-kmer vectors for all input family files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "419a00ba-8551-456e-b980-6422a255bd8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time:\t2022-09-27 14:09:05.247753\n",
      "end time:\t2022-09-27 14:09:05.384744\n",
      "start time -> end time:\t0h 00m 00.137s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# log start time\n",
    "start_time = datetime.now()\n",
    "print(f\"start time:\\t{start_time}\")\n",
    "\n",
    "# load all kmer and vector data for demo files\n",
    "data = list()\n",
    "for input_ in input_data:\n",
    "    data.append(load_npz(input_))\n",
    "\n",
    "# concatenate all demo file data\n",
    "data = pd.concat(data, ignore_index=True)\n",
    "data[\"background\"] = [f in BGS for f in data[\"filename\"]]\n",
    "\n",
    "# log conversion step runtime\n",
    "timepoint = datetime.now()\n",
    "print(f\"end time:\\t{timepoint}\")\n",
    "print(\n",
    "    f\"start time -> end time:\\t{skm.utils._format_timedelta(timepoint - start_time)}\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dd5730-5146-4321-86db-9db7e1d0631a",
   "metadata": {},
   "source": [
    "Finally, we train a kmer-based scoring model for the family of interest based on the prevalence of kmers in each family."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6a15832-9b9f-437d-baf7-6d269dda77c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring sequences against cNorB Family ...\n",
      "\n",
      "start time:\t2022-09-27 14:09:12.276531\n",
      "\n",
      "class probabilities:\t2022-09-27 14:09:47.039791\n",
      "start time -> class probabilities:\t0h 00m 34.763s\n",
      "\n",
      "end time:\t2022-09-27 14:09:47.705886\n",
      "\n",
      "\n",
      "Scoring sequences against NapB Family ...\n",
      "\n",
      "start time:\t2022-09-27 14:09:47.706120\n",
      "\n",
      "class probabilities:\t2022-09-27 14:10:18.281998\n",
      "start time -> class probabilities:\t0h 00m 30.576s\n",
      "\n",
      "end time:\t2022-09-27 14:10:18.824410\n",
      "\n",
      "\n",
      "Scoring sequences against nirS Family ...\n",
      "\n",
      "start time:\t2022-09-27 14:10:18.824584\n",
      "\n",
      "class probabilities:\t2022-09-27 14:10:48.074993\n",
      "start time -> class probabilities:\t0h 00m 29.250s\n",
      "\n",
      "end time:\t2022-09-27 14:10:48.628768\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# perform scoring against each family identification using K-fold CV\n",
    "for family in FAS:\n",
    "    print(f\"Scoring sequences against {family} {label.title()} ...\\n\")\n",
    "    \n",
    "    # log start time\n",
    "    start_time = datetime.now()\n",
    "    print(f\"start time:\\t{start_time}\\n\")\n",
    "\n",
    "    # define i/o\n",
    "    input_kmerobj = os.path.join(output_dir, \"kmerize\", f\"{family}.kmers\")\n",
    "    output_data = os.path.join(output_dir, \"scoring\", \"sequences\", f\"{family}.csv.gz\")\n",
    "    output_weights = os.path.join(output_dir, \"scoring\", \"weights\", f\"{family}.csv.gz\")\n",
    "    output_scorer = os.path.join(output_dir, \"scoring\", f\"{family}.scorer\")\n",
    "    \n",
    "    # create any missing directories (note: snakemake automatically handles this)\n",
    "    if not os.path.exists(os.path.dirname(output_data)):\n",
    "        os.makedirs(os.path.dirname(output_data))\n",
    "    if not os.path.exists(os.path.dirname(output_weights)):\n",
    "        os.makedirs(os.path.dirname(output_weights))\n",
    "\n",
    "    # get kmers for this particular set of sequences\n",
    "    kmer = skm.io.load_pickle(input_kmerobj)\n",
    "\n",
    "    # parse family names and only add if some are valid\n",
    "    families = [\n",
    "        skm.utils.get_family(\n",
    "            skm.utils.split_file_ext(fn)[0], regex=config[\"input_file_regex\"]\n",
    "        )\n",
    "        for fn in data[\"filename\"]\n",
    "    ]\n",
    "    if any(families):\n",
    "        data[label] = families\n",
    "\n",
    "    # binary T/F for classification into randomly selected family\n",
    "    binary_labels = [value == family for value in data[label]]\n",
    "\n",
    "    # define k-fold split indices\n",
    "    if config[\"model\"][\"cv\"] > 1:\n",
    "        cv = StratifiedKFold(n_splits=config[\"model\"][\"cv\"], shuffle=True)\n",
    "\n",
    "        # stratify splits by [0,1] family assignment\n",
    "        for n, (i_train, _) in enumerate(\n",
    "            cv.split(data[\"sequence_vector\"], binary_labels)\n",
    "        ):\n",
    "            data[f\"train_cv-{n + 1:02d}\"] = [idx in i_train for idx in data.index]\n",
    "\n",
    "    elif config[\"model\"][\"cv\"] in [0, 1]:\n",
    "        i_train, _ = train_test_split(data.index, stratify=binary_labels)\n",
    "        data[\"train\"] = [idx in i_train for idx in data.index]\n",
    "\n",
    "    # generate family scores and object\n",
    "    scorer = skm.score.KmerScorer()\n",
    "    scorer.fit(\n",
    "        list(kmer.kmer_set.kmers),\n",
    "        data,\n",
    "        family,\n",
    "        label_col=label,\n",
    "        vec_col=\"sequence_vector\",\n",
    "        **config[\"score\"][\"scaler_kwargs\"],\n",
    "    )\n",
    "\n",
    "    # append scored sequences to dataframe\n",
    "    tmp = data.merge(\n",
    "        pd.DataFrame(scorer.scores[\"sample\"]), left_index=True, right_index=True\n",
    "    )\n",
    "    if tmp.empty:\n",
    "        raise ValueError(\"Blank df\")\n",
    "\n",
    "    # save score loadings\n",
    "    class_probabilities = (\n",
    "        pd.DataFrame(scorer.probabilities, index=scorer.kmers.basis)\n",
    "        .reset_index()\n",
    "        .rename(columns={\"index\": \"kmer\"})\n",
    "    )\n",
    "\n",
    "    # log time to compute class probabilities\n",
    "    timepoint = datetime.now()\n",
    "    print(f\"class probabilities:\\t{timepoint}\")\n",
    "    print(\n",
    "        f\"start time -> class probabilities:\\t{skm.utils._format_timedelta(timepoint - start_time)}\\n\"\n",
    "    )\n",
    "\n",
    "    # save all files to respective outputs\n",
    "    delete_cols = [\"vec\", \"sequence_vector\"]\n",
    "    for col in delete_cols:\n",
    "        if col in class_probabilities.columns:\n",
    "            class_probabilities = class_probabilities.drop(columns=col)\n",
    "    tmp.drop(columns=\"sequence_vector\").to_csv(\n",
    "        output_data, index=False, compression=\"gzip\"\n",
    "    )\n",
    "    class_probabilities.to_csv(output_weights, index=False, compression=\"gzip\")\n",
    "    with open(output_scorer, \"wb\") as f:\n",
    "        pickle.dump(scorer, f)\n",
    "\n",
    "    # record script endtime\n",
    "    timepoint = datetime.now()\n",
    "    print(f\"end time:\\t{timepoint}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650535c7",
   "metadata": {},
   "source": [
    "The contents of one of these dataframes is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e1643c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example family:\t cNorB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>sequence_id</th>\n",
       "      <th>family</th>\n",
       "      <th>cNorB_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cNorB</td>\n",
       "      <td>WP_004255833.1</td>\n",
       "      <td>cNorB</td>\n",
       "      <td>0.799270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cNorB</td>\n",
       "      <td>WP_043108184.1</td>\n",
       "      <td>cNorB</td>\n",
       "      <td>0.746088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cNorB</td>\n",
       "      <td>WP_011311073.1</td>\n",
       "      <td>cNorB</td>\n",
       "      <td>0.818612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cNorB</td>\n",
       "      <td>WP_014238124.1</td>\n",
       "      <td>cNorB</td>\n",
       "      <td>0.783957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cNorB</td>\n",
       "      <td>WP_013029258.1</td>\n",
       "      <td>cNorB</td>\n",
       "      <td>0.723229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>nirS</td>\n",
       "      <td>WP_011383805.1</td>\n",
       "      <td>nirS</td>\n",
       "      <td>0.001770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>nirS</td>\n",
       "      <td>WP_049724801.1</td>\n",
       "      <td>nirS</td>\n",
       "      <td>0.020561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>nirS</td>\n",
       "      <td>WP_041099757.1</td>\n",
       "      <td>nirS</td>\n",
       "      <td>0.001652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>nirS</td>\n",
       "      <td>WP_015258444.1</td>\n",
       "      <td>nirS</td>\n",
       "      <td>0.025523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>nirS</td>\n",
       "      <td>WP_014238329.1</td>\n",
       "      <td>nirS</td>\n",
       "      <td>0.000076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>367 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    filename     sequence_id family  cNorB_score\n",
       "0      cNorB  WP_004255833.1  cNorB     0.799270\n",
       "1      cNorB  WP_043108184.1  cNorB     0.746088\n",
       "2      cNorB  WP_011311073.1  cNorB     0.818612\n",
       "3      cNorB  WP_014238124.1  cNorB     0.783957\n",
       "4      cNorB  WP_013029258.1  cNorB     0.723229\n",
       "..       ...             ...    ...          ...\n",
       "362     nirS  WP_011383805.1   nirS     0.001770\n",
       "363     nirS  WP_049724801.1   nirS     0.020561\n",
       "364     nirS  WP_041099757.1   nirS     0.001652\n",
       "365     nirS  WP_015258444.1   nirS     0.025523\n",
       "366     nirS  WP_014238329.1   nirS     0.000076\n",
       "\n",
       "[367 rows x 4 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_family = \"cNorB\"  # np.random.choice(FAS)\n",
    "print(\"example family:\\t\", example_family)\n",
    "\n",
    "# show relevant subset of columns\n",
    "example_score_output = pd.read_csv(\n",
    "    os.path.join(output_dir, \"scoring\", \"sequences\", f\"{example_family}.csv.gz\")\n",
    ")[[\"filename\", \"sequence_id\", \"family\", f\"{example_family}_score\"]]\n",
    "example_score_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "125b0907-bc71-49c9-8897-9b5273bc6cde",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'output/scoring/sequences/cNorB.csv.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fam \u001b[38;5;129;01min\u001b[39;00m FAS:\n\u001b[0;32m----> 2\u001b[0m     example_score_output \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscoring\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msequences\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfam\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.csv.gz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequence_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfamily\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m      6\u001b[0m     plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m2\u001b[39m), dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m)\n\u001b[1;32m      7\u001b[0m     ax \u001b[38;5;241m=\u001b[39m sns\u001b[38;5;241m.\u001b[39mboxplot(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfamily\u001b[39m\u001b[38;5;124m\"\u001b[39m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_score\u001b[39m\u001b[38;5;124m\"\u001b[39m, data\u001b[38;5;241m=\u001b[39mexample_score_output)\n",
      "File \u001b[0;32m~/miniconda3/envs/snekmer/lib/python3.10/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/snekmer/lib/python3.10/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/snekmer/lib/python3.10/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniconda3/envs/snekmer/lib/python3.10/site-packages/pandas/io/parsers/readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/snekmer/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1213\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/envs/snekmer/lib/python3.10/site-packages/pandas/io/common.py:714\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;66;03m# error: Incompatible types in assignment (expression has type\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;66;03m# \"GzipFile\", variable has type \"Union[str, BaseBuffer]\")\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m     handle \u001b[38;5;241m=\u001b[39m \u001b[43mgzip\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGzipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[assignment]\u001b[39;49;00m\n\u001b[1;32m    715\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcompression_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    720\u001b[0m     handle \u001b[38;5;241m=\u001b[39m gzip\u001b[38;5;241m.\u001b[39mGzipFile(\n\u001b[1;32m    721\u001b[0m         \u001b[38;5;66;03m# No overload variant of \"GzipFile\" matches argument types\u001b[39;00m\n\u001b[1;32m    722\u001b[0m         \u001b[38;5;66;03m# \"Union[str, BaseBuffer]\", \"str\", \"Dict[str, Any]\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcompression_args,\n\u001b[1;32m    726\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/snekmer/lib/python3.10/gzip.py:174\u001b[0m, in \u001b[0;36mGzipFile.__init__\u001b[0;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[1;32m    172\u001b[0m     mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fileobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     fileobj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmyfileobj \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(fileobj, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'output/scoring/sequences/cNorB.csv.gz'"
     ]
    }
   ],
   "source": [
    "for fam in FAS:\n",
    "    example_score_output = pd.read_csv(\n",
    "        os.path.join(\"output\", \"scoring\", \"sequences\", f\"{fam}.csv.gz\")\n",
    "    )[[\"filename\", \"sequence_id\", \"family\", f\"{fam}_score\"]]\n",
    "    \n",
    "    plt.figure(figsize=(6, 2), dpi=150)\n",
    "    ax = sns.boxplot(x=\"family\", y=f\"{fam}_score\", data=example_score_output)\n",
    "\n",
    "    ax.set_title(\n",
    "        f\"Distribution of {fam} Scores for Sequences in Different Families\"\n",
    "    )\n",
    "    ax.set_xlabel(\"Family\")\n",
    "    ax.set_ylabel(f\"{fam} Score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc98b0f",
   "metadata": {},
   "source": [
    "We can assess how well the scores perform using the weights determined for each family. Note that it is immediately obvious that the `cNorB` scoring method performs well in identifying cNorB sequences versus the NapB and nirS sequences.\n",
    "\n",
    "Not all family scoring methods perform similarly well in terms of differentation between sequences belonging to different families. The user can manually inspect the other families, but we note that the NapB family scorer is worse at generating high separation between the in-family and out-of-family sequences. Differing scorer performances can be attributed to a variety of factors, e.g. parameters such as the alphabet and k, existing levels of similarity between sequences in different families, etc.\n",
    "\n",
    "The probabilities and scores assigned to each feature in the kmer set is also computed and output into a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0178e92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cNorB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kmer</th>\n",
       "      <th>sample</th>\n",
       "      <th>background</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VSSSSSVVVVSSSV</td>\n",
       "      <td>0.011535</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SSSSSVVVVSSSVV</td>\n",
       "      <td>0.206522</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SSSSVVVVSSSVVV</td>\n",
       "      <td>0.206522</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SSSVVVVSSSVVVV</td>\n",
       "      <td>0.141304</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SSVVVVSSSVVVVV</td>\n",
       "      <td>0.141304</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13126</th>\n",
       "      <td>SSSSSVVSSVVSVV</td>\n",
       "      <td>-0.010204</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13127</th>\n",
       "      <td>SSSSVVSSVVSVVV</td>\n",
       "      <td>-0.010204</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13128</th>\n",
       "      <td>SSSVSSSVVSVVSV</td>\n",
       "      <td>-0.010204</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13129</th>\n",
       "      <td>SSVSSSVVSVVSVV</td>\n",
       "      <td>-0.010204</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13130</th>\n",
       "      <td>SVSSSVVSVVSVVV</td>\n",
       "      <td>-0.010204</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13131 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 kmer    sample  background\n",
       "0      VSSSSSVVVVSSSV  0.011535         NaN\n",
       "1      SSSSSVVVVSSSVV  0.206522         NaN\n",
       "2      SSSSVVVVSSSVVV  0.206522         NaN\n",
       "3      SSSVVVVSSSVVVV  0.141304         NaN\n",
       "4      SSVVVVSSSVVVVV  0.141304         NaN\n",
       "...               ...       ...         ...\n",
       "13126  SSSSSVVSSVVSVV -0.010204         NaN\n",
       "13127  SSSSVVSSVVSVVV -0.010204         NaN\n",
       "13128  SSSVSSSVVSVVSV -0.010204         NaN\n",
       "13129  SSVSSSVVSVVSVV -0.010204         NaN\n",
       "13130  SVSSSVVSVVSVVV -0.010204         NaN\n",
       "\n",
       "[13131 rows x 3 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(example_family)\n",
    "pd.read_csv(os.path.join(output_dir, \"scoring\", \"weights\", f\"{example_family}.csv.gz\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cffa038-14c8-4ce0-9c5d-90f29649d0f2",
   "metadata": {},
   "source": [
    "Note that this example does not include any sequences specified as background sequences, and thus the `background` column does not contribute additional weights in scoring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c17657",
   "metadata": {},
   "source": [
    "## Rule 5: Build supervised machine learning models\n",
    "\n",
    "Finally, models are constructed which accept the pre-determined family scores as input and train logistic regression models to output putative in-family assignment for a given sequence. The model can also be used to approximate the probability that a given sequence belongs to the family of interest.\n",
    "\n",
    "Each model is further validated using K-fold cross-validation, and the results from each cross-validation split are summarized in figures depicting the Receiver-Operator Characteristics (ROC) Curve and Precision-Recall (PR) Curve. In the following example, `K=5`.\n",
    "\n",
    "The code, as adapted for presentation in a Jupyter notebook, is included below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f3dfc402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building models for cNorB Family ...\n",
      "\n",
      "input files:\t [['output/features/cNorB/cNorB.json.gz', 'output/features/cNorB/NapB.json.gz', 'output/features/cNorB/nirS.json.gz'], 'output/features/cNorB.csv.gz', 'output/score/weights/cNorB.csv.gz', 'output/labels/cNorB.txt']\n",
      "output files:\t ['output/model/results/cNorB.csv', 'output/model/cNorB.pkl']\n",
      "start time:\t2022-09-22 16:16:29.149590\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'output/features/cNorB.csv.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [95]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart time:\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mstart_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# load all input data and encode rule-wide variables\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvector\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     25\u001b[0m     literal_eval(vec) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(vec, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m vec \u001b[38;5;28;01mfor\u001b[39;00m vec \u001b[38;5;129;01min\u001b[39;00m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvector\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     26\u001b[0m ]\n\u001b[1;32m     27\u001b[0m scores \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(input_scores)\n",
      "File \u001b[0;32m~/miniconda3/envs/snekmer/lib/python3.10/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/snekmer/lib/python3.10/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/snekmer/lib/python3.10/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniconda3/envs/snekmer/lib/python3.10/site-packages/pandas/io/parsers/readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/snekmer/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1213\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/envs/snekmer/lib/python3.10/site-packages/pandas/io/common.py:714\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;66;03m# error: Incompatible types in assignment (expression has type\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;66;03m# \"GzipFile\", variable has type \"Union[str, BaseBuffer]\")\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m     handle \u001b[38;5;241m=\u001b[39m \u001b[43mgzip\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGzipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[assignment]\u001b[39;49;00m\n\u001b[1;32m    715\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcompression_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    720\u001b[0m     handle \u001b[38;5;241m=\u001b[39m gzip\u001b[38;5;241m.\u001b[39mGzipFile(\n\u001b[1;32m    721\u001b[0m         \u001b[38;5;66;03m# No overload variant of \"GzipFile\" matches argument types\u001b[39;00m\n\u001b[1;32m    722\u001b[0m         \u001b[38;5;66;03m# \"Union[str, BaseBuffer]\", \"str\", \"Dict[str, Any]\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcompression_args,\n\u001b[1;32m    726\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/snekmer/lib/python3.10/gzip.py:174\u001b[0m, in \u001b[0;36mGzipFile.__init__\u001b[0;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[1;32m    172\u001b[0m     mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fileobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     fileobj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmyfileobj \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(fileobj, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'output/features/cNorB.csv.gz'"
     ]
    }
   ],
   "source": [
    "for fa in FAS:\n",
    "    print(f\"Building models for {fa} {label.title()} ...\\n\")\n",
    "    \n",
    "    # define all input and output files\n",
    "#     input_files = expand(\n",
    "#         os.path.join(output_dir, \"features\", f\"{fa}\", \"{fa2}.json.gz\"), fa2=FAS\n",
    "#     )\n",
    "#     input_data = os.path.join(output_dir, \"features\", f\"{fa}.csv.gz\")\n",
    "#     input_scores = os.path.join(output_dir, \"score\", \"weights\", f\"{fa}.csv.gz\")\n",
    "#     input_kmers = os.path.join(output_dir, \"labels\", f\"{fa}.txt\")\n",
    "#     output_model = os.path.join(output_dir, \"model\", f\"{fa}.pkl\")\n",
    "#     output_results = os.path.join(output_dir, \"model\", \"results\", f\"{fa}.csv\")\n",
    "\n",
    "    # define i/o\n",
    "    input_matrix = os.path.join(output_dir, \"scoring\", f\"{fa}.matrix\")\n",
    "    input_weights = os.path.join(output_dir, \"scoring\", \"weights\", f\"{fa}.csv.gz\")\n",
    "    input_kmerobj = os.path.join(output_dir, \"kmerize\", f\"{fa}.kmers\")\n",
    "    output_model = os.path.join(output_dir, \"model\", f\"{fa}.model\")\n",
    "    output_results = os.path.join(output_dir, \"model\", \"results\", f\"{fa}.csv\")\n",
    "    \n",
    "    print(\"input files:\\t\", [input_matrix, input_weights, input_kmerobj])\n",
    "    print(\"output files:\\t\", [output_model, output_results], \"\\n\")\n",
    "\n",
    "    # create any missing output directories\n",
    "    if not os.path.exists(os.path.dirname(output_results)):\n",
    "        os.makedirs(os.path.dirname(output_results))\n",
    "\n",
    "    start_time = datetime.now()\n",
    "    print(f\"start time:\\t{start_time}\\n\")\n",
    "\n",
    "    # load all input data and encode rule-wide variables\n",
    "    data = skm.io.load_pickle(input_matrix)\n",
    "    scores = pd.read_csv(input_weights)\n",
    "    family = skm.utils.get_family(\n",
    "        skm.utils.split_file_ext(input_scores)[0], regex=config[\"input\"][\"regex\"]\n",
    "    )\n",
    "    \n",
    "    # get kmers for this particular set of sequences\n",
    "    kmers = skm.io.load_pickle(input_kmerobj)\n",
    "\n",
    "    # prevent kmer NA being read as np.nan\n",
    "    if config[\"k\"] == 2:\n",
    "        scores[\"kmer\"] = scores[\"kmer\"].fillna(\"NA\")\n",
    "\n",
    "    # get alphabet name\n",
    "    if config[\"alphabet\"] in skm.alphabet.ALPHABET_ORDER.keys():\n",
    "        alphabet_name = skm.alphabet.ALPHABET_ORDER[config[\"alphabet\"]].capitalize()\n",
    "    else:\n",
    "        alphabet_name = str(config[\"alphabet\"]).capitalize()\n",
    "\n",
    "    # create dataframe skeleton for AUC per family\n",
    "    results = {\n",
    "        \"family\": [],\n",
    "        \"alphabet_name\": [],\n",
    "        \"k\": [],\n",
    "        \"scoring\": [],\n",
    "        \"score\": [],\n",
    "        \"cv_split\": [],\n",
    "    }\n",
    "\n",
    "    # generate [0, 1] labels for binary family assignment\n",
    "    binary_labels = [value == family for value in data[\"family\"]]\n",
    "    le = LabelEncoder()\n",
    "    le.fit(binary_labels)\n",
    "\n",
    "    # set and format input and label arrays; initialize model objs\n",
    "    X_all = data[f\"{family}_score\"].values.reshape(-1, 1)\n",
    "    y_all = le.transform(binary_labels).ravel()\n",
    "\n",
    "    # set random seed if specified\n",
    "    rng = np.random.default_rng()\n",
    "    random_state = rng.integers(low=0, high=32767)  # max for int16\n",
    "    if str(config[\"model\"][\"random_state\"]) != \"None\":\n",
    "        random_state = config[\"model\"][\"random_state\"]\n",
    "\n",
    "    # set and format input and label arrays; initialize model objs\n",
    "    cv = config[\"model\"][\"cv\"]\n",
    "    X, y = {i: {} for i in range(cv)}, {i: {} for i in range(cv)}\n",
    "    for n in range(cv):\n",
    "\n",
    "        # remove score cols that were generated from full dataset\n",
    "        unscored_cols = [col for col in list(data.columns) if \"_score\" not in col]\n",
    "\n",
    "        # filter data by training data per split\n",
    "        i_train = data[data[f\"train_cv-{n + 1:02d}\"]].index\n",
    "        i_test = data[~data[f\"train_cv-{n + 1:02d}\"]].index\n",
    "        df_train = data.iloc[i_train][unscored_cols].reset_index(drop=True)\n",
    "        df_test = data.iloc[i_test][unscored_cols].reset_index(drop=True)\n",
    "        df_train_labels = [\n",
    "            True if value == family else False for value in df_train[\"family\"]\n",
    "        ]\n",
    "        df_test_labels = [\n",
    "            True if value == family else False for value in df_test[\"family\"]\n",
    "        ]\n",
    "\n",
    "        # score kmers separately per split\n",
    "        scorer = skm.model.KmerScorer()\n",
    "        scorer.fit(\n",
    "            kmers,\n",
    "            df_train,\n",
    "            family,\n",
    "            label_col=label,\n",
    "            **config[\"score\"][\"scaler_kwargs\"],\n",
    "        )\n",
    "\n",
    "        # append scored sequences to dataframe\n",
    "        df_train = df_train.merge(\n",
    "            pd.DataFrame(scorer.scores[\"sample\"]), left_index=True, right_index=True\n",
    "        )\n",
    "        if df_train.empty:\n",
    "            raise ValueError(\"Blank df\")\n",
    "        df_test = df_test.merge(\n",
    "            pd.DataFrame(\n",
    "                scorer.predict(skm.model.to_feature_matrix(df_test[\"vector\"]), kmers)\n",
    "            ),\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "        ).rename(columns={0: f\"{family}_score\"})\n",
    "\n",
    "        # save score loadings\n",
    "        scores = (\n",
    "            pd.DataFrame(scorer.probabilities, index=scorer.kmers.basis)\n",
    "            .reset_index()\n",
    "            .rename(columns={\"index\": \"kmer\"})\n",
    "        )\n",
    "\n",
    "        # save X,y array data for plot\n",
    "        X[n][\"train\"] = df_train[f\"{family}_score\"].values.reshape(-1, 1)\n",
    "        y[n][\"train\"] = le.transform(df_train_labels).ravel()\n",
    "\n",
    "        X[n][\"test\"] = df_test[f\"{family}_score\"].values.reshape(-1, 1)\n",
    "        y[n][\"test\"] = le.transform(df_test_labels).ravel()\n",
    "\n",
    "    # ROC-AUC figure\n",
    "    clf = LogisticRegression(\n",
    "        random_state=random_state, solver=\"liblinear\", class_weight=\"balanced\"\n",
    "    )\n",
    "    fig, ax, auc_rocs = skm.plot.cv_roc_curve(\n",
    "        clf,\n",
    "        X,\n",
    "        y,\n",
    "        title=(f\"{family} ROC Curve ({alphabet_name},  k={config['k']})\"),\n",
    "        dpi=100,\n",
    "    )\n",
    "\n",
    "    # collate ROC-AUC results\n",
    "    results[\"family\"] += [family] * cv\n",
    "    results[\"alphabet_name\"] += [alphabet_name.lower()] * cv\n",
    "    results[\"k\"] += [config[\"k\"]] * cv\n",
    "    results[\"scoring\"] += [\"roc_auc\"] * cv\n",
    "    results[\"score\"] += auc_rocs\n",
    "    results[\"cv_split\"] += [i + 1 for i in range(cv)]\n",
    "\n",
    "    # display and ROC-AUC figure\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # PR-AUC figure\n",
    "    fig, ax, pr_aucs = skm.plot.cv_pr_curve(\n",
    "        clf,\n",
    "        X,\n",
    "        y,\n",
    "        title=(f\"{family} PR Curve ({alphabet_name}, k={config['k']})\"),\n",
    "        dpi=100,\n",
    "    )\n",
    "\n",
    "    # collate PR-AUC results\n",
    "    results[\"family\"] += [family] * cv\n",
    "    results[\"alphabet_name\"] += [alphabet_name.lower()] * cv\n",
    "    results[\"k\"] += [config[\"k\"]] * cv\n",
    "    results[\"scoring\"] += [\"pr_auc\"] * cv\n",
    "    results[\"score\"] += pr_aucs\n",
    "    results[\"cv_split\"] += [i + 1 for i in range(cv)]\n",
    "\n",
    "    # display PR-AUC figure\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # save model\n",
    "    clf.fit(X_all, y_all)\n",
    "    with open(output_model, \"wb\") as save_model:\n",
    "        pickle.dump(clf, save_model)\n",
    "\n",
    "    # save full results\n",
    "    pd.DataFrame(results).to_csv(output_results, index=False)\n",
    "\n",
    "    # record script runtime\n",
    "    end_time = datetime.now()\n",
    "    print(f\"end time:\\t{end_time}\")\n",
    "    print(f\"total time:\\t{skm.utils.format_timedelta(end_time - start_time)}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5582d1b3",
   "metadata": {},
   "source": [
    "The models are objects stored as pickle files (.PKL) that can be applied elsewhere, e.g. to a new set of unknown sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82772d09",
   "metadata": {},
   "source": [
    "## Snekmer Search Mode\n",
    "\n",
    "Say a user trains the four models above, and would then like to score and evaluate sequences with unknown family assignments. The user can use `snekmer search`, which uses the kmer basis set for the desired family to create kmer vectors for unknown sequences, then apply the family scorer to the vectorized unknown sequences, and finally use the model to predict family assignments for the unknown sequences.\n",
    "\n",
    "These steps are illustrated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0465d5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "family:\t cNorB\n",
      "start time:\t2022-03-16 09:55:17.791642\n",
      "end time:\t2022-03-16 09:55:20.247583\n",
      "total time:\t0h 00m 02.456s\n",
      "\n",
      "family:\t NapD\n",
      "start time:\t2022-03-16 09:55:20.247792\n",
      "end time:\t2022-03-16 09:55:24.291122\n",
      "total time:\t0h 00m 04.43s\n",
      "\n",
      "family:\t NapB\n",
      "start time:\t2022-03-16 09:55:24.291281\n",
      "end time:\t2022-03-16 09:55:28.031143\n",
      "total time:\t0h 00m 03.740s\n",
      "\n",
      "family:\t nirS\n",
      "start time:\t2022-03-16 09:55:28.031352\n",
      "end time:\t2022-03-16 09:55:30.382982\n",
      "total time:\t0h 00m 02.352s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "families = [\n",
    "    skm.utils.get_family(\n",
    "        skm.utils.split_file_ext(fa)[0], regex=config[\"input\"][\"regex\"]\n",
    "    )\n",
    "    for fa in FAS\n",
    "]\n",
    "\n",
    "for fam in families:\n",
    "    print(\"family:\\t\", fam)\n",
    "    start_time = datetime.now()\n",
    "    print(f\"start time:\\t{start_time}\")\n",
    "\n",
    "    # define the basis set for the example family\n",
    "    input_basis = os.path.join(\"demo_files\", \"output\", \"labels\", f\"{fam}.txt\")\n",
    "    basis = skm.io.read_output_kmers(input_basis)\n",
    "\n",
    "    # define set of unknown vectors\n",
    "    input_fastas = glob(os.path.join(\"demo_files\", \"search\", \"input\", \"*.fasta\"))\n",
    "    output_features = os.path.join(\"demo_files\", \"search\", \"output\", \"features\", fam)\n",
    "    if not os.path.exists(output_features):\n",
    "        os.makedirs(output_features)\n",
    "\n",
    "    # load pre-trained scorer and model\n",
    "    input_model = os.path.join(\"demo_files\", \"output\", \"model\", f\"{fam}.pkl\")\n",
    "    with open(input_model, \"rb\") as mf:\n",
    "        model = pickle.load(mf)\n",
    "    input_scorer = os.path.join(\"demo_files\", \"output\", \"score\", f\"{fam}.pkl\")\n",
    "    with open(input_scorer, \"rb\") as sf:\n",
    "        scorer = pickle.load(sf)\n",
    "\n",
    "    # define collated family model result output\n",
    "    output_results = os.path.join(\n",
    "        \"demo_files\", \"search\", \"output\", \"search\", f\"{fam}.csv\"\n",
    "    )\n",
    "    if not os.path.exists(os.path.dirname(output_results)):\n",
    "        os.makedirs(os.path.dirname(output_results))\n",
    "\n",
    "    # STEP 1: build kmer vectors according to new family basis set\n",
    "    results = list()\n",
    "    for i, fasta in enumerate(input_fastas):\n",
    "        f = skm.utils.get_family(\n",
    "            skm.utils.split_file_ext(fasta)[0], regex=config[\"input\"][\"regex\"]\n",
    "        )\n",
    "\n",
    "        vec_results = {\"seq_id\": [], \"vector\": []}\n",
    "        seq_list, id_list = skm.io.read_fasta(fasta)\n",
    "\n",
    "        for seq, sid in zip(seq_list, id_list):\n",
    "            vec_results[\"seq_id\"] += [sid]\n",
    "            vec_results[\"vector\"] += [\n",
    "                skm.transform.vectorize_string(\n",
    "                    seq,\n",
    "                    config[\"k\"],\n",
    "                    params[\"alphabet\"],\n",
    "                    start=config[\"start\"],\n",
    "                    end=config[\"end\"],\n",
    "                    filter_list=basis,\n",
    "                    verbose=False,  # suppress for batch processing\n",
    "                )\n",
    "            ]\n",
    "        with gzip.open(\n",
    "            os.path.join(output_features, f\"{f}.json.gz\"), \"wt\", encoding=\"ascii\"\n",
    "        ) as zipfile:\n",
    "            json.dump(vec_results, zipfile)\n",
    "\n",
    "        df = pd.DataFrame(vec_results)\n",
    "        vecs = skm.utils.to_feature_matrix(df[\"vector\"].values)\n",
    "\n",
    "        # score unknown sequences using pre-trained scorer\n",
    "        scores = scorer.predict(vecs, basis)\n",
    "\n",
    "        # predict probabilities and classes of new vecs using model\n",
    "        predictions = model.predict(scores.reshape(-1, 1))\n",
    "        predicted_probas = model.predict_proba(scores.reshape(-1, 1))\n",
    "\n",
    "        # display results (score, family assignment, and probability)\n",
    "        df[f\"{fam}_score\"] = scores  # scorer output\n",
    "        df[fam] = [True if p == 1 else False for p in predictions]\n",
    "        df[f\"{fam}_probability\"] = [p[1] for p in predicted_probas]\n",
    "        df[\"filename\"] = os.path.basename(fasta)\n",
    "        results.append(df)\n",
    "\n",
    "    results = pd.concat(results, ignore_index=True).drop(columns=[\"vector\"])\n",
    "    results.to_csv(output_results, index=False)\n",
    "\n",
    "    # record script runtime\n",
    "    end_time = datetime.now()\n",
    "    print(f\"end time:\\t{end_time}\")\n",
    "    print(f\"total time:\\t{skm.utils.format_timedelta(end_time - start_time)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d6e4d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example family:\t cNorB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq_id</th>\n",
       "      <th>cNorB_score</th>\n",
       "      <th>cNorB</th>\n",
       "      <th>cNorB_probability</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WP_006484650.1</td>\n",
       "      <td>0.007890</td>\n",
       "      <td>False</td>\n",
       "      <td>0.051469</td>\n",
       "      <td>three.fasta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WP_006484654.1</td>\n",
       "      <td>0.004527</td>\n",
       "      <td>False</td>\n",
       "      <td>0.050095</td>\n",
       "      <td>three.fasta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WP_006484657.1</td>\n",
       "      <td>0.000893</td>\n",
       "      <td>False</td>\n",
       "      <td>0.048649</td>\n",
       "      <td>three.fasta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WP_006484664.1</td>\n",
       "      <td>0.078243</td>\n",
       "      <td>False</td>\n",
       "      <td>0.089690</td>\n",
       "      <td>three.fasta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WP_006484673.1</td>\n",
       "      <td>0.023950</td>\n",
       "      <td>False</td>\n",
       "      <td>0.058538</td>\n",
       "      <td>three.fasta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>WP_006484674.1</td>\n",
       "      <td>0.036273</td>\n",
       "      <td>False</td>\n",
       "      <td>0.064569</td>\n",
       "      <td>three.fasta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>WP_006484682.1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.048300</td>\n",
       "      <td>three.fasta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>WP_004186391.1</td>\n",
       "      <td>0.010123</td>\n",
       "      <td>False</td>\n",
       "      <td>0.052402</td>\n",
       "      <td>one.fasta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>WP_004186661.1</td>\n",
       "      <td>0.005337</td>\n",
       "      <td>False</td>\n",
       "      <td>0.050423</td>\n",
       "      <td>one.fasta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>WP_004186709.1</td>\n",
       "      <td>0.000794</td>\n",
       "      <td>False</td>\n",
       "      <td>0.048610</td>\n",
       "      <td>one.fasta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>WP_004189550.1</td>\n",
       "      <td>0.053628</td>\n",
       "      <td>False</td>\n",
       "      <td>0.074047</td>\n",
       "      <td>one.fasta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>WP_004191477.1</td>\n",
       "      <td>0.006825</td>\n",
       "      <td>False</td>\n",
       "      <td>0.051030</td>\n",
       "      <td>one.fasta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>WP_006481537.1</td>\n",
       "      <td>0.001436</td>\n",
       "      <td>False</td>\n",
       "      <td>0.048863</td>\n",
       "      <td>two.fasta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>WP_006481556.1</td>\n",
       "      <td>0.019322</td>\n",
       "      <td>False</td>\n",
       "      <td>0.056413</td>\n",
       "      <td>two.fasta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>WP_006481558.1</td>\n",
       "      <td>0.018116</td>\n",
       "      <td>False</td>\n",
       "      <td>0.055871</td>\n",
       "      <td>two.fasta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>WP_006481561.1</td>\n",
       "      <td>0.010103</td>\n",
       "      <td>False</td>\n",
       "      <td>0.052393</td>\n",
       "      <td>two.fasta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>WP_006481562.1</td>\n",
       "      <td>0.017384</td>\n",
       "      <td>False</td>\n",
       "      <td>0.055544</td>\n",
       "      <td>two.fasta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>WP_006481566.1</td>\n",
       "      <td>0.034471</td>\n",
       "      <td>False</td>\n",
       "      <td>0.063652</td>\n",
       "      <td>two.fasta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>WP_006481567.1</td>\n",
       "      <td>0.016020</td>\n",
       "      <td>False</td>\n",
       "      <td>0.054941</td>\n",
       "      <td>two.fasta</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            seq_id  cNorB_score  cNorB  cNorB_probability     filename\n",
       "0   WP_006484650.1     0.007890  False           0.051469  three.fasta\n",
       "1   WP_006484654.1     0.004527  False           0.050095  three.fasta\n",
       "2   WP_006484657.1     0.000893  False           0.048649  three.fasta\n",
       "3   WP_006484664.1     0.078243  False           0.089690  three.fasta\n",
       "4   WP_006484673.1     0.023950  False           0.058538  three.fasta\n",
       "5   WP_006484674.1     0.036273  False           0.064569  three.fasta\n",
       "6   WP_006484682.1     0.000000  False           0.048300  three.fasta\n",
       "7   WP_004186391.1     0.010123  False           0.052402    one.fasta\n",
       "8   WP_004186661.1     0.005337  False           0.050423    one.fasta\n",
       "9   WP_004186709.1     0.000794  False           0.048610    one.fasta\n",
       "10  WP_004189550.1     0.053628  False           0.074047    one.fasta\n",
       "11  WP_004191477.1     0.006825  False           0.051030    one.fasta\n",
       "12  WP_006481537.1     0.001436  False           0.048863    two.fasta\n",
       "13  WP_006481556.1     0.019322  False           0.056413    two.fasta\n",
       "14  WP_006481558.1     0.018116  False           0.055871    two.fasta\n",
       "15  WP_006481561.1     0.010103  False           0.052393    two.fasta\n",
       "16  WP_006481562.1     0.017384  False           0.055544    two.fasta\n",
       "17  WP_006481566.1     0.034471  False           0.063652    two.fasta\n",
       "18  WP_006481567.1     0.016020  False           0.054941    two.fasta"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"example family:\\t\", example_family)\n",
    "pd.read_csv(\n",
    "    os.path.join(\"demo_files\", \"search\", \"output\", \"search\", f\"{example_family}.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf5c845",
   "metadata": {},
   "source": [
    "In this example application, none of the sequences contained in any of the three unknown files are predicted to belong to the example family, cNorB. The family probability scores themselves (see column: **cNorB_probability**) are very low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd3daa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snekmer",
   "language": "python",
   "name": "snekmer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
