{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 128])\n",
      "tensor([0.0216, 0.5907, 0.0000, 0.5206, 0.0000, 0.0000, 0.1397, 0.6159, 0.4056,\n",
      "        0.4208, 0.0000, 0.2074, 0.0000, 0.0000, 0.9542, 0.0000, 1.2939, 0.6681,\n",
      "        0.0000, 0.0000, 0.1675, 0.0088, 0.1731, 0.0000, 0.0000, 0.0000, 0.0638,\n",
      "        0.3921, 0.7446, 0.7494, 0.0000, 0.1399, 0.0000, 0.0161, 0.3200, 0.4624,\n",
      "        0.3254, 0.0177, 0.4225, 0.0000, 0.0000, 0.0956, 0.0000, 0.0000, 0.7382,\n",
      "        0.4396, 0.0000, 0.3653, 0.0000, 0.0000, 0.1651, 0.5046, 0.3079, 0.8124,\n",
      "        1.1301, 0.2877, 0.3248, 0.3151, 0.0000, 0.0000, 0.4420, 0.5377, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.6859, 0.0000, 0.0320, 0.3293, 0.0255, 0.4700,\n",
      "        0.0000, 0.1249, 0.5061, 0.0224, 0.1814, 0.9213, 0.1211, 0.9248, 0.0925,\n",
      "        0.0000, 0.4294, 0.0000, 0.5676, 0.0000, 0.0000, 0.0000, 0.0000, 0.7133,\n",
      "        0.5086, 0.5779, 0.7203, 0.0000, 0.9594, 0.0438, 0.0000, 0.0000, 0.0000,\n",
      "        0.2101, 0.7225, 0.0000, 0.1696, 0.2620, 0.6636, 0.1987, 0.5072, 0.1857,\n",
      "        0.9408, 0.0000, 0.2527, 0.0000, 0.4005, 0.1247, 0.7441, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.2951, 0.0000, 0.7168, 0.3844, 0.0000, 0.0000,\n",
      "        0.2134, 0.0000], grad_fn=<MaxBackward0>)\n",
      "tensor([9, 1, 0, 0, 0, 0, 4, 3, 6, 1, 0, 5, 0, 0, 9, 0, 6, 1, 0, 0, 3, 2, 4, 0,\n",
      "        0, 0, 9, 1, 9, 5, 0, 6, 0, 0, 0, 8, 1, 9, 4, 0, 0, 6, 0, 0, 1, 6, 0, 8,\n",
      "        0, 0, 3, 8, 4, 8, 1, 1, 5, 4, 0, 0, 4, 9, 0, 0, 0, 0, 4, 0, 3, 9, 8, 4,\n",
      "        0, 5, 8, 8, 9, 9, 7, 4, 1, 0, 9, 0, 8, 0, 0, 0, 0, 4, 0, 8, 4, 0, 7, 9,\n",
      "        0, 0, 0, 8, 3, 0, 4, 8, 9, 6, 4, 0, 4, 0, 3, 0, 4, 9, 5, 0, 0, 0, 0, 0,\n",
      "        8, 0, 1, 8, 0, 0, 9, 0])\n",
      "torch.Size([10, 256])\n",
      "tensor([0.0000, 0.1885, 0.0952, 0.0000, 0.0000, 0.0729, 0.2546, 0.1596, 0.2373,\n",
      "        0.0757, 0.0569, 0.4930, 0.4205, 0.0000, 0.1116, 0.3908, 0.3691, 0.0000,\n",
      "        0.0000, 0.2681, 0.2452, 0.4380, 0.1495, 0.1292, 0.5663, 0.0000, 0.0027,\n",
      "        0.1621, 0.1435, 0.0000, 0.3915, 0.2157, 0.0000, 0.5502, 0.6760, 0.0980,\n",
      "        0.0675, 0.4185, 0.3325, 0.0000, 0.3590, 0.3334, 0.1374, 0.3026, 0.1976,\n",
      "        0.1428, 0.1876, 0.1960, 0.1367, 0.8386, 0.0000, 0.3924, 0.5861, 0.0000,\n",
      "        0.0000, 0.0000, 0.0679, 0.3150, 0.0000, 0.1170, 0.1944, 0.1293, 0.1310,\n",
      "        0.0000, 0.4251, 0.0000, 0.2281, 0.0000, 0.0000, 0.1071, 0.1900, 0.2882,\n",
      "        0.0000, 0.2863, 0.1887, 0.2036, 0.0000, 0.0000, 0.0068, 0.2848, 0.1682,\n",
      "        0.3037, 0.0000, 0.0000, 0.0736, 0.4681, 0.3366, 0.0000, 0.0000, 0.6040,\n",
      "        0.4224, 0.0241, 0.0000, 0.0000, 0.0000, 0.0000, 0.2337, 0.1222, 0.3245,\n",
      "        0.3101, 0.0154, 0.0000, 0.3189, 0.0704, 0.4282, 0.2079, 0.0000, 0.5002,\n",
      "        0.1124, 0.0000, 0.0000, 0.0000, 0.0000, 0.1649, 0.6966, 0.0330, 0.3068,\n",
      "        0.0000, 0.0000, 0.3905, 0.0000, 0.1502, 0.0679, 0.0000, 0.0000, 0.2404,\n",
      "        0.0000, 0.2209, 0.1540, 0.0000, 0.4083, 0.4986, 0.0000, 0.0981, 0.1088,\n",
      "        0.0000, 0.0000, 0.0936, 0.0922, 0.4243, 0.6809, 0.0661, 0.3261, 0.2670,\n",
      "        0.1590, 0.0000, 0.0301, 0.0000, 0.1209, 0.0335, 0.0797, 0.0000, 0.0000,\n",
      "        0.1370, 0.0615, 0.0700, 0.0000, 0.2369, 0.0000, 0.0000, 0.0961, 0.0178,\n",
      "        0.2909, 0.0000, 0.3172, 0.0000, 0.2532, 0.0000, 0.1126, 0.0000, 0.3702,\n",
      "        0.4498, 0.0000, 0.1593, 0.3885, 0.3236, 0.0895, 0.0316, 0.0000, 0.0000,\n",
      "        0.2315, 0.0000, 0.0430, 0.2459, 0.4144, 0.1712, 0.3613, 0.0000, 0.2709,\n",
      "        0.0000, 0.3063, 0.1131, 0.3175, 0.4250, 0.0000, 0.2330, 0.0000, 0.4582,\n",
      "        0.1974, 0.2441, 0.1276, 0.1394, 0.0411, 0.2421, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.2820, 0.4476, 0.2942, 0.2978, 0.1539, 0.0418, 0.0000, 0.2010,\n",
      "        0.2193, 0.1581, 0.0618, 0.1720, 0.2704, 0.0000, 0.0000, 0.1127, 0.0000,\n",
      "        0.1078, 0.4914, 0.2946, 0.4018, 0.1123, 0.0947, 0.2082, 0.0386, 0.0000,\n",
      "        0.1273, 0.1641, 0.3923, 0.4446, 0.0000, 0.3227, 0.1786, 0.0000, 0.0000,\n",
      "        0.2936, 0.0000, 0.0084, 0.0925, 0.1577, 0.3504, 0.6300, 0.0000, 0.0000,\n",
      "        0.0946, 0.6804, 0.1474, 0.0000], grad_fn=<MaxBackward0>)\n",
      "tensor([0, 3, 0, 0, 0, 5, 1, 8, 4, 4, 1, 9, 3, 0, 4, 4, 8, 0, 0, 4, 4, 4, 8, 3,\n",
      "        8, 0, 1, 8, 4, 0, 8, 8, 0, 8, 8, 0, 4, 8, 6, 0, 1, 8, 3, 1, 4, 9, 3, 4,\n",
      "        1, 4, 0, 8, 2, 0, 0, 0, 6, 8, 0, 9, 6, 3, 8, 0, 8, 0, 8, 0, 0, 6, 9, 4,\n",
      "        0, 1, 8, 9, 0, 0, 4, 8, 4, 4, 0, 0, 4, 9, 8, 0, 0, 8, 8, 8, 0, 0, 0, 0,\n",
      "        0, 7, 8, 9, 4, 0, 8, 1, 4, 8, 0, 9, 8, 0, 0, 0, 0, 8, 5, 0, 5, 0, 0, 9,\n",
      "        0, 9, 8, 0, 0, 8, 0, 8, 4, 0, 8, 9, 0, 0, 9, 0, 0, 3, 9, 8, 5, 8, 4, 8,\n",
      "        3, 0, 4, 0, 5, 9, 0, 0, 0, 6, 0, 6, 0, 0, 0, 0, 9, 2, 1, 0, 4, 0, 6, 0,\n",
      "        8, 0, 5, 5, 0, 8, 7, 8, 4, 4, 0, 0, 4, 0, 0, 8, 8, 8, 4, 0, 0, 0, 8, 8,\n",
      "        8, 3, 0, 3, 0, 1, 8, 2, 6, 1, 8, 4, 0, 0, 0, 0, 9, 5, 6, 8, 3, 0, 0, 3,\n",
      "        8, 8, 4, 4, 8, 0, 0, 0, 0, 0, 1, 2, 4, 1, 0, 9, 0, 0, 0, 6, 4, 2, 0, 8,\n",
      "        5, 0, 0, 6, 0, 4, 9, 8, 4, 9, 0, 0, 6, 9, 3, 0])\n",
      "Average Sequence Identity to Input: 59.0%\n",
      "AAIEVGLETPKGTKAKCHATDVIIIGREKTEKTDKLETGGATSISGIKGVAKNGKGAAPAFGPRLSDEDIEDVACVVLTI\n",
      "ADLAHGAQIFSANCAACHIGGNNIVMPDKTLKKDALETYGMNSVEAITTQVTNGKNAMPAFGGRLEDDDIADVANYVLSQAEKGW\n",
      "ADLAAGEQIFSANCAACHAGGNNVIMPEKTLKKEALEKYGMNSVDAITTQVTNGKNAMPAFGGRLSDEDIEDVANYVLSQSEW\n",
      "ADLINGEQIFSANCAACHPGGGNVIMPEKTLKKAALEEYSMDSVEAITTQVVNGKNAAPAFGGRLEESQIEDVAYYVLSQAEW\n",
      "ADSESGKKIFSANCSACAAGGNNVIMPDKTLKKEALKNNGMNSLSAITTQKSNGKNAGSAFGGRLADDDIEDVANYVLDQAEKG\n",
      "ADLENGAQIFSANCAACHAGGNNIIMPEKTLKKEALEKYGMNSVEAITYQVTNGKNAMPAFGGRLTDEQIEDVATYVLSQSEKGW\n",
      "ADIEAGEQIFSANCAACHGGGNNVIMPEKTLKKDALEKNGMKSIDAITTQVTNGKNAMPAFGGRLSAEDIDDVANYVLSQAE\n",
      "ADLAAGEQIFSANCAACHAGGNNVIMPEKTLKKDALETYSMNSIDAITTQVTNGKNAMPAFGGRLEDEDIEDVANYVLSQAEGW\n",
      "GDLNGGAKIFSANCAACHIGGGNIAIPGKTLKKDALKQYGMYSIEAIIYQVTNGKNAMPAFGGRLKDELIEDVAAYVLEQAEQGWW\n",
      "ADLAHGAKIFSANCAACHAGGNNVIMPEKTLKKEALEKYGMNSIEAITYQVTNGKNAMPAFGGRLSDEDIEDVANYVLSQAEKGW\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Sep  5 11:31:34 2018\n",
    "\n",
    "@author: Lewis Iain Moffat\n",
    "\n",
    "\n",
    "This script takes in a supplied sequence (as yet does not do multiple sequences\n",
    ") that is in a fasta file or a text document. This then takes this sequence and \n",
    "runs it through a forward pass of the autoencoder, encoding it and decoding it. \n",
    "This produces the same sequence with variation added. This presumes the protein\n",
    "sequence is not a metal binder however if it is it shouldn't affect the \n",
    "variation. This is because the metal binding variational model is used instead\n",
    "of the grammar model. This is for simplicities sake i.e. you don't need to have\n",
    "the grammar of a protein before being able to run this script. \n",
    "\n",
    "The only two arguments that need to be passed are the text_file and number of \n",
    "sequences out wanted. These are written to standard out\n",
    "\n",
    "This assumes you are running things on a cpu by default. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Imports\n",
    "# =============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import argparse\n",
    "import utils\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "# =============================================================================\n",
    "# Sort out Command Line arguments\n",
    "# =============================================================================\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-infile\", type=str,\n",
    "        help=\"file with sequence\", default=\"examples/seq2seq_example.txt\")# its either struc or nostruc\n",
    "parser.add_argument(\"-numout\", type=int,\n",
    "        help=\"number of sequences generated\", default=10)\n",
    "args = parser.parse_args(\"-infile examples/seq2seq_example.txt -numout 10\".split()) #add the string normally written in the command line as the parsed arguments\n",
    "args_dict = vars(args)        \n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Pytorch Module\n",
    "# =============================================================================\n",
    "class VAE(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, batch_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.batch_size = batch_size\n",
    "           \n",
    "\n",
    "        self.fc = torch.nn.Linear(input_size, hidden_sizes[0])  # 2 for bidirection \n",
    "        self.BN = torch.nn.BatchNorm1d(hidden_sizes[0])\n",
    "        self.fc1 = torch.nn.Linear(hidden_sizes[0], hidden_sizes[1])\n",
    "        self.BN1 = torch.nn.BatchNorm1d(hidden_sizes[1])\n",
    "        self.fc2 = torch.nn.Linear(hidden_sizes[1], hidden_sizes[2])\n",
    "        self.BN2 = torch.nn.BatchNorm1d(hidden_sizes[2])\n",
    "        self.fc3_mu = torch.nn.Linear(hidden_sizes[2], hidden_sizes[3])\n",
    "        self.fc3_sig = torch.nn.Linear(hidden_sizes[2], hidden_sizes[3])\n",
    "        \n",
    "        self.fc4 = torch.nn.Linear(hidden_sizes[3]+8, hidden_sizes[2])\n",
    "        self.BN4 = torch.nn.BatchNorm1d(hidden_sizes[2])\n",
    "        self.fc5 = torch.nn.Linear(hidden_sizes[2], hidden_sizes[1])\n",
    "        self.BN5 = torch.nn.BatchNorm1d(hidden_sizes[1])\n",
    "        self.fc6 = torch.nn.Linear(hidden_sizes[1], hidden_sizes[0])\n",
    "        self.BN6 = torch.nn.BatchNorm1d(hidden_sizes[0])\n",
    "        self.fc7 = torch.nn.Linear(hidden_sizes[0], input_size-8)\n",
    "\n",
    "    def sample_z(self,x_size, mu, log_var):\n",
    "        # Using reparameterization trick to sample from a gaussian\n",
    "        eps = torch.randn(x_size, self.hidden_sizes[-1])\t\n",
    "        return mu + torch.exp(log_var / 2) * eps\n",
    "    \n",
    "    def forward(self, x, code, struc=None):\n",
    "        \n",
    "        ###########\n",
    "        # Encoder #\n",
    "        ###########\n",
    "        \n",
    "        # get the code from the tensor\n",
    "        # add the conditioned code\n",
    "        x = torch.cat((x,code),1)    \n",
    "        # Layer 0\n",
    "        out1 = self.fc(x)        \n",
    "        out1 = nn.relu(self.BN(out1))\n",
    "        # Layer 1\n",
    "        out2 = self.fc1(out1)\n",
    "        out2 = nn.relu(self.BN1(out2))\n",
    "        # Layer 2\n",
    "        out3 = self.fc2(out2)\n",
    "        out3 = nn.relu(self.BN2(out3))\n",
    "        # Layer 3 - mu\n",
    "        mu   = self.fc3_mu(out3)\n",
    "        # layer 3 - sig\n",
    "        sig  = nn.softplus(self.fc3_sig(out3))        \n",
    "\n",
    "\n",
    "        ###########\n",
    "        # Decoder #\n",
    "        ###########\n",
    "        \n",
    "        # sample from the distro\n",
    "        \n",
    "        sample= self.sample_z(x.size(0),mu, sig)\n",
    "        # add the conditioned code\n",
    "        sample = torch.cat((sample, code),1)\n",
    "        # Layer 4\n",
    "        out4 = self.fc4(sample)\n",
    "        #print(code)\n",
    "        # print(sample.names)\n",
    "        out4 = nn.relu(self.BN4(out4))\n",
    "        a = out4.shape\n",
    "        print(a) #making sure the shape of out4 matches the values/indices size that is returned\n",
    "        values, indices = out4.max(0) #find highest values of tensor and record highest value as well as index\n",
    "        print(values) #show the highest values on the command line\n",
    "        print(indices) #show the matching index on the command line\n",
    "        f = open(\"out4_nodes.txt\",\"w+\") #open text file to output nodes from first layer of decoder\n",
    "        #   wout = out4.tolist() #write tensor object of first output nodes to list\n",
    "        #   wout = ''.join(str(e) for e in wout) #change list into string\n",
    "        #   f.write(wout) #write string of output nodes to test\n",
    "        wvalues = str(values) #change the values from tensor to string\n",
    "        f.write(wvalues) #write the values to out4_nodes\n",
    "        wout = str(indices) #indices of higest values in tensor to string\n",
    "        f.write(wout) #writes the indices to out4_nodes\n",
    "        # Layer 5\n",
    "        out5 = self.fc5(out4)\n",
    "        out5 = nn.relu(self.BN5(out5))\n",
    "        a = out5.shape\n",
    "        print(a)\n",
    "        values, indices = out5.max(0) #find highest values of tensor and record highest value as well as index\n",
    "        print(values) #show the highest values\n",
    "        print(indices) #show the matching index\n",
    "        f = open(\"out5_nodes.txt\",\"w+\") #open text file to output nodes from first layer of decoder\n",
    "        #   wout = out4.tolist() #write tensor object of first output nodes to list\n",
    "        #   wout = ''.join(str(e) for e in wout) #change list into string\n",
    "        #   f.write(wout) #write string of output nodes to test\n",
    "        wvalues = str(values)\n",
    "        f.write(wvalues) #write the values to out5_nodes\n",
    "        wout = str(indices) #indices of higest values in tensor to string\n",
    "        f.write(wout) #writes the indices to out5_nodes\n",
    "        # Layer 6\n",
    "        out6 = self.fc6(out5)\n",
    "        out6 = nn.relu(self.BN6(out6))\n",
    "        # Layer 7\n",
    "        out7 = nn.sigmoid(self.fc7(out6))\n",
    "        \n",
    "        return out7, mu, sig\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Create and Load model into memory\n",
    "# =============================================================================\n",
    "\n",
    "X_dim=3088\n",
    "hidden_size=[512,256,128,16]\n",
    "batch_size=args_dict[\"numout\"]\n",
    "vae = VAE(X_dim, hidden_size, batch_size)\n",
    "# load model\n",
    "vae.load_state_dict(torch.load(\"models/metal16_nostruc\", map_location=lambda storage, loc: storage))\n",
    "\n",
    "# =============================================================================\n",
    "#  Define function to produce sequences. \n",
    "# =============================================================================\n",
    "\n",
    "    \n",
    "def newMetalBinder(model,data):\n",
    "    \"\"\"\n",
    "    Generates a new sequence based on a metal code; the first 3080 dims are the\n",
    "    sequence, the final 8 are the metal binding flags. Fold is optional\n",
    "    \"\"\"\n",
    "    scores=[]\n",
    "    model.eval()\n",
    "    \n",
    "    code = np.tile(np.zeros(8),(model.batch_size,1))\n",
    "    x = np.tile(data[:3080],(model.batch_size,1))\n",
    "    X = torch.from_numpy(x).type(torch.FloatTensor)\n",
    "    C = torch.from_numpy(code).type(torch.FloatTensor)\n",
    "\n",
    "    x_sample, z_mu, z_var = model(X, C)\n",
    "    \n",
    "    \n",
    "    len_aa=140*22\n",
    "    y_label=np.argmax(x[:,:len_aa].reshape(batch_size,-1,22), axis=2)\n",
    "    y_pred=np.argmax(x_sample[:,:len_aa].cpu().data.numpy().reshape(batch_size,-1,22), axis=2)\n",
    "    for idx, row in enumerate(y_label):\n",
    "        scores.append(accuracy_score(row[:np.argmax(row)],y_pred[idx][:np.argmax(row)]))\n",
    "    print(\"Average Sequence Identity to Input: {0:.1f}%\".format(np.mean(scores)*100))\n",
    "    \n",
    "    out_seqs=x_sample[:,:len_aa].cpu().data.numpy()\n",
    "    for seq in out_seqs:\n",
    "        print(utils.vec_to_seq(seq))\n",
    "        \n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Produce new sequence\n",
    "# =============================================================================\n",
    "\n",
    "# first we read in the sequence from the \n",
    "with open(args_dict[\"infile\"],'r') as in_file:\n",
    "    seq=in_file.readlines()\n",
    "\n",
    "# format the sequence so if it is a FASTA file then we turf the line with >\n",
    "for idx, line in enumerate(seq):\n",
    "    seq[idx]=line.replace(\"\\n\",\"\")\n",
    "\n",
    "seq_in=\"\"\n",
    "for line in seq:\n",
    "    if \">\" in line:\n",
    "        continue\n",
    "    else:\n",
    "        seq_in=seq_in+line\n",
    "\n",
    "# now have a string which is the sequence        \n",
    "seq_in_vec=utils.seq_to_vec(seq_in)\n",
    "\n",
    "newMetalBinder(vae,seq_in_vec)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-be1b16765782>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     52\u001b[0m         help=\"number of sequences generated\", default=10)\n\u001b[0;32m     53\u001b[0m \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"examples/seq2seq_example.txt\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"r+\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[0margs_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvars\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\argparse.py\u001b[0m in \u001b[0;36mparse_args\u001b[1;34m(self, args, namespace)\u001b[0m\n\u001b[0;32m   1769\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0margv\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1770\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'unrecognized arguments: %s'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1771\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1772\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\argparse.py\u001b[0m in \u001b[0;36merror\u001b[1;34m(self, message)\u001b[0m\n\u001b[0;32m   2519\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_usage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2520\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'prog'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'message'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2521\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%(prog)s: error: %(message)s\\n'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\argparse.py\u001b[0m in \u001b[0;36mexit\u001b[1;34m(self, status, message)\u001b[0m\n\u001b[0;32m   2506\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2507\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_print_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2508\u001b[1;33m         \u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2509\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2510\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSystemExit\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "python seq_to_seq.py -infile examples/seq2seq_example.txt -numout 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
