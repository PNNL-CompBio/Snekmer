{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([10, 128])\n",
      "128\n",
      "torch.Size([10, 256])\n",
      "256\n",
      "(10, 3080)\n",
      "Average Sequence Identity to Input: 60.1%\n",
      "[1.1633015e-01 6.1108196e-01 1.1680838e-02 ... 2.4804574e-16 4.8697825e-16\n",
      " 1.0000000e+00]\n",
      "[1.8130545e-01 6.9561148e-01 5.7744263e-03 ... 6.8005178e-17 1.5393639e-16\n",
      " 1.0000000e+00]\n",
      "[2.8535354e-01 3.2712936e-01 9.2944168e-03 ... 1.4238643e-15 1.8644940e-15\n",
      " 1.0000000e+00]\n",
      "[2.1861203e-03 7.5355351e-01 2.1442639e-02 ... 2.2769651e-19 4.2823748e-19\n",
      " 1.0000000e+00]\n",
      "[2.1523491e-01 5.3341520e-01 2.1010539e-02 ... 1.3831701e-17 3.2444681e-17\n",
      " 1.0000000e+00]\n",
      "[2.52371550e-01 6.57651424e-01 5.14393784e-02 ... 6.72852037e-18\n",
      " 1.34941365e-17 1.00000000e+00]\n",
      "[1.4901452e-01 5.7754391e-01 5.6306254e-03 ... 3.8031517e-16 8.9857162e-16\n",
      " 1.0000000e+00]\n",
      "[2.9872788e-02 9.3515557e-01 2.9262018e-03 ... 4.6611092e-22 1.6563628e-21\n",
      " 1.0000000e+00]\n",
      "[2.8725848e-01 8.0325373e-02 2.1072498e-03 ... 2.9573607e-15 5.4466654e-15\n",
      " 1.0000000e+00]\n",
      "[1.3291746e-01 7.0576584e-01 5.6505664e-03 ... 4.4419784e-16 1.0054354e-15\n",
      " 1.0000000e+00]\n",
      "ADLENGAKIFSANCAACHAGGGNVINREKTLKKDALEKYGMDSVEAITAQVTNGKNAMPAFGGRLTDEQIEDVANYVLEQSEW\n",
      "ADLEHGAQIFSANCAACHAGGNNVIMPEKTLKKEALEKYGMNSIEAITYQVTNGKNAMPAFGGRLSDEQIEDVANYVLSQAEW\n",
      "ADLEHGAAIFSANCAACHGGGNNVIMPEKTLKKDDLEKNGMNSVEAITYQVKNGKAAMPAFKGRLEDDDIDDVANYVLSQAAGG\n",
      "ADVAAGEQIFSANCAACPAGGNNVIMANKTLKKEALEEYGMNSLEAITYQVTNGKNAAPAFGGRLSDEDIENVAAYVLSQAENG\n",
      "ADLESGEQIFSANCSACHSGGKNVIMPNKTLKKEVLEKYGMNSIDAIMYQVYNGKNAMPAFMGRLSDDQIEDVAAYVLEQAEQGW\n",
      "ADLAAGAQIFSANCAACHGGGNNIIMPEKTLKKDALEKYSMNSIEAITTQVTNGKGAMPAFGGRLEADDIDAVANYVLSQAEQGW\n",
      "ADLANGAKVFSANCAACHAGGGNLIMPGKTLKKEALEKYGMNSIEAITYQVTNGKNAMPAFGGRLTDEDIEDVAAYVLSQAEKGW\n",
      "ADIAAGEQIFSANCAACHAGGNNAIMPEKTLKKDALEKNGMNSIDAITYQVTNGKNAAPAFGGRLSDEDIEDVANYVLSQSEQGW\n",
      "MDSANGAVVFSGGCAACHAGGRNGVAANKTLKKEALEKFLRVSAARITTIMKLGVNAGPAFAFRLSGNDIEEAANVAANYVDQAW\n",
      "ADIEAGEQIFSANCAACHAGGNNVIMPDKTLKKDALEKNGMNSVEAITNQVTNGKNAMPAFGGRLSDEDIEDVANYVLSQSEKGW\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Sep  5 11:31:34 2018\n",
    "\n",
    "@author: Lewis Iain Moffat\n",
    "\n",
    "\n",
    "This script takes in a supplied sequence (as yet does not do multiple sequences\n",
    ") that is in a fasta file or a text document. This then takes this sequence and \n",
    "runs it through a forward pass of the autoencoder, encoding it and decoding it. \n",
    "This produces the same sequence with variation added. This presumes the protein\n",
    "sequence is not a metal binder however if it is it shouldn't affect the \n",
    "variation. This is because the metal binding variational model is used instead\n",
    "of the grammar model. This is for simplicities sake i.e. you don't need to have\n",
    "the grammar of a protein before being able to run this script. \n",
    "\n",
    "The only two arguments that need to be passed are the text_file and number of \n",
    "sequences out wanted. These are written to standard out\n",
    "\n",
    "This assumes you are running things on a cpu by default. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Imports\n",
    "# =============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import argparse\n",
    "import utils\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "# =============================================================================\n",
    "# Sort out Command Line arguments\n",
    "# =============================================================================\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-infile\", type=str,\n",
    "        help=\"file with sequence\", default=\"examples/seq2seq_example.txt\")# its either struc or nostruc\n",
    "parser.add_argument(\"-numout\", type=int,\n",
    "        help=\"number of sequences generated\", default=10)\n",
    "args = parser.parse_args(\"-infile examples/seq2seq_example.txt -numout 10\".split()) #add the string normally written in the command line as the parsed arguments\n",
    "args_dict = vars(args)        \n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Pytorch Module\n",
    "# =============================================================================\n",
    "class VAE(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, batch_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.batch_size = batch_size\n",
    "           \n",
    "\n",
    "        self.fc = torch.nn.Linear(input_size, hidden_sizes[0])  # 2 for bidirection \n",
    "        self.BN = torch.nn.BatchNorm1d(hidden_sizes[0])\n",
    "        self.fc1 = torch.nn.Linear(hidden_sizes[0], hidden_sizes[1])\n",
    "        self.BN1 = torch.nn.BatchNorm1d(hidden_sizes[1])\n",
    "        self.fc2 = torch.nn.Linear(hidden_sizes[1], hidden_sizes[2])\n",
    "        self.BN2 = torch.nn.BatchNorm1d(hidden_sizes[2])\n",
    "        self.fc3_mu = torch.nn.Linear(hidden_sizes[2], hidden_sizes[3])\n",
    "        self.fc3_sig = torch.nn.Linear(hidden_sizes[2], hidden_sizes[3])\n",
    "        \n",
    "        self.fc4 = torch.nn.Linear(hidden_sizes[3]+8, hidden_sizes[2])\n",
    "        self.BN4 = torch.nn.BatchNorm1d(hidden_sizes[2])\n",
    "        #print(self.BN4) batchnorm1d = batch normalization\n",
    "        self.fc5 = torch.nn.Linear(hidden_sizes[2], hidden_sizes[1])\n",
    "        self.BN5 = torch.nn.BatchNorm1d(hidden_sizes[1])\n",
    "        self.fc6 = torch.nn.Linear(hidden_sizes[1], hidden_sizes[0])\n",
    "        self.BN6 = torch.nn.BatchNorm1d(hidden_sizes[0])\n",
    "        self.fc7 = torch.nn.Linear(hidden_sizes[0], input_size-8)\n",
    "        #this is how the size goes back down from 5120 to 3080 from out6 to out7\n",
    "        #self.fc4 = torch.nn.Linear(hidden_sizes[3+8], input_size-8)\n",
    "            #torch models have sizes saved\n",
    "        \n",
    "    #testing this with out4 and out5; size mismatch for fc4.weight: copying a param with shape torch.Size([128, 24]) from checkpoint, the shape in current model is torch.Size([3080, 512]).\n",
    "\t#size mismatch for fc4.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([3080]).\n",
    "\n",
    "    def sample_z(self,x_size, mu, log_var):\n",
    "        # Using reparameterization trick to sample from a gaussian\n",
    "        eps = torch.randn(x_size, self.hidden_sizes[-1])\t\n",
    "        return mu + torch.exp(log_var / 2) * eps\n",
    "    \n",
    "    def forward(self, x, code, struc=None):\n",
    "        \n",
    "        ###########\n",
    "        # Encoder #\n",
    "        ###########\n",
    "        \n",
    "        # get the code from the tensor\n",
    "        # add the conditioned code\n",
    "        x = torch.cat((x,code),1)    \n",
    "        # Layer 0\n",
    "        out1 = self.fc(x)        \n",
    "        out1 = nn.relu(self.BN(out1))\n",
    "        # Layer 1\n",
    "        out2 = self.fc1(out1)\n",
    "        out2 = nn.relu(self.BN1(out2))\n",
    "        # Layer 2\n",
    "        out3 = self.fc2(out2)\n",
    "        out3 = nn.relu(self.BN2(out3))\n",
    "        # Layer 3 - mu\n",
    "        mu   = self.fc3_mu(out3)\n",
    "        # layer 3 - sig\n",
    "        sig  = nn.softplus(self.fc3_sig(out3))        \n",
    "\n",
    "\n",
    "        ###########\n",
    "        # Decoder #\n",
    "        ###########\n",
    "        \n",
    "        # sample from the distro\n",
    "       \n",
    "        sample= self.sample_z(x.size(0),mu, sig)\n",
    "        # add the conditioned code\n",
    "        print(x.size(0))\n",
    "            #batch size\n",
    "        f = open(\"sample_decoder.txt\", \"w+\")\n",
    "        wsample = sample\n",
    "        wsample = wsample.tolist()\n",
    "        wsample = ''.join(str(e) for e in wsample)\n",
    "        f.write(wsample)\n",
    "        \n",
    "        sample = torch.cat((sample, code),1)\n",
    "        # Layer 4\n",
    "        out4 = self.fc4(sample)\n",
    "        test = out4 # to return without nn.relu\n",
    "        #print(code)\n",
    "        # print(sample.names)\n",
    "        out4 = nn.relu(self.BN4(out4))\n",
    "        print(out4.shape) \n",
    "            #making sure the shape of out4 matches the values/indices size that is returned\n",
    "        values, indices = out4.max(0) \n",
    "            #find highest values of tensor and record highest value as well as index\n",
    "            #print(values) #show the highest values on the command line\n",
    "        print(len(values))\n",
    "            #print(indices) #show the matching index on the command line\n",
    "        f = open(\"out4_nodes.txt\",\"w+\") \n",
    "            #open text file to output nodes from first layer of decoder\n",
    "            #   wout = out4.tolist() #write tensor object of first output nodes to list\n",
    "            #   wout = ''.join(str(e) for e in wout) #change list into string\n",
    "            #   f.write(wout) #write string of output nodes to test\n",
    "        wvalues = str(values) \n",
    "            #change the values from tensor to string\n",
    "        f.write(wvalues) \n",
    "            #write the values to out4_nodes\n",
    "        wout = str(indices) \n",
    "            #indices of higest values in tensor to string\n",
    "        f.write(wout) \n",
    "            #writes the indices to out4_nodes\n",
    "        # Layer 5\n",
    "        out5 = self.fc5(out4)\n",
    "        out5 = nn.relu(self.BN5(out5))\n",
    "        print(out5.shape)\n",
    "        values, indices = out5.max(0) \n",
    "            #find highest values of tensor and record highest value as well as index\n",
    "        #print(values) \n",
    "            #show the highest values\n",
    "        print(len(values))\n",
    "        #print(indices) \n",
    "            #show the matching index\n",
    "        f = open(\"out5_nodes.txt\",\"w+\") \n",
    "            #open text file to output nodes from first layer of decoder\n",
    "        #   wout = out4.tolist() \n",
    "            #write tensor object of first output nodes to list\n",
    "        #   wout = ''.join(str(e) for e in wout) \n",
    "            #change list into string\n",
    "        #   f.write(wout) #write string of output nodes to test\n",
    "        wvalues = str(values)\n",
    "        f.write(wvalues) \n",
    "            #write the values to out5_nodes\n",
    "        wout = str(indices) \n",
    "            #indices of higest values in tensor to string\n",
    "        f.write(wout) \n",
    "            #writes the indices to out5_nodes\n",
    "        # Layer 6\n",
    "        out6 = self.fc6(out5)\n",
    "        out6 = nn.relu(self.BN6(out6))\n",
    "        # Layer 7\n",
    "        out7 = nn.sigmoid(self.fc7(out6))\n",
    "        #test = nn.sigmoid(self.fc4(test))\n",
    "        # test = nn.sigmoid(out4) \n",
    "            #out4 to 1280 size versus out7 to 3080 size\n",
    "        # test = nn.sigmoid(test) \n",
    "            #out4 with nn.relu to 1280 size versus out7 to 3080 size\n",
    "       # test = nn.sigmoid(out5) \n",
    "        #out5 to 2560 size versus out7 to 3080 size - what about without nn.relu?\n",
    "       \n",
    "       # test = nn.sigmoid(out6) \n",
    "            #out6 to 5120 size\n",
    "        return out7, mu, sig\n",
    "        #return test, mu, sig\n",
    "        #return out4, mu, sig\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Create and Load model into memory\n",
    "# =============================================================================\n",
    "\n",
    "X_dim=3088\n",
    "hidden_size=[512,256,128,16]\n",
    "#hidden_size=[16]\n",
    "batch_size=args_dict[\"numout\"]\n",
    "vae = VAE(X_dim, hidden_size, batch_size)\n",
    "# load model\n",
    "vae.load_state_dict(torch.load(\"models/metal16_nostruc\", map_location=lambda storage, loc: storage))\n",
    "\n",
    "# =============================================================================\n",
    "#  Define function to produce sequences. \n",
    "# =============================================================================\n",
    "\n",
    "    \n",
    "def newMetalBinder(model,data):\n",
    "    \"\"\"\n",
    "    Generates a new sequence based on a metal code; the first 3080 dims are the\n",
    "    sequence, the final 8 are the metal binding flags. Fold is optional\n",
    "    \"\"\"\n",
    "    scores=[]\n",
    "    model.eval()\n",
    "    \n",
    "    code = np.tile(np.zeros(8),(model.batch_size,1))\n",
    "    #print(code)\n",
    "    x = np.tile(data[:3080],(model.batch_size,1))\n",
    "    f = open(\"code_decoder.txt\", \"w+\")\n",
    "    wcode = x\n",
    "    wcode = wcode.tolist()\n",
    "    wcode = ''.join(str(e) for e in wcode)\n",
    "    f.write(wcode)\n",
    "    X = torch.from_numpy(x).type(torch.FloatTensor)\n",
    "    C = torch.from_numpy(code).type(torch.FloatTensor)\n",
    "\n",
    "    x_sample, z_mu, z_var = model(X, C)\n",
    "    \n",
    "    \n",
    "    len_aa=140*22 #where do these numbers come from? 140*22 = 3080 which is the dimension of X\n",
    "    y_label=np.argmax(x[:,:len_aa].reshape(batch_size,-1,22), axis=2)\n",
    "    print(x_sample[:,:len_aa].cpu().data.numpy().shape)\n",
    "    y_pred=np.argmax(x_sample[:,:len_aa].cpu().data.numpy().reshape(batch_size,-1,22), axis=2)\n",
    "   # print(x_sample[:,:len_aa].cpu().data.numpy().shape) dimensions of out7/normal output from decoder\n",
    "    #y_pred=np.argmax(x_sample[:,:len_aa].cpu().data.numpy().reshape(batch_size,-1,22), axis=2)\n",
    "    for idx, row in enumerate(y_label):\n",
    "        scores.append(accuracy_score(row[:np.argmax(row)],y_pred[idx][:np.argmax(row)]))\n",
    "    print(\"Average Sequence Identity to Input: {0:.1f}%\".format(np.mean(scores)*100))\n",
    "    \n",
    "    out_seqs=x_sample[:,:len_aa].cpu().data.numpy() #make sure you understand what each part of this command does\n",
    "    for seq in out_seqs:\n",
    "        print(seq) #see the difference between utils.vec_to_seq and out_seqs\n",
    "    for seq in out_seqs:\n",
    "        print(utils.vec_to_seq(seq))\n",
    "        \n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Produce new sequence\n",
    "# =============================================================================\n",
    "\n",
    "# first we read in the sequence from the \n",
    "with open(args_dict[\"infile\"],'r') as in_file:\n",
    "    seq=in_file.readlines()\n",
    "\n",
    "# format the sequence so if it is a FASTA file then we turf the line with >\n",
    "for idx, line in enumerate(seq):\n",
    "    seq[idx]=line.replace(\"\\n\",\"\")\n",
    "\n",
    "seq_in=\"\"\n",
    "for line in seq:\n",
    "    if \">\" in line:\n",
    "        continue\n",
    "    else:\n",
    "        seq_in=seq_in+line\n",
    "\n",
    "# now have a string which is the sequence        \n",
    "seq_in_vec=utils.seq_to_vec(seq_in)\n",
    "\n",
    "newMetalBinder(vae,seq_in_vec)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-19-4885625ec1fb>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-19-4885625ec1fb>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    python seq_to_seq.py -infile examples/seq2seq_example.txt -numout 10\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#python seq_to_seq.py -infile examples/seq2seq_example.txt -numout 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
